
### 机器学习经典算法应用
---

分类问题：
* 逻辑回归(工业界最常用)
* 支持向量机
* 随机森林
* 朴素贝叶斯(NLP中常用)
* 深度神经网络(视频、图片、语音等多媒体数据中使用)


处理回归问题的常用算法包括：
* 线性回归
* 普通最小二乘回归（Ordinary Least Squares Regression）
* 逐步回归（Stepwise Regression）
* 多元自适应回归样条（Multivariate Adaptive Regression Splines）

处理聚类问题的常用算法包括：
* K均值（K-means）
* 基于密度聚类
* LDA (Latent Dirichlet Allocation) 
* 等等。
Linear Discriminant Analysis, 以下简称LDA

降维算法：
* 主成分分析（PCA）
* 奇异值分解（SVD）

推荐系统：
* 协同过滤算法


模型融合(model ensemble)和提升(boosting)的算法包括：
* bagging
* adaboost
* GBDT
* GBRT

---


工业界使用相关：
数据处理：
* Hadoop: 基本上是工业界的标配了。一般用来做特征清洗、特征处理的相关工作。
* Spark: 提供了MLlib这样的大数据机器学习平台，实现了很多常用算法。但可靠性、稳定性上有待提高。

---

基本工作流程：
1. 将问题抽象成数学问题： 
    明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。

2. 获取数据： 
    * 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
    * 数据要有代表性，否则必然会过拟合。
    * 对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
    * 要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

3. 特征预处理与特征选择
    * 良好的数据要能够提取出良好的特征才能真正发挥效力。
    * 特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。
    * 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

4. 训练模型与调优
    调参，调参，调参。。。

5. 模型诊断
    * 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
    * 误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
    * 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。
    
6. 模型融合
    * 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
    * 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

7. 上线运行
    * 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

---

机器学习理论类：

    1.  写出全概率公式&贝叶斯公式
    2.  模型训练为什么要引入偏差(bias)和方差(variance)？  证
    3.  CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型
    4.  如何解决过拟合问题？
    5.  One-hot的作用是什么？为什么不直接使用数字作为表示
    6.  决策树和随机森林的区别是什么？
    7.  朴素贝叶斯为什么“朴素naive”？
    8.  kmeans初始点除了随机选取之外的方法
    9.  LR明明是分类模型为什么叫回归
    10. 梯度下降如何并行化
    11. LR中的L1/L2正则项是啥
    12. 简述决策树构建过程
    13. 解释Gini系数
    14. 决策树的优缺点
    15. 出现估计概率值为 0 怎么处理
    16. 随机森林的生成过程
    17. 介绍一下Boosting的思想
    18. gbdt的中的tree是什么tree？有什么特征
    19. xgboost对比gbdt/boosting Tree有了哪些方向上的优化
    20. 什么叫最优超平面
    21. 什么是支持向量
    22. SVM如何解决多分类问题
    23. 核函数的作用是啥

特征工程类：

    1.  怎么去除DataFrame里的缺失值？
    2.  特征无量纲化的常见操作方法
    3.  如何对类别变量进行独热编码？
    4.  如何把“年龄”字段按照我们的阈值分段？
    5.  如何根据变量相关性画出热力图？
    6.  如何把分布修正为类正态分布？
    7.  怎么简单使用PCA来划分数据且可视化呢？
    8.  怎么简单使用LDA来划分数据且可视化呢？


深度学习类：
    1.  你觉得batch-normalization过程是什么样的
    2.  激活函数有什么用？常见的激活函数的区别是什么？
    3.  Softmax的原理是什么？有什么作用？
    CNN的平移不变性是什么？如何实现的？
    4.  VGG，GoogleNet，ResNet等网络之间的区别是什么？
    5.  残差网络为什么能解决梯度消失的问题
    6.  LSTM为什么能解决梯度消失/爆炸的问题
    7.  Attention对比RNN和CNN，分别有哪点你觉得的优势
    8.  写出Attention的公式
    9.  Attention机制，里面的q,k,v分别代表什么
    10. 为什么self-attention可以替代seq2seq

自然语言处理（NLP）类：
    1.  GolVe的损失函数
    2.  为什么GolVe会用的相对比W2V少
    3.  层次softmax流程
    4.  负采样流程
    5.  怎么衡量学到的embedding的好坏
    6.  阐述CRF原理
    7.  详述LDA原理
    8.  LDA中的主题矩阵如何计算
    9.  LDA和Word2Vec区别？LDA和Doc2Vec区别
    10. Bert的双向体现在什么地方
    11. Bert的是怎样预训练的
    12. 在数据中随机选择 15% 的标记，其中80%被换位[mask]，10%不变、10%随机替换其他单词，原因是什么
    13. 为什么BERT有3个嵌入层，它们都是如何实现的
    14. 手写一个multi-head attention

推荐系统类：
    1.  DNN与DeepFM之间的区别
    2.  你在使用deepFM的时候是如何处理欠拟合和过拟合问题的
    3.  deepfm的embedding初始化有什么值得注意的地方吗
    4.  YoutubeNet 变长数据如何处理的
    5.  YouTubeNet如何避免百万量级的softmax问题的
    6.  推荐系统有哪些常见的评测指标？
    7.  MLR的原理是什么？做了哪些优化？

计算机视觉（CV）类：
    1.  常见的模型加速方法
    2.  目标检测里如何有效解决常见的前景少背景多的问题
    3.  目标检测里有什么情况是SSD、YOLOv3、Faster R-CNN等所不能解决的，假设网络拟合能力无限强
    4.  ROIPool和ROIAlign的区别
    5.  介绍常见的梯度下降优化方法
    6.  Detection你觉的还有哪些可做的点
    7.  mini-Batch SGD相对于GD有什么优点
    8.  人体姿态估计主流的两个做法是啥？简单介绍下
    9.  卷积的实现原理以及如何快速高效实现局部weight sharing的卷积操作方式
    10. CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果
