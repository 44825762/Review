### 深度学习 模型优化方法

#### CNN 模型优化
* 两个卷积层
* 激活函数 => 线性修正单元(ReLU)
* 正则化 => Batch Normalization 批标准正则化
* 拓展数据集 => 参考AlexNet 可实现数据百倍拓展
* 继续插入额外的全连接层
* Dropout 方法 参数设置方法待补充
* 通过训练多个网络，采用投票的方式组合网络 提高准确率
* 冻结部分参数，训练另一部分参数 
* 通过检查点保存模型，可以实现在检查点状态继续训练


#### 激活函数

激活函数，并不是去激活什么，而是指如何把“激活的神经元的特征”通过函数把特征保留并映射出来（保留特征，去除一些数据中是的冗余），这是神经网络能解决非线性问题关键。

* 激活函数作用
激活函数的主要作用是提供网络的非线性建模能力。在卷积层中，我们主要采用了卷积的方式来处理，也就是对每个像素点赋予一个权值，这个操作显然就是线性的。但是对于我们样本来说，不一定是线性可分的，为了解决这个问题，我们可以进行线性变化，或者我们引入非线性因素，解决线性模型所不能解决的问题。如果没有激活函数，那么该网络仅能够表达线性映射，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此也可以认为，只有加入了激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。

激活函数还可以构建稀疏矩阵，也就是稀疏性，这个特性可以去除数据中的冗余，最大可能保留数据的特征，也就是大多数为0的稀疏矩阵来表示（这个特性主要是对于Relu）
待补充

#### 冻结部分参数

加载预训练模型后，比如在resnet的基础上连接了新的模块，那么resnet部分将不需要参与训练。
具体代码待补充

