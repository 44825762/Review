{"./":{"url":"./","title":"Introduction","keywords":"","body":"计算机知识整理 本文档使用 Gitbook 制作 贡献者 LGR - https://github.com/44825762 部分内容来自各大论坛，如有侵权请联系 44825762@163.com Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/":{"url":"basic/","title":"基础知识","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/":{"url":"basic/network/","title":"计算机网络","keywords":"","body":"五层协议的体系结构 包括 应用层、传输层、网络层、数据链路层、物理层 各层涉及的协议： 应用层: (典型设备:应用程序，如FTP，SMTP ，HTTP) DHCP(Dynamic Host Configuration Protocol)动态主机分配协议，使用UDP协议工作， 主要有两个用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。实现即插即用连网。 FTP （File Transfer Protocol）文件传输协议减少或消除不同操作系统下处理文件的不兼容性。 HTTP （Hypertext Transfer Protocol ）超文本传输协议 ，面向事务的应用层协议 SMTP （Simple Mail Transfer Protocol ）简单邮件传输协议 用于发送邮件。 RPC （Remote Procedure Call Protocol ）（RFC- 1831）远程过程调用协议 传输层: (典型设备: 进程和端口) 数据单元：数据段 （Segment） TCP （Transmission Control Protocol）传输控制协议提供可靠的面向连接的服务，传输数据前须先建立连接，结束后释放。可靠的全双工信道。可靠、有序、无丢失、不重复。 UDP (User Datagram Protocol ）用户数据报协议发送数据前无需建立连接，不使用拥塞控制，不保证可靠交付，最大努力交付。 网络层: (典型设备:路由器，防火墙、多层交换机) 数据单元：数据包（Packet ） IP (IPv4 · IPv6) (Internet Protocol) 网络之间互连的协议 ARP (Address Resolution Protocol) 即地址解析协议，实现通过IP 地址得知其物理地址 RARP (Reverse Address Resolution Protocol)反向地址转换协议允许局域网的物理机器从网关服务器的 ARP 表或者缓存上请求其 IP地址。 ICMP (Internet Control Message Protocol ）Internet 控制报文协议。它是TCP/IP 协议族的一个子协议，用于在IP 主机、路由器之间传递控制消息。 ICMPv6 : IGMP (Internet Group Management Protocol)Internet组管理协议,是因特网协议家族中的一个组播协议，用于IP主机向任一个直接相邻的路由器报告他们的组成员情况。 数据链路层: (典型设备: 网卡，网桥，交换机) 数据单元：帧 （Frame） PPP(Point-to-Ponit Protocol)点对点协议面向字节，由三部分组成：一个将IP 数据报封装到串行链路的方法；一个用于建立、配置和测试数据链路连接的链路控制协议 停止等待协议：CSMA/CD(Carrrier Sense Multiple Access with Collision Detection)载波监听多点接入/碰撞检测协议。总线型网络，协议的实质是载波监听和碰撞检测。载波监听即发数据前先检测总线上是否有其他计算机在发送数据，如暂时不发数据，避免碰撞。碰撞检测为计算机边发送数据边检测信道上的信号电压大小。 ARQ（Automatic Repeat-reQuest ）自动重传请求协议，错误纠正协议之一，包括停止等待ARQ 协议和连续ARQ 协议，错误侦测、正面确认、逾时重传与负面确认继以重传等机制。 物理层:(典型设备：中继器，集线器、网线、HUB) 数据单元：比特 （Bit） 以太网物理层、调制解调器、PLC 、SONET/SDH 、G.709 、光导纤维、 同轴电缆、双绞线 用户访问一个网站的过程以及需要的协议 DNS解析 浏览器 -> 局部DNS服务器(DNS代理解析服务器) -> 根域名服务器 -> .xxx的顶级域名服务器 -> 一级域名的顶级域名服务器 -> 找到域名的权威DNS的Address记录或者Cname. HTTP交互过程 封装HTTP协议报头，HTTP主要包括请求行，请求头部，空行，请求主题.请求行包括为请求方法 资源路径(url) 协议版本，请求方法主要有POST GET HEAD PUT DELETE MOVE TCP交互过程 IP交互过程 ARP交互过程 把ip地址转换成一个唯一的MAC的地址，这里有一个ARP协议寻址过程 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/HTTP.html":{"url":"basic/network/HTTP.html","title":"HTTP 协议","keywords":"","body":"HTTP 的特性 HTTP 协议构建于 TCP/IP 协议之上，是一个应用层协议，默认端口号是 80 HTTP 是无连接无状态的 HTTP 报文 请求报文 HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体。类似于下面这样： HTTP 定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE,HEAD。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而 HTTP 中的GET，POST，PUT，DELETE就对应着对这个资源的查，增，改，删4个操作。 GET 用于信息获取，而且应该是安全的 和 幂等的。 所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 幂等的意味着对同一 URL 的多个请求应该返回同样的结果。 GET 请求报文示例： GET /books/?sex=man&name=Professional HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.7.6) Gecko/20050225 Firefox/1.0.1 Connection: Keep-Alive POST 表示可能修改变服务器上的资源的请求。 POST / HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.7.6) Gecko/20050225 Firefox/1.0.1 Content-Type: application/x-www-form-urlencoded Content-Length: 40 Connection: Keep-Alive sex=man&name=Professional 其他 HEAD：请求获取由Request-URI所标识的资源的响应消息报头。 PUT：在给定的URL下存储一个文档。 DELETE：删除给定的URL所标志的资源。 注意: GET 可提交的数据量受到URL长度的限制，HTTP 协议规范没有对 URL 长度进行限制。这个限制是特定的浏览器及服务器对它的限制 理论上讲，POST 是没有大小限制的，HTTP 协议规范也没有进行大小限制，出于安全考虑，服务器软件在实现时会做一定限制 参考上面的报文示例，可以发现 GET 和 POST 数据内容是一模一样的，只是位置不同，一个在 URL 里，一个在 HTTP 包的包体里 POST 提交数据的方式 HTTP 协议中规定 POST 提交的数据必须在 body 部分中，但是协议中没有规定数据使用哪种编码方式或者数据格式。实际上，开发者完全可以自己决定消息主体的格式，只要最后发送的 HTTP 请求满足上面的格式就可以。 但是，数据发送出去，还要服务端解析成功才有意义。一般服务端语言如 PHP、Python 等，以及它们的 framework，都内置了自动解析常见数据格式的功能。服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。下面就正式开始介绍它们： application/x-www-form-urlencoded 这是最常见的 POST 数据提交方式。浏览器的原生 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。上个小节当中的例子便是使用了这种提交方式。可以看到 body 当中的内容和 GET 请求是完全相同的。 multipart/form-data 这又是一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 表单的 enctype 等于 multipart/form-data。直接来看一个请求示例： POST http://www.example.com HTTP/1.1 Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\"text\" title ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\"file\"; filename=\"chrome.png\" Content-Type: image/png PNG ... content of chrome.png ... ------WebKitFormBoundaryrGKCBY7qhFd3TrwA-- 这个例子稍微复杂点。首先生成了一个 boundary 用于分割不同的字段，为了避免与正文内容重复，boundary 很长很复杂。然后 Content-Type 里指明了数据是以 multipart/form-data 来编码，本次请求的 boundary 是什么内容。消息主体里按照字段个数又分为多个结构类似的部分，每部分都是以 --boundary 开始，紧接着是内容描述信息，然后是回车，最后是字段具体内容（文本或二进制）。如果传输的是文件，还要包含文件名和文件类型信息。消息主体最后以 --boundary-- 标示结束。关于 multipart/form-data 的详细定义，请前往 RFC1867 查看（或者相对友好一点的 MDN 文档）。 这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。 上面提到的这两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 表单也只支持这两种方式（通过 元素的 enctype 属性指定，默认为 application/x-www-form-urlencoded。其实 enctype 还支持 text/plain，不过用得非常少）。 随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，例如 application/json，text/xml，乃至 application/x-protobuf 这种二进制格式，只要服务器可以根据 Content-Type 和 Content-Encoding 正确地解析出请求，都是没有问题的。 响应报文 HTTP 响应与 HTTP 请求相似，HTTP响应也由3个部分构成，分别是： 状态行 响应头(Response Header) 响应正文 状态行由协议版本、数字形式的状态代码、及相应的状态描述，各元素之间以空格分隔。 常见的状态码有如下几种： 200 OK 客户端请求成功 301 Moved Permanently 请求永久重定向 302 Moved Temporarily 请求临时重定向 304 Not Modified 文件未修改，可以直接使用缓存的文件。 400 Bad Request 由于客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized 请求未经授权。这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务。服务器通常会在响应正文中给出不提供服务的原因 404 Not Found 请求的资源不存在，例如，输入了错误的URL 500 Internal Server Error 服务器发生不可预期的错误，导致无法完成客户端的请求。 503 Service Unavailable 服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常。 下面是一个HTTP响应的例子： HTTP/1.1 200 OK Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:23:42 GMT Content-Length:112 ... 条件 GET HTTP 条件 GET 是 HTTP 协议为了减少不必要的带宽浪费，提出的一种方案。详见 RFC2616 。 HTTP 条件 GET 使用的时机？ 客户端之前已经访问过某网站，并打算再次访问该网站。 HTTP 条件 GET 使用的方法？ 客户端向服务器发送一个包询问是否在上一次访问网站的时间后是否更改了页面，如果服务器没有更新，显然不需要把整个网页传给客户端，客户端只要使用本地缓存即可，如果服务器对照客户端给出的时间已经更新了客户端请求的网页，则发送这个更新了的网页给用户。 下面是一个具体的发送接受报文示例： 客户端发送请求： GET / HTTP/1.1 Host: www.sina.com.cn:80 If-Modified-Since:Thu, 4 Feb 2010 20:39:13 GMT Connection: Close 第一次请求时，服务器端返回请求数据，之后的请求，服务器根据请求中的 If-Modified-Since 字段判断响应文件没有更新，如果没有更新，服务器返回一个 304 Not Modified响应，告诉浏览器请求的资源在浏览器上没有更新，可以使用已缓存的上次获取的文件。 HTTP/1.0 304 Not Modified Date: Thu, 04 Feb 2010 12:38:41 GMT Content-Type: text/html Expires: Thu, 04 Feb 2010 12:39:41 GMT Last-Modified: Thu, 04 Feb 2010 12:29:04 GMT Age: 28 X-Cache: HIT from sy32-21.sina.com.cn Connection: close 如果服务器端资源已经更新的话，就返回正常的响应。 持久连接 我们知道 HTTP 协议采用“请求-应答”模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP 协议为无连接的协议）；当使用 Keep-Alive 模式（又称持久连接、连接重用）时，Keep-Alive 功能使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive 功能避免了建立或者重新建立连接。 在 HTTP 1.0 版本中，并没有官方的标准来规定 Keep-Alive 如何工作，因此实际上它是被附加到 HTTP 1.0协议上，如果客户端浏览器支持 Keep-Alive ，那么就在HTTP请求头中添加一个字段 Connection: Keep-Alive，当服务器收到附带有 Connection: Keep-Alive 的请求时，它也会在响应头中添加一个同样的字段来使用 Keep-Alive 。这样一来，客户端和服务器之间的HTTP连接就会被保持，不会断开（超过 Keep-Alive 规定的时间，意外断电等情况除外），当客户端发送另外一个请求时，就使用这条已经建立的连接。 在 HTTP 1.1 版本中，默认情况下所有连接都被保持，如果加入 \"Connection: close\" 才关闭。目前大部分浏览器都使用 HTTP 1.1 协议，也就是说默认都会发起 Keep-Alive 的连接请求了，所以是否能完成一个完整的 Keep-Alive 连接就看服务器设置情况。 由于 HTTP 1.0 没有官方的 Keep-Alive 规范，并且也已经基本被淘汰，以下讨论均是针对 HTTP 1.1 标准中的 Keep-Alive 展开的。 注意： HTTP Keep-Alive 简单说就是保持当前的TCP连接，避免了重新建立连接。 HTTP 长连接不可能一直保持，例如 Keep-Alive: timeout=5, max=100，表示这个TCP通道可以保持5秒，max=100，表示这个长连接最多接收100次请求就断开。 HTTP 是一个无状态协议，这意味着每个请求都是独立的，Keep-Alive 没能改变这个结果。另外，Keep-Alive也不能保证客户端和服务器之间的连接一定是活跃的，在 HTTP1.1 版本中也如此。唯一能保证的就是当连接被关闭时你能得到一个通知，所以不应该让程序依赖于 Keep-Alive 的保持连接特性，否则会有意想不到的后果。 使用长连接之后，客户端、服务端怎么知道本次传输结束呢？两部分：1. 判断传输数据是否达到了Content-Length 指示的大小；2. 动态生成的文件没有 Content-Length ，它是分块传输（chunked），这时候就要根据 chunked 编码来判断，chunked 编码的数据在最后有一个空 chunked 块，表明本次传输数据结束，详见这里。什么是 chunked 分块传输呢？下面我们就来介绍一下。 Transfer-Encoding Transfer-Encoding 是一个用来标示 HTTP 报文传输格式的头部值。尽管这个取值理论上可以有很多，但是当前的 HTTP 规范里实际上只定义了一种传输取值——chunked。 如果一个HTTP消息（请求消息或应答消息）的Transfer-Encoding消息头的值为chunked，那么，消息体由数量未定的块组成，并以最后一个大小为0的块为结束。 每一个非空的块都以该块包含数据的字节数（字节数以十六进制表示）开始，跟随一个CRLF （回车及换行），然后是数据本身，最后块CRLF结束。在一些实现中，块大小和CRLF之间填充有白空格（0x20）。 最后一块是单行，由块大小（0），一些可选的填充白空格，以及CRLF。最后一块不再包含任何数据，但是可以发送可选的尾部，包括消息头字段。消息最后以CRLF结尾。 一个示例响应如下： HTTP/1.1 200 OK Content-Type: text/plain Transfer-Encoding: chunked 25 This is the data in the first chunk 1A and this is the second one 0 注意： chunked 和 multipart 两个名词在意义上有类似的地方，不过在 HTTP 协议当中这两个概念则不是一个类别的。multipart 是一种 Content-Type，标示 HTTP 报文内容的类型，而 chunked 是一种传输格式，标示报头将以何种方式进行传输。 chunked 传输不能事先知道内容的长度，只能靠最后的空 chunk 块来判断，因此对于下载请求来说，是没有办法实现进度的。在浏览器和下载工具中，偶尔我们也会看到有些文件是看不到下载进度的，即采用 chunked 方式进行下载。 chunked 的优势在于，服务器端可以边生成内容边发送，无需事先生成全部的内容。HTTP/2 不支持 Transfer-Encoding: chunked，因为 HTTP/2 有自己的 streaming 传输方式（Source：MDN - Transfer-Encoding）。 HTTP Pipelining（HTTP 管线化） 默认情况下 HTTP 协议中每个传输层连接只能承载一个 HTTP 请求和响应，浏览器会在收到上一个请求的响应之后，再发送下一个请求。在使用持久连接的情况下，某个连接上消息的传递类似于请求1 -> 响应1 -> 请求2 -> 响应2 -> 请求3 -> 响应3。 HTTP Pipelining（管线化）是将多个 HTTP 请求整批提交的技术，在传送过程中不需等待服务端的回应。使用 HTTP Pipelining 技术之后，某个连接上的消息变成了类似这样请求1 -> 请求2 -> 请求3 -> 响应1 -> 响应2 -> 响应3。 注意下面几点： 管线化机制通过持久连接（persistent connection）完成，仅 HTTP/1.1 支持此技术（HTTP/1.0不支持） 只有 GET 和 HEAD 请求可以进行管线化，而 POST 则有所限制 初次创建连接时不应启动管线机制，因为对方（服务器）不一定支持 HTTP/1.1 版本的协议 管线化不会影响响应到来的顺序，如上面的例子所示，响应返回的顺序并未改变 HTTP /1.1 要求服务器端支持管线化，但并不要求服务器端也对响应进行管线化处理，只是要求对于管线化的请求不失败即可 由于上面提到的服务器端问题，开启管线化很可能并不会带来大幅度的性能提升，而且很多服务器端和代理程序对管线化的支持并不好，因此现代浏览器如 Chrome 和 Firefox 默认并未开启管线化支持 更多关于 HTTP Pipelining 的知识可以参考这里。 会话跟踪 什么是会话？ 客户端打开与服务器的连接发出请求到服务器响应客户端请求的全过程称之为会话。 什么是会话跟踪？ 会话跟踪指的是对同一个用户对服务器的连续的请求和接受响应的监视。 为什么需要会话跟踪？ 浏览器与服务器之间的通信是通过HTTP协议进行通信的，而HTTP协议是”无状态”的协议，它不能保存客户的信息，即一次响应完成之后连接就断开了，下一次的请求需要重新连接，这样就需要判断是否是同一个用户，所以才有会话跟踪技术来实现这种要求。 会话跟踪常用的方法: URL 重写 URL(统一资源定位符)是Web上特定页面的地址，URL重写的技术就是在URL结尾添加一个附加数据以标识该会话,把会话ID通过URL的信息传递过去，以便在服务器端进行识别不同的用户。 隐藏表单域 将会话ID添加到HTML表单元素中提交到服务器，此表单元素并不在客户端显示 Cookie Cookie 是Web 服务器发送给客户端的一小段信息，客户端请求时可以读取该信息发送到服务器端，进而进行用户的识别。对于客户端的每次请求，服务器都会将 Cookie 发送到客户端,在客户端可以进行保存,以便下次使用。 客户端可以采用两种方式来保存这个 Cookie 对象，一种方式是保存在客户端内存中，称为临时 Cookie，浏览器关闭后这个 Cookie 对象将消失。另外一种方式是保存在客户机的磁盘上，称为永久 Cookie。以后客户端只要访问该网站，就会将这个 Cookie 再次发送到服务器上，前提是这个 Cookie 在有效期内，这样就实现了对客户的跟踪。 Cookie 是可以被客户端禁用的。 Session: 每一个用户都有一个不同的 session，各个用户之间是不能共享的，是每个用户所独享的，在 session 中可以存放信息。 在服务器端会创建一个 session 对象，产生一个 sessionID 来标识这个 session 对象，然后将这个 sessionID 放入到 Cookie 中发送到客户端，下一次访问时，sessionID 会发送到服务器，在服务器端进行识别不同的用户。 Session 的实现依赖于 Cookie，如果 Cookie 被禁用，那么 session 也将失效。 跨站攻击 CSRF（Cross-site request forgery，跨站请求伪造） CSRF(XSRF) 顾名思义，是伪造请求，冒充用户在站内的正常操作。 例如，一论坛网站的发贴是通过 GET 请求访问，点击发贴之后 JS 把发贴内容拼接成目标 URL 并访问： http://example.com/bbs/create_post.php?title=标题&content=内容 那么，我们只需要在论坛中发一帖，包含一链接： http://example.com/bbs/create_post.php?title=我是脑残&content=哈哈 只要有用户点击了这个链接，那么他们的帐户就会在不知情的情况下发布了这一帖子。可能这只是个恶作剧，但是既然发贴的请求可以伪造，那么删帖、转帐、改密码、发邮件全都可以伪造。 如何防范 CSRF 攻击？可以注意以下几点： 关键操作只接受 POST 请求 验证码 CSRF 攻击的过程，往往是在用户不知情的情况下构造网络请求。所以如果使用验证码，那么每次操作都需要用户进行互动，从而简单有效的防御了CSRF攻击。 但是如果你在一个网站作出任何举动都要输入验证码会严重影响用户体验，所以验证码一般只出现在特殊操作里面，或者在注册时候使用。 检测 Referer 常见的互联网页面与页面之间是存在联系的，比如你在 www.baidu.com 应该是找不到通往www.google.com 的链接的，再比如你在论坛留言，那么不管你留言后重定向到哪里去了，之前的那个网址一定会包含留言的输入框，这个之前的网址就会保留在新页面头文件的 Referer 中 通过检查 Referer 的值，我们就可以判断这个请求是合法的还是非法的，但是问题出在服务器不是任何时候都能接受到 Referer 的值，所以 Referer Check 一般用于监控 CSRF 攻击的发生，而不用来抵御攻击。 Token 目前主流的做法是使用 Token 抵御 CSRF 攻击。下面通过分析 CSRF 攻击来理解为什么 Token 能够有效 CSRF 攻击要成功的条件在于攻击者能够预测所有的参数从而构造出合法的请求。所以根据不可预测性原则，我们可以对参数进行加密从而防止 CSRF 攻击。 另一个更通用的做法是保持原有参数不变，另外添加一个参数 Token，其值是随机的。这样攻击者因为不知道 Token 而无法构造出合法的请求进行攻击。 Token 使用原则 Token 要足够随机————只有这样才算不可预测 Token 是一次性的，即每次请求成功后要更新Token————这样可以增加攻击难度，增加预测难度 Token 要注意保密性————敏感操作使用 post，防止 Token 出现在 URL 中 注意：过滤用户输入的内容不能阻挡 csrf，我们需要做的是过滤请求的来源。 XSS（Cross Site Scripting，跨站脚本攻击） XSS 全称“跨站脚本”，是注入攻击的一种。其特点是不对服务器端造成任何伤害，而是通过一些正常的站内交互途径，例如发布评论，提交含有 JavaScript 的内容文本。这时服务器端如果没有过滤或转义掉这些脚本，作为内容发布到了页面上，其他用户访问这个页面的时候就会运行这些脚本。 运行预期之外的脚本带来的后果有很多中，可能只是简单的恶作剧——一个关不掉的窗口： while (true) { alert(\"你关不掉我~\"); } 也可以是盗号或者其他未授权的操作。 XSS 是实现 CSRF 的诸多途径中的一条，但绝对不是唯一的一条。一般习惯上把通过 XSS 来实现的 CSRF 称为 XSRF。 如何防御 XSS 攻击？ 理论上，所有可输入的地方没有对输入数据进行处理的话，都会存在 XSS 漏洞，漏洞的危害取决于攻击代码的威力，攻击代码也不局限于 script。防御 XSS 攻击最简单直接的方法，就是过滤用户的输入。 如果不需要用户输入 HTML，可以直接对用户的输入进行 HTML escape 。下面一小段脚本： window.location.href=”http://www.baidu.com”; 经过 escape 之后就成了： &lt;script&gt;window.location.href=&quot;http://www.baidu.com&quot;&lt;/script&gt; 它现在会像普通文本一样显示出来，变得无毒无害，不能执行了。 当我们需要用户输入 HTML 的时候，需要对用户输入的内容做更加小心细致的处理。仅仅粗暴地去掉 script 标签是没有用的，任何一个合法 HTML 标签都可以添加 onclick 一类的事件属性来执行 JavaScript。更好的方法可能是，将用户的输入使用 HTML 解析库进行解析，获取其中的数据。然后根据用户原有的标签属性，重新构建 HTML 元素树。构建的过程中，所有的标签、属性都只从白名单中拿取。 参考资料 浅谈HTTP中Get与Post的区别 http请求与http响应详细解析 HTTP 条件 Get (Conditional Get) HTTP中的长连接与短连接 HTTP Keep-Alive模式 分块传输编码 HTTP 管线化(HTTP pipelining) HTTP协议及其POST与GET操作差异 & C#中如何使用POST、GET等 四种常见的 POST 提交数据方式 会话跟踪 总结 XSS 与 CSRF 两种跨站攻击 CSRF简单介绍与利用方法 XSS攻击及防御 百度百科：HTTP Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/HTTPS.html":{"url":"basic/network/HTTPS.html","title":"HTTP over SSL/TLS","keywords":"","body":"HTTPS 基本过程 HTTPS 即 HTTP over TLS，是一种在加密信道进行 HTTP 内容传输的协议。 TLS 的早期版本叫做 SSL。SSL 的 1.0, 2.0, 3.0 版本均已经被废弃，出于安全问题考虑广大浏览器也不再对老旧的 SSL 版本进行支持了，因此这里我们就统一使用 TLS 名称了。 TLS 的基本过程如下（取自 what-happens-when-zh_CN）： 客户端发送一个 ClientHello 消息到服务器端，消息中同时包含了它的 Transport Layer Security (TLS) 版本，可用的加密算法和压缩算法。 服务器端向客户端返回一个 ServerHello 消息，消息中包含了服务器端的 TLS 版本，服务器所选择的加密和压缩算法，以及数字证书认证机构（Certificate Authority，缩写 CA）签发的服务器公开证书，证书中包含了公钥。客户端会使用这个公钥加密接下来的握手过程，直到协商生成一个新的对称密钥。证书中还包含了该证书所应用的域名范围（Common Name，简称 CN），用于客户端验证身份。 客户端根据自己的信任 CA 列表，验证服务器端的证书是否可信。如果认为可信（具体的验证过程在下一节讲解），客户端会生成一串伪随机数，使用服务器的公钥加密它。这串随机数会被用于生成新的对称密钥 服务器端使用自己的私钥解密上面提到的随机数，然后使用这串随机数生成自己的对称主密钥 客户端发送一个 Finished 消息给服务器端，使用对称密钥加密这次通讯的一个散列值 服务器端生成自己的 hash 值，然后解密客户端发送来的信息，检查这两个值是否对应。如果对应，就向客户端发送一个 Finished 消息，也使用协商好的对称密钥加密 从现在开始，接下来整个 TLS 会话都使用对称秘钥进行加密，传输应用层（HTTP）内容 从上面的过程可以看到，TLS 的完整过程需要三个算法（协议），密钥交互算法，对称加密算法，和消息认证算法（TLS 的传输会使用 MAC(message authentication code) 进行完整性检查）。 我们以 Github 网站使用的 TLS 为例，使用浏览器可以看到它使用的加密为 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256。其中密钥交互算法是 ECDHE_RSA，对称加密算法是 AES_128_GCM，消息认证（MAC）算法为 SHA256。 TLS 证书机制 HTTPS 过程中很重要的一个步骤，是服务器需要有 CA 颁发的证书，客户端根据自己的信任 CA 列表验证服务器的身份。现代浏览器中，证书验证的过程依赖于证书信任链。 所谓证书信任链，即一个证书要依靠上一级证书来证明自己是可信的，最顶层的证书被称为根证书，拥有根证书的机构被称为根 CA。 还是以 Github 为例，在浏览器中我们可以看到它的证书信任链如下： DigiCert High Assurance EV Root CA -> DigiCert SHA2 Extended Validation Server CA -> Github.com 从上到下即 Root CA -> 二级 CA -> 网站。 前面提到，证书当中包括 CN(Common Name)，浏览器在验证证书的同时，也会验证 CN 的正确性。即不光需要验证“这是一个合法的证书”，还需要验证“这是一个用于 Github.com 的证书”。 既然所有的信任，最终要落到根 CA 上，根证书本身又是怎么获得的呢？答案也很简单，根证书一般是操作系统自带的。不管是桌面系统 Windows，macOS 还是移动端系统 Android, iOS 都会内置一系列根证书。随着操作系统本身的升级，根证书也会随着升级进行更新。 对浏览器而已，浏览器当然也有选择信任某个根证书的权利。Chrome 浏览器一般是跟随系统根证书信任的。Firefox 浏览器通常是使用自带的一套证书信任机制，不受系统证书的影响。 在使用 curl 等工具时，我们还可以自行选择证书进行信任。 有权威的信任，最终都要落到一个单点信任，不管是 Root CA，还是微软，苹果，谷歌等操作系统厂商。 中间人攻击 HTTPS 的过程并不是密不透风的，HTTPS 有若干漏洞，给中间人攻击（Man In The Middle Attack，简称 MITM）提供了可能。 所谓中间人攻击，指攻击者与通讯的两端分别建立独立的联系，并交换其所收到的数据，使通讯的两端认为他们正在通过一个私密的连接与对方直接对话，但事实上整个会话都被攻击者完全控制。在中间人攻击中，攻击者可以拦截通讯双方的通话并插入新的内容。 SSL 剥离 SSL 剥离即阻止用户使用 HTTPS 访问网站。由于并不是所有网站都只支持 HTTPS，大部分网站会同时支持 HTTP 和 HTTPS 两种协议。用户在访问网站时，也可能会在地址栏中输入 http:// 的地址，第一次的访问完全是明文的，这就给了攻击者可乘之机。通过攻击 DNS 响应，攻击者可以将自己变成中间人。 DNS 作为基于 UDP 的协议是相当不安全的，为了保证 DNS 的安全可以使用 DNS over TCP 等机制，这里不赘述了。 HSTS 为了防止上面说的这种情况，一种叫做 HSTS 的技术被引入了。HSTS（HTTP Strict Transport Security）是用于强制浏览器使用 HTTPS 访问网站的一种机制。它的基本机制是在服务器返回的响应中，加上一个特殊的头部，指示浏览器对于此网站，强制使用 HTTPS 进行访问： Strict-Transport-Security: max-age=31536000; includeSubdomains; preload 可以看到如果这个过期时间非常长，就是导致在很长一段时间内，浏览器都会强制使用 HTTPS 访问该网站。 HSTS 有一个很明显的缺点，是需要等待第一个服务器的影响中的头部才能生效，但如果第一次访问该网站就被攻击呢？为了解决这个问题，浏览器中会带上一些网站的域名，被称为 HSTS preload list。对于在这个 list 的网站来说，直接强制使用 HTTPS。 伪造证书攻击 HSTS 只解决了 SSL 剥离的问题，然而即使在全程使用 HTTPS 的情况下，我们仍然有可能被监听。 假设我们想访问 www.google.com，但我们的 DNS 服务器被攻击了，指向的 IP 地址并非 Google 的服务器，而是攻击者的 IP。当攻击者的服务器也有合法的证书的时候，我们的浏览器就会认为对方是 Google 服务器，从而信任对方。这样，攻击者便可以监听我们和谷歌之前的所有通信了。 可以看到攻击者有两步需要操作，第一步是需要攻击 DNS 服务器。第二步是攻击者自己的证书需要被用户信任，这一步对于用户来说是很难控制的，需要证书颁发机构能够控制自己不滥发证书。 2015 年 Google 称发现赛门铁克旗下的 Thawte 未经同意签发了众多域名的数千个证书，其中包括 Google 旗下的域名和不存在的域名。当年 12 月，Google 发布公告称 Chrome、Android 及其他 Google 产品将不再信任赛门铁克旗下的\"Class 3 Public Primary CA\"根证书。 2016 年 Mozilla 发现沃通 CA 存在严重的信任问题，例如偷签 github.com 的证书，故意倒填证书日期绕过浏览器对 SHA-1 证书的限制等，将停止信任 WoSign 和 StartCom 签发的新证书。 HPKP HPKP 技术是为了解决伪造证书攻击而诞生的。 HPKP（Public Key Pinning Extension for HTTP）在 HSTS 上更进一步，HPKP 直接在返回头中存储服务器的公钥指纹信息，一旦发现指纹和实际接受到的公钥有差异，浏览器就可以认为正在被攻击： Public-Key-Pins: pin-sha256=\"base64==\"; max-age=expireTime [; includeSubDomains][; report-uri=\"reportURI\"] 和 HSTS 类似，HPKP 也依赖于服务器的头部返回，不能解决第一次访问的问题，浏览器本身也会内置一些 HPKP 列表。 HPKP 技术仍然不能阻止第一次访问的攻击问题，部署和配置 HPKP 相当繁琐，一旦网站配置错误，就会导致网站证书验证失败，且在过期时间内无法有效恢复。HPKP 的机制也引来了一些安全性问题。Chrome 67 中废除了对 HPKP 的支持，在 Chrome 72 中 HPKP 被彻底移除。 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/TCP.html":{"url":"basic/network/TCP.html","title":"TCP 协议","keywords":"","body":"TCP 的特性 TCP 提供一种面向连接的、可靠的字节流服务 在一个 TCP 连接中，仅有两方进行彼此通信。广播和多播不能用于 TCP TCP 使用校验和，确认和重传机制来保证可靠传输 TCP 给数据分节进行排序，并使用累积确认保证数据的顺序不变和非重复 TCP 使用滑动窗口机制来实现流量控制，通过动态改变窗口的大小进行拥塞控制 注意：TCP 并不能保证数据一定会被对方接收到，因为这是不可能的。TCP 能够做到的是，如果有可能，就把数据递送到接收方，否则就（通过放弃重传并且中断连接这一手段）通知用户。因此准确说 TCP 也不是 100% 可靠的协议，它所能提供的是数据的可靠递送或故障的可靠通知。 三次握手与四次挥手 所谓三次握手(Three-way Handshake)，是指建立一个 TCP 连接时，需要客户端和服务器总共发送3个包。 三次握手的目的是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。在 socket 编程中，客户端执行 connect() 时。将触发三次握手。 第一次握手(SYN=1, seq=x): 客户端发送一个 TCP 的 SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号 X,保存在包头的序列号(Sequence Number)字段里。 发送完毕后，客户端进入 SYN_SEND 状态。 第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1): 服务器发回确认包(ACK)应答。即 SYN 标志位和 ACK 标志位均为1。服务器端选择自己 ISN 序列号，放到 Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的 ISN 加1，即X+1。 发送完毕后，服务器端进入 SYN_RCVD 状态。 第三次握手(ACK=1，ACKnum=y+1) 客户端再次发送确认包(ACK)，SYN 标志位为0，ACK 标志位为1，并且把服务器发来 ACK 的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN的+1 发送完毕后，客户端进入 ESTABLISHED 状态，当服务器端接收到这个包时，也进入 ESTABLISHED 状态，TCP 握手结束。 三次握手的过程的示意图如下： TCP 的连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，也叫做改进的三次握手。客户端或服务器均可主动发起挥手动作，在 socket 编程中，任何一方执行 close() 操作即可产生挥手操作。 第一次挥手(FIN=1，seq=x) 假设客户端想要关闭连接，客户端发送一个 FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。 发送完毕后，客户端进入 FIN_WAIT_1 状态。 第二次挥手(ACK=1，ACKnum=x+1) 服务器端确认客户端的 FIN 包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。 发送完毕后，服务器端进入 CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入 FIN_WAIT_2 状态，等待服务器端关闭连接。 第三次挥手(FIN=1，seq=y) 服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN 置为1。 发送完毕后，服务器端进入 LAST_ACK 状态，等待来自客户端的最后一个ACK。 第四次挥手(ACK=1，ACKnum=y+1) 客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 TIME_WAIT状态，等待可能出现的要求重传的 ACK 包。 服务器端接收到这个确认包之后，关闭连接，进入 CLOSED 状态。 客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 CLOSED 状态。 四次挥手的示意图如下： TCP 协议如何保证可靠传输 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP使用的流量控制协议是可变大小的滑动窗口协议。（TCP利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 停止等待协议 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 拥塞控制和流量控制不同，拥塞控制是一个全局性的过程，流量控制指点对点通信量的控制。 TCP的流量控制 (滑动窗口) TCP 采用大小可变的滑动窗口进行流量控制(窗口大小的单位是字节),在TCP报文段首部的窗口字段写入的数值就是当前给对方设置的发送窗口数值的上限。发送窗口在连接建立时由双方商定。但在通信的过程中,接收端可根据自己的资源情况,随时动态地调整对方的发送窗口上限值(可增大或减小)。 控制过程: 在通信过程中,接收方根据自己接收缓存的大小,动态地调整发送方的发送窗口大小,这就是接收窗口rwnd,即调整TCP报文段首部中的“窗口”字段值,来限制发送方向网络注入报文的速率。同时,发送方根据其对当前网络拥塞程序的估计而确定的窗口值,称为拥塞窗口cwnd,其大小与网络的带宽和时延密切相关。发送窗口的实际大小是取rwnd和cwnd中的最小值。 TCP阻塞控制 在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就会变坏，这种情况就叫做拥塞。 拥塞控制就是 防止过多的数据注入网络中，这样可以使网络中的路由器或链路不致过载。 拥塞控制的方法主要有以下四种： 1.慢启动：不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小 2.拥塞避免： 3.快重传：网络拥塞时的处理 4.快恢复：网络拥塞时的处理，快重传配合使用的还有快恢复算法 正向代理和反向代理 正向代理就是我们日常用的代理，通过代理我们本机去访问服务器。 反向代理刚好相反，客户端是感觉不到反向代理的存在的。反向代理代理服务器，代替服务器接收访问请求，然后从服务器取到内容，再返还回给客户端。反向代理可以用于实现负载均衡，保证内网安全等。 SYN攻击 什么是 SYN 攻击（SYN Flood）？ 在三次握手过程中，服务器发送 SYN-ACK 之后，收到客户端的 ACK 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 SYN_RCVD 状态。当收到 ACK 后，服务器才能转入 ESTABLISHED 状态. SYN 攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。 SYN 攻击是一种典型的 DoS/DDoS 攻击。 如何检测 SYN 攻击？ 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。 如何防御 SYN 攻击？ SYN攻击不能完全被阻止，除非将TCP协议重新设计。我们所做的是尽可能的减轻SYN攻击的危害，常见的防御 SYN 攻击的方法有如下几种： 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术 TCP KeepAlive TCP 的连接，实际上是一种纯软件层面的概念，在物理层面并没有“连接”这种概念。TCP 通信双方建立交互的连接，但是并不是一直存在数据交互，有些连接会在数据交互完毕后，主动释放连接，而有些不会。在长时间无数据交互的时间段内，交互双方都有可能出现掉电、死机、异常重启等各种意外，当这些意外发生之后，这些 TCP 连接并未来得及正常释放，在软件层面上，连接的另一方并不知道对端的情况，它会一直维护这个连接，长时间的积累会导致非常多的半打开连接，造成端系统资源的消耗和浪费，为了解决这个问题，在传输层可以利用 TCP 的 KeepAlive 机制实现来实现。主流的操作系统基本都在内核里支持了这个特性。 TCP KeepAlive 的基本原理是，隔一段时间给连接对端发送一个探测包，如果收到对方回应的 ACK，则认为连接还是存活的，在超过一定重试次数之后还是没有收到对方的回应，则丢弃该 TCP 连接。 TCP-Keepalive-HOWTO 有对 TCP KeepAlive 特性的详细介绍，有兴趣的同学可以参考。这里主要说一下，TCP KeepAlive 的局限。首先 TCP KeepAlive 监测的方式是发送一个 probe 包，会给网络带来额外的流量，另外 TCP KeepAlive 只能在内核层级监测连接的存活与否，而连接的存活不一定代表服务的可用。例如当一个服务器 CPU 进程服务器占用达到 100%，已经卡死不能响应请求了，此时 TCP KeepAlive 依然会认为连接是存活的。因此 TCP KeepAlive 对于应用层程序的价值是相对较小的。需要做连接保活的应用层程序，例如 QQ，往往会在应用层实现自己的心跳功能。 参考资料 计算机网络：自顶向下方法 TCP三次握手及四次挥手详细图解 TCP协议三次握手过程分析 TCP协议中的三次握手和四次挥手(图解) 百度百科：SYN攻击 TCP-Keepalive-HOWTO Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/UDP.html":{"url":"basic/network/UDP.html","title":"UDP 协议","keywords":"","body":"UDP 简介 UDP 是一个简单的传输层协议。和 TCP 相比，UDP 有下面几个显著特性： UDP 缺乏可靠性。UDP 本身不提供确认，序列号，超时重传等机制。UDP 数据报可能在网络中被复制，被重新排序。即 UDP 不保证数据报会到达其最终目的地，也不保证各个数据报的先后顺序，也不保证每个数据报只到达一次 UDP 数据报是有长度的。每个 UDP 数据报都有长度，如果一个数据报正确地到达目的地，那么该数据报的长度将随数据一起传递给接收方。而 TCP 是一个字节流协议，没有任何（协议上的）记录边界。 UDP 是无连接的。UDP 客户和服务器之前不必存在长期的关系。UDP 发送数据报之前也不需要经过握手创建连接的过程。 UDP 支持多播和广播。 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/IP.html":{"url":"basic/network/IP.html","title":"IP 协议","keywords":"","body":"IP 协议简介 IP 协议位于 TCP/IP 协议的第三层——网络层。与传输层协议相比，网络层的责任是提供点到点(hop by hop)的服务，而传输层（TCP/UDP）则提供端到端(end to end)的服务。 IP路由表 IP路由表通常包括三项内容，他们是子网掩码、目的网络地址、到目的网络路径上“下一个”路由器的地址。 IP 地址的分类 A类地址 B类地址 C类地址 D 类地址 广播与多播 广播和多播仅用于UDP（TCP是面向连接的）。 广播 一共有四种广播地址： 受限的广播 受限的广播地址为255.255.255.255。该地址用于主机配置过程中IP数据报的目的地址，在任何情况下，router不转发目的地址为255.255.255.255的数据报，这样的数据报仅出现在本地网络中。 指向网络的广播 指向网络的广播地址是主机号为全1的地址。A类网络广播地址为netid.255.255.255，其中netid为A类网络的网络号。 一个router必须转发指向网络的广播，但它也必须有一个不进行转发的选择。 指向子网的广播 指向子网的广播地址为主机号为全1且有特定子网号的地址。作为子网直接广播地址的IP地址需要了解子网的掩码。例如，router收到128.1.2.255的数据报，当B类网路128.1的子网掩码为255.255.255.0时，该地址就是指向子网的广播地址；但是如果子网掩码为255.255.254.0，该地址就不是指向子网的广播地址。 指向所有子网的广播 指向所有子网的广播也需要了解目的网络的子网掩码，以便与指向网络的广播地址区分开来。指向所有子网的广播地址的子网号和主机号为全1.例如，如果子网掩码为255.255.255.0，那么128.1.255.255就是一个指向所有子网的广播地址。 当前的看法是这种广播是陈旧过时的，更好的方式是使用多播而不是对所有子网的广播。 广播示例: PING 192.168.0.255 (192.168.0.255): 56 data bytes 64 bytes from 192.168.0.107: icmp_seq=0 ttl=64 time=0.199 ms 64 bytes from 192.168.0.106: icmp_seq=0 ttl=64 time=45.357 ms 64 bytes from 192.168.0.107: icmp_seq=1 ttl=64 time=0.203 ms 64 bytes from 192.168.0.106: icmp_seq=1 ttl=64 time=269.475 ms 64 bytes from 192.168.0.107: icmp_seq=2 ttl=64 time=0.102 ms 64 bytes from 192.168.0.106: icmp_seq=2 ttl=64 time=189.881 ms 可以看到的确收到了来自两个主机的答复，其中 192.168.0.107 是本机地址。 多播 多播又叫组播，使用D类地址，D类地址分配的28bit均用作多播组号而不再表示其他。 多播组地址包括1110的最高4bit和多播组号。它们通常可以表示为点分十进制数，范围从224.0.0.0到239.255.255.255。 多播的出现减少了对应用不感兴趣主机的处理负荷。 多播的特点： 允许一个或多个发送者（组播源）发送单一的数据包到多个接收者（一次的，同时的）的网络技术 可以大大的节省网络带宽，因为无论有多少个目标地址，在整个网络的任何一条链路上只传送单一的数据包 多播技术的核心就是针对如何节约网络资源的前提下保证服务质量。 多播示例： PING 224.0.0.1 (224.0.0.1): 56 data bytes 64 bytes from 192.168.0.107: icmp_seq=0 ttl=64 time=0.081 ms 64 bytes from 192.168.0.106: icmp_seq=0 ttl=64 time=123.081 ms 64 bytes from 192.168.0.107: icmp_seq=1 ttl=64 time=0.122 ms 64 bytes from 192.168.0.106: icmp_seq=1 ttl=64 time=67.312 ms 64 bytes from 192.168.0.107: icmp_seq=2 ttl=64 time=0.132 ms 64 bytes from 192.168.0.106: icmp_seq=2 ttl=64 time=447.073 ms 64 bytes from 192.168.0.107: icmp_seq=3 ttl=64 time=0.132 ms 64 bytes from 192.168.0.106: icmp_seq=3 ttl=64 time=188.800 ms BGP 边界网关协议（BGP）是运行于 TCP 上的一种自治系统的路由协议 BGP 是唯一一个用来处理像因特网大小的网络的协议，也是唯一能够妥善处理好不相关路由域间的多路连接的协议 BGP是一种外部网关协议（Exterior Gateway Protocol，EGP），与OSPF、RIP等内部网关协议（Interior Gateway Protocol，IGP）不同，BGP不在于发现和计算路由，而在于控制路由的传播和选择最佳路由 BGP使用TCP作为其传输层协议（端口号179），提高了协议的可靠性 BGP既不是纯粹的矢量距离协议，也不是纯粹的链路状态协议 BGP支持CIDR（Classless Inter-Domain Routing，无类别域间路由） 路由更新时，BGP只发送更新的路由，大大减少了BGP传播路由所占用的带宽，适用于在Internet上传播大量的路由信息 BGP路由通过携带AS路径信息彻底解决路由环路问题 BGP提供了丰富的路由策略，能够对路由实现灵活的过滤和选择 BGP易于扩展，能够适应网络新的发展 参考资料 多播与广播 TCP_IP：广播和多播 百度百科：BGP Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/sub_net_computing.html":{"url":"basic/network/sub_net_computing.html","title":"子网掩码计算","keywords":"","body":"子网掩码计算 1.将整个地址转换为二进制： 255.255.255.255 -> 11111111.11111111.11111111.11111111 TIP:1为广播，0为不广播 2.计算每个子网中需要包括多少地址，并计算对应二进制的位数，既需要多少位二进制来表示 3. 从后向前，将子网掩码中的1变为0. -> 既 11111111.11111111.11111111.11111111 -> 11111111.11111111.11111111.11110000 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/common_port.html":{"url":"basic/network/common_port.html","title":"常用端口相关","keywords":"","body":"常用端口 1.按端口号分布划分 知名端口（Well-Known Ports） 知名端口即众所周知的端口号，范围从0到1023，这些端口号一般固定分配给一些服务。比如21端口分配给FTP服务，25端口分配给SMTP（简单邮件传输协议）服务，80端口分配给HTTP服务，135端口分配给RPC（远程过程调用）服务等等。 动态端口（Dynamic Ports） 动态端口的范围从1024到65535，这些端口号一般不固定分配给某个服务，也就是说许多服务都可以使用这些端口。只要运行的程序向系统提出访问网络的申请，那么系统就可以从这些端口号中分配一个供该程序使用。比如1024端口就是分配给第一个向系统发出申请的程序。在关闭程序进程后，就会释放所占用的端口号。 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/network/Socket-Programming-Basic.html":{"url":"basic/network/Socket-Programming-Basic.html","title":"Socket 编程","keywords":"","body":"Socket 基本概念 Socket 是对 TCP/IP 协议族的一种封装，是应用层与TCP/IP协议族通信的中间软件抽象层。从设计模式的角度看来，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 Socket 还可以认为是一种网络间不同计算机上的进程通信的一种方法，利用三元组（ip地址，协议，端口）就可以唯一标识网络中的进程，网络中的进程通信可以利用这个标志与其它进程进行交互。 Socket 起源于 Unix ，Unix/Linux 基本哲学之一就是“一切皆文件”，都可以用“打开(open) –> 读写(write/read) –> 关闭(close)”模式来进行操作。因此 Socket 也被处理为一种特殊的文件。 写一个简易的 WebServer 一个简易的 Server 的流程如下： 1.建立连接，接受一个客户端连接。 2.接受请求，从网络中读取一条 HTTP 请求报文。 3.处理请求，访问资源。 4.构建响应，创建带有 header 的 HTTP 响应报文。 5.发送响应，传给客户端。 省略流程 3，大体的程序与调用的函数逻辑如下： socket() 创建套接字 bind() 分配套接字地址 listen() 等待连接请求 accept() 允许连接请求 read()/write() 数据交换 close() 关闭连接 代码如下： #include #include #include #include #include #include #include #include #include using namespace std; const int port = 9090; const int buffer_size = 1Bad Request400 Bad Request\"; write(client_sock, status, sizeof(status)); write(client_sock, header, sizeof(header)); write(client_sock, body, sizeof(body)); } 参考资料 Linux Socket编程 揭开 Socket 编程的面纱 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/algo/":{"url":"basic/algo/","title":"数据结构与算法","keywords":"","body":"推荐资源 网站 LeetCode 力扣(LeetCode 中文) 领扣 LintCode 题解 LeetCode 题解(旧版，只有老题，已不再更新) LeetCode 题解(有新题，还在更新) 图解 LeetCode Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/algo/Array.html":{"url":"basic/algo/Array.html","title":"数组","keywords":"","body":"数据结构 算法 之 Array C++ Array 操作及定义 Python Array 操作及定义 LeetCode 4 解决方案： 个人认为这个解决方案已经很高效了，或许还有更高效的 class Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -> float: # 此处 python 数组融合可以使用 + 完成 num = nums1+nums2; num.sort(); lenth = len(num); if lenth % 2 == 0: return (float((num[int(lenth/2)]+num[int(lenth/2)-1]))/2) else: return (float(num[int(lenth/2)])) C++ 时间是上面python的2/3，内存占用大概是上面Python的8倍，原因是使用Vector会占用大量空间 Vector 时间复杂度o(n)，可以用list,时间复杂度o(1) #include #include #include #include #include using namespace std; class Solution { public: double findMedianSortedArrays(vector& nums1, vector& nums2) { // Vector 的拼接，在 nums1 的后面拼接 nums2 nums1.insert(nums1.end(),nums2.begin(),nums2.end()); sort(nums1.begin(),nums1.end()); int length = nums1.size(); if (length%2 == 1){ //cout C 的一个实现，主要是使用数组完成 void sort(float *a,int len){ int temp; int i,j; for(i = 0; i a[i]) { temp = a[i]; a[i] = a[j]; a[j] = temp; } } } double findMedianSortedArrays(int* nums1, int nums1Size, int* nums2, int nums2Size){ int length1 = nums1Size; int length2 = nums2Size; int new_length = length1+length2; float new_array[new_length]; for (int i = 0; i Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-17 20:49:18 "},"basic/algo/Linked-List.html":{"url":"basic/algo/Linked-List.html","title":"链表 C++","keywords":"","body":"链表 C++ 涉及以下操作 链表定义 插入函数 在指定位置插入 删除指定数据节点 修改指定数据 反转链表 (递归) 打印 获取链表节点个数 待补充 检测是否有环,找到环的入口 循环链表 逆序遍历 单链表排序 在 O(1) 时间删除链表节点 删除单链表倒数第 n 个节点 判断两个无环单链表是否相交 两个链表相交扩展：求两个无环单链表的第一个相交点 两个链表相交扩展：判断两个有环单链表是否相交 // 在链表的后面插入 // 前插 // 在指定位置后插入 // 查看链表长度 // 打印链表 // 删除指定位置元素 // 修改指定位置数据 // 反转链表-递归 LeetCode 206 // 反转链表-非递归 #include using namespace std; struct ListNode{ float data; ListNode *next; }; // 在链表的后面插入 // 指针的引用 使用 *& void insert(struct ListNode *&Node, float num){ ListNode *per = Node; // 这地方的关键是，插入前要把 Node 指针移动到链表的最后，否则就是不停地在第二个节点多次赋值 while (per->next != NULL){ per = per->next; } ListNode *NewNode = new ListNode; NewNode->data = num; NewNode->next = NULL; per->next = NewNode; } // 前插 void insert_forward(ListNode *&Node, float num){ ListNode *temp = new ListNode; temp->data = num; temp->next = Node; Node = temp; } // 在指定位置后插入 void insert_in_loc(ListNode *&Node, int loc, float num){ ListNode *per = Node; for (int i = 0; i next; } ListNode *temp = new ListNode; temp->data = num; temp->next = Node->next; Node->next = temp; Node = per; } // 查看链表长度 int check_length(ListNode *&Node){ // 每一次遍历都需要使用临时指针，遍历完再将头指针给到原来的链表 ListNode *per = Node; int i=0; if (Node != NULL) { do { ++i; // cout data next; }while (Node != NULL); } Node = per; return i; } // 打印链表 void show_list(ListNode *&Node){ ListNode *per = Node; cout data next; } while (Node != NULL); } Node = per; cout next; } else { for (int i = 0; i next; } ListNode *temp = Node->next->next; Node->next = temp; Node = per; } } // 修改指定位置数据 void alert_loc_data(ListNode *&Node, int loc,float num){ ListNode *per = Node; if (loc == 0){ Node->data = num; }else { for (int i = 0; i next; } Node->data = num; Node=per; } } // 反转链表-递归 ListNode* reverseList_recursion(ListNode *&head) { if (head == NULL || head->next == NULL) return head; ListNode *p = reverseList_recursion(head->next); head->next->next = head; head->next = NULL; return p; } // 反转链表-非递归 ListNode* reverseList(ListNode* head) { ListNode* cur = NULL, *pre = head; while (pre != NULL) { ListNode* t = pre->next; pre->next = cur; cur = pre; pre = t; } return cur; } // 融合两个排序链表 ListNode* mergeTwoLists(ListNode *&l1, ListNode *&l2) { if(l1 == NULL) return l2; if(l2 == NULL) return l1; ListNode* merge = NULL; if(l1->valval){ merge = l1; merge->next = mergeTwoLists(l1->next,l2); } else{ merge = l2; merge->next = mergeTwoLists(l1,l2->next); } return merge; } int main() { struct ListNode *Node = nullptr; Node = new ListNode; Node->data = 0; Node->next = NULL; insert_forward(Node, -1); insert(Node, 1); insert(Node, 2); insert(Node, 3); insert_in_loc(Node,3,1.5); delete_loc_node(Node,4); //show_list(Node); alert_loc_data(Node,5,5); show_list(Node); int length = check_length(Node); cout 例题 单链表翻转 LeetCode 206 // 反转链表-递归 ListNode* reverseList_recursion(ListNode *&head) { if (head == NULL || head->next == NULL) return head; ListNode *p = reverseList_recursion(head->next); head->next->next = head; head->next = NULL; return p; } // 反转链表-非递归 ListNode* reverseList(ListNode* head) { ListNode* cur = NULL, *pre = head; while (pre != NULL) { ListNode* t = pre->next; pre->next = cur; cur = pre; pre = t; } return cur; } 单链表判断是否有环 LeetCode 141 最容易想到的思路是存一个所有 Node 地址的 Hash 表，从头开始遍历，将 Node 存到 Hash 表中，如果出现了重复，则说明链表有环。 一个经典的方法是双指针（也叫快慢指针），使用两个指针遍历链表，一个指针一次走一步，另一个一次走两步，如果链表有环，两个指针必然相遇。 双指针算法实现： bool hasCycle(ListNode *head) { if (head == nullptr) { return false; } ListNode *fast,*slow; slow = head; fast = head->next; while (fast && fast->next) { slow = slow->next; fast = fast->next->next; if (slow == fast) { return true; } } return false; } 单链表找环入口 LeetCode 141 作为上一题的扩展，为了找到环所在的位置，在快慢指针相遇的时候，此时慢指针没有遍历完链表，再设置一个指针从链表头部开始遍历，这两个指针相遇的点，就是链表环的入口。 算法实现： ListNode *detectCycle(ListNode *head) { if (head == nullptr) { return nullptr; } ListNode *fast,*slow; slow = head; fast = head; while (fast && fast->next) { slow = slow->next; fast = fast->next->next; if (slow == fast) { ListNode *slow2 = head; while (slow2 != slow) { slow = slow->next; slow2 = slow2->next; } return slow2; } } return nullptr; } 单链表找交点 LeetCode 160 和找环的方法类似，同样可以使用 Hash 表存储所有节点，发现重复的节点即交点。 一个容易想到的方法是，先得到两个链表的长度，然后得到长度的差值 distance，两个指针分别从两个链表头部遍历，其中较长链表指针先走 distance 步，然后同时向后走，当两个指针相遇的时候，即链表的交点： int getListLength(ListNode *head) { if (head == nullptr) { return 0; } int length = 0; ListNode *p = head; while (p!=nullptr) { p = p->next; length ++; } return length; } ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) { int lengthA = getListLength(headA); int lengthB = getListLength(headB); if (lengthA > lengthB) { std::swap(headA, headB); }; int distance = abs(lengthB - lengthA); ListNode *p1 = headA; ListNode *p2 = headB; while(distance--) { p2 = p2->next; } while (p1 != nullptr && p2 != nullptr) { if (p1 == p2) return p1; p1 = p1->next; p2 = p2->next; } return NULL; } 另一个较快的方法时，两个指针 pa，pb 分别从 headA，headB开始遍历，当 pa 遍历到尾部的时候，指向 headB，当 pb 遍历到尾部的时候，转向 headA。当两个指针再次相遇的时候，如果两个链表有交点，则指向交点，如果没有则指向 NULL： ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) { ListNode *pa = headA; ListNode *pb = headB; while (pa != pb) { pa = pa != nullptr ? pa->next : headB; pb = pb != nullptr ? pb->next : headA; } return pa; } 单链表找中间节点 LeetCode 876 用快慢指针法,快指针走两个，慢指针走一个，当快指针走到链表结尾时，慢指针刚好走到链表的中间： ListNode* middleNode(ListNode* head) { ListNode *slow = head; ListNode *fast = head; while (fast && fast->next) { slow = slow->next; fast = fast->next->next; } return slow; } 单链表合并 LeetCode 21 两个链表本身都是排序过的，把两个链表从头节点开始，逐个节点开始进行比较，最后剩下的节点接到尾部： // 融合两个排序链表 ListNode* mergeTwoLists(ListNode *&l1, ListNode *&l2) { if(l1 == NULL) return l2; if(l2 == NULL) return l1; ListNode* merge = NULL; if(l1->valval){ merge = l1; merge->next = mergeTwoLists(l1->next,l2); } else{ merge = l2; merge->next = mergeTwoLists(l1,l2->next); } return merge; } Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-23 14:19:50 "},"basic/algo/Hash-Table.html":{"url":"basic/algo/Hash-Table.html","title":"哈希表","keywords":"","body":"哈希表 哈希表（Hash Table，也叫散列表），是根据关键码值 (Key-Value) 而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。哈希表的实现主要需要解决两个问题，哈希函数和冲突解决。 C++ 中的实现： C++ 中涉及哈希表的算法，通常使用 map 或 unordered_map 实现。 unordered_map的底层是一个防冗余的哈希表（开链法避免地址冲突）。unordered_map的key需要定义hash_value函数并且重载operator ==。 LeetCode 30 一个不成熟的思路，找出 words 在 s 中所有的出现次数，再判断出现次数和 words 的长度，如果绝对值等于 words 的长度，则认定 words 中的词相连，但问题是，如果 words 中的词多于 3 个，则需要使用多维数组，且计算 words 中的词是否相连也很困难，因为相连的词位置可能会变。 此题最好的方法还是 哈希表。 vector findSubstring(string s, vector& words) { vector index; if (s.empty() || words.empty()) return index; unordered_map m1; //words for(auto a : words){ ++m1[a]; } int n = words.size(), m = words[0].size(); for (int i = 0; i m2; for (j = 0; j m1[sub]){break;} } if (j == n){ index.push_back(i); } m2.clear(); // 清空map } return index; } Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-24 16:59:27 "},"basic/algo/Tree.html":{"url":"basic/algo/Tree.html","title":"树","keywords":"","body":"基本知识 目录 二叉树 二叉树的性质 满二叉树 完全二叉树 树类搜索算法 二叉树的存储结构 堆 霍夫曼树 二叉排序树 平衡二叉树 B-树 Trie 树 - 字典树 字典树-待补充 红黑树-待补充 并查集 二叉树基本计算公式 LeetCode 例题 二叉树 二叉树 二叉树是有限个结点的集合，这个集合或者是空集，或者是由一个根结点和两株互不相交的二叉树组成，其中一株叫根的做左子树，另一棵叫做根的右子树。 二叉树的性质 满二叉树 深度为k且有2^k －1个结点的二叉树称为满二叉树 完全二叉树 深度为 k 的，有n个结点的二叉树，当且仅当其每个结点都与深度为 k 的满二叉树中编号从 1 至 n 的结点一一对应，称之为完全二叉树。（除最后一层外，每一层上的节点数均达到最大值；在最后一层上只缺少右边的若干结点） 性质4：具有 n 个结点的完全二叉树的深度为 log2n + 1 注意 仅有前序和后序遍历，不能确定一个二叉树，必须有中序遍历的结果 树类搜索算法 参考 一般来说就是深度优先搜索,广度优先搜索,A搜索,IDA搜索等几种，通常用的最多就是DFS和BFS。 二叉树的存储结构 堆 如果一棵完全二叉树的任意一个非终端结点的元素都不小于其左儿子结点和右儿子结点（如果有的话） 的元素，则称此完全二叉树为最大堆。 同样，如果一棵完全二叉树的任意一个非终端结点的元素都不大于其左儿子结点和右儿子结点（如果 有的话）的元素，则称此完全二叉树为最小堆。 最大堆的根结点中的元素在整个堆中是最大的； 最小堆的根结点中的元素在整个堆中是最小的。 霍夫曼树 定义：给定n个权值作为n的叶子结点，构造一棵二叉树，若带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman tree)。 构造： 假设有n个权值，则构造出的哈夫曼树有n个叶子结点。 n个权值分别设为 w1、w2、…、wn，则哈夫曼树的构造规则为： 将w1、w2、…，wn看成是有 n 棵树的森林(每棵树仅有一个结点)； 在森林中选出两个根结点的权值最小的树合并，作为一棵新树的左、右子树，且新树的根结点权值为其左、右子树根结点权值之和； 从森林中删除选取的两棵树，并将新树加入森林； 重复(2)、(3)步，直到森林中只剩一棵树为止，该树即为所求得的哈夫曼树。 二叉排序树 二叉排序树（Binary Sort Tree）又称二叉查找树（Binary Search Tree），亦称二叉搜索树。 二叉排序树或者是一棵空树，或者是具有下列性质的二叉树： 若左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值； 左、右子树也分别为二叉排序树； 没有键值相等的节点 二分查找的时间复杂度是O(log(n))，最坏情况下的时间复杂度是O(n)（相当于顺序查找） 平衡二叉树 平衡二叉树（balanced binary tree）,又称 AVL 树。它或者是一棵空树,或者是具有如下性质的二叉树： 它的左子树和右子树都是平衡二叉树， 左子树和右子树的深度之差的绝对值不超过1。 平衡二叉树是对二叉搜索树(又称为二叉排序树)的一种改进。二叉搜索树有一个缺点就是，树的结构是无法预料的，随意性很大，它只与节点的值和插入的顺序有关系，往往得到的是一个不平衡的二叉树。在最坏的情况下，可能得到的是一个单支二叉树，其高度和节点数相同，相当于一个单链表，对其正常的时间复杂度有O(log(n))变成了O(n)，从而丧失了二叉排序树的一些应该有的优点。 B-树 B-树：B-树是一种非二叉的查找树， 除了要满足查找树的特性，还要满足以下结构特性： 一棵 m 阶的B-树： 树的根或者是一片叶子(一个节点的树),或者其儿子数在 2 和 m 之间。 除根外，所有的非叶子结点的孩子数在 m/2 和 m 之间。 所有的叶子结点都在相同的深度。 B-树的平均深度为logm/2(N)。执行查找的平均时间为O(logm)； Trie 树 - 字典树 Trie 树，又称前缀树，字典树， 是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。 Trie 树查询和插入时间复杂度都是 O(n)，是一种以空间换时间的方法。当节点树较多的时候，Trie 树占用的内存会很大。 Trie 树常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能。 并查集 参考1 参考2 参考3 二叉树基本计算公式 例题 二叉树的遍历 二叉树前中后序遍历 二叉树的前中后序遍历，使用递归算法实现最为简单，以前序遍历（LeetCode 144）为例： void preorder(TreeNode *p, vector& result) { if (p == NULL) { return; } result.push_back(p->val); preorder(p->left, result); preorder(p->right, result); } vector preorderTraversal(TreeNode* root) { vector result; if (root == nullptr) { return result; } preorder(root, result); return result; } 二叉树的非递归遍历，主要的思想是使用栈（Stack）来进行存储操作，记录经过的节点。 非递归前序遍历（LeetCode 144）： vector preorderTraversal(TreeNode* root) { TreeNode *p = root; vector result; if (!p) { return result; } stack q; while (p || !q.empty()) { if (p) { result.push_back(p->val); q.push(p); p = p->left; } else { p = q.top(); q.pop(); p = p->right; } } return result; } 非递归中序遍历（LeetCode 94）： vector inorderTraversal(TreeNode* root) { TreeNode *p = root; vector result; if (!p) { return result; } stack q; while (p || !q.empty()) { if (p) { q.push(p); p = p->left; } else { p = q.top(); result.push_back(p->val); q.pop(); p = p->right; } } return result; } 非递归遍历中，后序遍历相对更难实现，因为需要在遍历完左右子节点之后，再遍历根节点，因此不能直接将根节点出栈。这里使用一个 last 指针记录上次出栈的节点，当且仅当节点的右孩子为空（top->right == NULL），或者右孩子已经出栈（top->right == last），才将本节点出栈： 非递归后序遍历（LeetCode 145）： vector postorderTraversal(TreeNode* root) { TreeNode *p = root; vector result; if (!p) { return result; } TreeNode *top, *last = NULL; stack q; while (p || !q.empty()) { if (p) { q.push(p); p = p->left; } else { top = q.top(); if (top->right == NULL || top->right == last) { q.pop(); result.push_back(top->val); last = top; } else { p = top->right; } } } return result; } 二叉树层序遍历 LeetCode 102 二叉树层序遍历有两种方法，分别是深度优先和广度优先： 深度优先（DFS）实现： void traversal(TreeNode *root, int level, vector> &result) { if (!root) { return; } // 保证每一层只有一个vector if (level > result.size()) { result.push_back(vector()); } result[level-1].push_back(root->val); traversal(root->left, level+1, result); traversal(root->right, level+1, result); } vector > levelOrder(TreeNode *root) { vector> result; traversal(root, 1, result); return result; } 广度优先（BFS）实现： vector> levelOrder(TreeNode* root) { std:queue q; TreeNode *p; vector> result; if (root == NULL) return result; q.push(root); while (!q.empty()) { int size = q.size(); vector levelResult; for (int i = 0; i val); if (p->left) { q.push(p->left); } if (p->right) { q.push(p->right); } } result.push_back(levelResult); } return result; } 二叉树子树 LeetCode 572 判断二叉树是否是另一棵二叉树的子树，使用递归实现： bool isSubtree(TreeNode* s, TreeNode* t) { if (!s) return false; if (sameTree(s, t)) return true; return isSubtree(s->left, t) || isSubtree(s->right, t); } bool sameTree(TreeNode* s, TreeNode* t) { if (!s && !t) return true; if (!s || !t) return false; if (s->val != t->val) return false; return sameTree(s->left, t->left) && sameTree(s->right, t->right); } 翻转二叉树 LeetCode 226 交互树的左右儿子节点，使用递归实现： TreeNode* invertTree(TreeNode* root) { if (root == nullptr) { return nullptr; } TreeNode *tmp = root->left; root->left = root->right; root->right = tmp; if (root->left) { invertTree(root->left); } if (root->right) { invertTree(root->right); } return root; } 参考资料 百度百科：哈弗曼树 百度百科：二叉排序树 百度百科：平衡二叉树 平衡二叉树及其应用场景 百度百科：B-树 前缀树 百度百科：前缀树 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-24 23:56:54 "},"basic/algo/search.html":{"url":"basic/algo/search.html","title":"DFS BFS搜索","keywords":"","body":"DFS BFS 深度优先搜索 广度优先搜索 DFS DFS思路 这是基本实现，后面还有双向DFS待补充 深度优先搜索，使用递归实现 大致算法模板： def dfs(a,b): if 判断是否达到目标，如果没有进行递归搜索 dfs():加入搜索方向，根据题意。 代码样例如下： 给定数组a,求和结果K，求是否可以得到K n = 5; a = [4,2,1,0,7]; k = 13; k2 = 15; def dfs(i,sum): if i == n: return sum == k; if dfs(i+1,sum): print(a[i]); return True; if dfs(i+1,sum+a[i]): print(a[i]); return True; return False; if(dfs(0,0)): print(\"yes\\n\"); else: print(\"NO\\n\") BFS 思路与DFS想通，但是实现略有不同。 关键因素： 记录搜索方向 例如：x = [0, 1,0，-1] y = [1, 0, -1, 0] 结合之后产生四个方向的坐标变化。 记录状态的队列： 一般使用队列实现，也可以使用数组实现。 状态记录主要记录访问过的点的坐标。 记录状态的队列就是搜索的队列以及顺序 距离记录 在一般的路径搜索，迷宫题目中需要找到最短的路径，加以修改可以返回迷宫路径 剩下就是和DFS差不多的条件判断及循环搜索 代码例子如下： N = 10; M = 10; # 0 - 墙壁 # 1 - 通道 # 3 - 起点 # 4 - 终点 A = [ [0,3,0,0,0,0,0,0,1,0], [1,1,1,1,1,1,0,1,1,0], [1,0,1,0,0,1,0,0,1,0], [1,0,1,1,1,1,1,1,1,1], [0,0,1,0,0,1,0,0,0,0], [1,1,1,1,0,1,1,1,1,0], [1,0,0,0,0,0,0,0,1,0], [1,1,1,1,0,1,1,1,1,1], [1,0,0,0,0,1,0,0,0,1], [1,1,1,1,0,1,1,1,4,0], ]; distance=[[9999 for i in range(10)] for j in range(10)] s = [0,1]; g = [9,8]; sx=s[0]; sy=s[1]; gx=g[0]; gy=g[1]; # 移动方向，4个方向 dx=[1, 0, -1, 0]; dy=[0, 1, 0, -1]; # 表示状态,标记是否访问过 state=[]; # 标记初始点为访问过的点 state.append([sx,sy]); # 设定距离为0 distance[sx][sy]=0; while len(state): point = state[0]; del state[0]; print(\"***\") print(state) # 判断是否找到出口 if A[point[0]][point[1]] == 4: #if point[0] == gx & point[1] == gy: print(\"YES\"); break; for i in range(0,4): x_index = point[0] + dx[i]; y_index = point[1] + dy[i]; # 判断移动条件以及是否访问过 if 0 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/algo/Greedy.html":{"url":"basic/algo/Greedy.html","title":"贪心","keywords":"","body":"贪心算法 建议观看MIT算法导论-贪心算法中的课程。 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/algo/DP.html":{"url":"basic/algo/DP.html","title":"动态规划","keywords":"","body":"动态规划 建议观看 MIT 算法导论-动态规划中的课程。 适用于动态规划的问题，需要满足最优子结构和无后效性，动态规划的求解过程，在于找到状态转移方程，进行自底向上的求解。 例题 爬楼梯问题 LeetCode 70 经典的动态规划问题之一，容易找到其状态转移方程为 dp[i] = dp[i-1] + dp[i-2]，从基础的 1 和 2 个台阶两个状态开始，自底向上求解： int climbStairs(int n) { if (n == 1) { return 1; } int* dp = new int[n+1](); dp[1] = 1; dp[2] = 2; for (int i = 3; i 从上面的代码中看到，dp[i] 只依赖 dp[i-1] 和 dp[i-2]，因此可以将代码简化： int climbStairs(int n) { int f0 = 1, f1 = 1, i, f2; for (i=2; i 容易看出其实结果就是 fibonacci 数列的第 n 项。 连续子数组的最大和 LeetCode 53 用 dp[n] 表示元素 n 作为末尾的连续序列的最大和，容易想到状态转移方程为dp[n] = max(dp[n-1] + num[n], num[n])，从第 1 个元素开始，自顶向上求解： int maxSubArray(vector& nums) { int* dp = new int[nums.size()](); dp[0] = nums[0]; int result = dp[0]; for (int i = 1; i 类似前一个问题，这个问题当中，求解 dp[i] 只依赖 dp[i-1]，因此可以使用变量来存储，简化代码： int maxSubArray(int A[], int n) { int result = INT_MIN; int f = 0; for (int i=0; i House Robber LeetCode 198 对于一个房子，有抢和不抢两种选择，容易得到状态转移方程 dp[i+1] = max(dp[i-1] + nums[i], dp[i])，示例代码如下： int rob(vector& nums) { int n = nums.size(); if (n == 0) { return 0; } vector dp = vector(n + 1); dp[0] = 0; dp[1] = nums[0]; for (int i = 1; i 同样的，可以使用两个变量简化代码： int rob(vector& nums) { int n = nums.size(); if (n == 0) { return 0; } int prev1 = 0; int prev2 = 0; for (int i = 0; i 最长回文子串 LeetCode 5 用 dp[i][j] 表示子串 i 到 j 是否是回文，使用动态规划求解： string longestPalindrome(string s) { int m = s.size(); if (m == 0) { return \"\"; } vector> dp(m, vector(m, 0)); int start = 0; int length = 1; for (int i = 0; i length) { start = i; length = j - i + 1; } } } } return s.substr(start, length); } 最小编辑距离 LeetCode 72 用 dp[i][j] 表示从 word[0..i) 转换到 word[0..j) 的最小操作，使用动态规划求解： int minDistance(string word1, string word2) { int m = word1.size(); int n = word2.size(); vector> dp(m + 1, vector(n + 1, 0)); // 全部删除，操作数量为 i for (int i = 0; i Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/arch/":{"url":"basic/arch/","title":"体系结构与操作系统","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/arch/Arch.html":{"url":"basic/arch/Arch.html","title":"体系结构基础","keywords":"","body":"冯·诺依曼体系结构 计算机处理的数据和指令一律用二进制数表示 顺序执行程序 计算机运行过程中，把要执行的程序和处理的数据首先存入主存储器（内存），计算机执行程序时，将自动地并按顺序从主存储器中取出指令一条一条地执行，这一概念称作顺序执行程序。 计算机硬件由运算器、控制器、存储器、输入设备和输出设备五大部分组成。 数据的机内表示 二进制表示 机器数 由于计算机中符号和数字一样,都必须用二进制数串来表示,因此,正负号也必须用0、1来表示。 用最高位0表示正、1表示负, 这种正负号数字化的机内表示形式就称为“机器数”,而相应的机器外部用正负号表示的数称为“真值”,将一个真值表示成二进制字串的机器数的过程就称为编码。 原码 原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制: [+1]原 = 0000 0001 [-1]原 = 1000 0001 第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是: [1111 1111 , 0111 1111] 即 [-127 , 127] 原码是人脑最容易理解和计算的表示方式 反码 反码的表示方法是: 正数的反码是其本身 负数的反码是在其原码的基础上, 符号位不变，其余各个位取反. [+1] = [00000001]原 = [00000001]反 [-1] = [10000001]原 = [11111110]反 可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算 补码 补码的表示方法是: 正数的补码就是其本身 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1。 (即在反码的基础上+1) [+1] = [00000001]原 = [00000001]反 = [00000001]补 [-1] = [10000001]原 = [11111110]反 = [11111111]补 对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值. 定点数与浮点数 定点数是小数点固定的数。在计算机中没有专门表示小数点的位，小数点的位置是约定默认的。一般固定在机器数的最低位之后，或是固定在符号位之后。前者称为定点纯整数，后者称为定点纯小数。 定点数表示法简单直观，但是数值表示的范围太小，运算时容易产生溢出。 浮点数是小数点的位置可以变动的数。为增大数值表示范围，防止溢出，采用浮点数表示法。浮点表示法类似于十进制中的科学计数法。 在计算机中通常把浮点数分成阶码和尾数两部分来表示，其中阶码一般用补码定点整数表示，尾数一般用补码或原码定点小数表示。为保证不损失有效数字，对尾数进行规格化处理，也就是平时所说的科学记数法，即保证尾数的最高位为1，实际数值通过阶码进行调整 阶符表示指数的符号位、阶码表示幂次、数符表示尾数的符号位、尾数表示规格化后的小数值。 N = 尾数×基数阶码（指数） 位(Bit)、字节(Byte)、字(Word) 位：\"位(bit)\"是电子计算机中最小的数据单位。每一位的状态只能是0或1。 字节：8个二进制位构成1个\"字节(Byte)\"，它是存储空间的基本计量单位。1个字节可以储存1个英文字母或者半个汉字，换句话说，1个汉字占据2个字节的存储空间。 字：\"字\"由若干个字节构成，字的位数叫做字长，不同档次的机器有不同的字长。例如一台8位机，它的1个字就等于1个字节，字长为8位。如果是一台16位机，那么，它的1个字就由2个字节构成，字长为16位。字是计算机进行数据处理和运算的单位。 字节序 字节顺序是指占内存多于一个字节类型的数据在内存中的存放顺序，通常有小端、大端两种字节顺序。 小端字节序指低字节数据存放在内存低地址处，高字节数据存放在内存高地址处； 大端字节序是高字节数据存放在低地址处，低字节数据存放在高地址处。 基于X86平台的PC机是小端字节序的，而有的嵌入式平台则是大端字节序的。所有网络协议也都是采用big endian的方式来传输数据的。所以有时我们也会把big endian方式称之为网络字节序。 比如数字0x12345678在两种不同字节序CPU中的存储顺序如下所示： Big Endian 低地址 高地址 ----------------------------------------------------> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 12 | 34 | 56 | 78 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Little Endian 低地址 高地址 ----------------------------------------------------> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 78 | 56 | 34 | 12 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 从上面两图可以看出，采用Big Endian方式存储数据是符合我们人类的思维习惯的。 联合体union的存放顺序是所有成员都从低地址开始存放，利用该特性，就能判断CPU对内存采用Little-endian还是Big-endian模式读写。 示例代码如下： union test{ short i; char str[sizeof(short)]; }tt; void main() { tt.i = 0x0102; if(sizeof(short) == 2) { if(tt.str[0] == 1 && tt.str[1] == 2) printf(\"大端字节序\"); else if(tt.str[0] = 2 && tt.str[1] == 1) printf(\"小端字节序\"); else printf(\"结果未知\"); } else printf(\"sizof(short)=%d,不等于2\",sizeof(short)); } 字节对齐 现代计算机中内存空间都是按照byte划分的，从理论上讲似乎对任何类型的变量的访问可以从任何地址开始，但实际情况是在访问特定类型变量的时候经常在特定的内存地址访问，这就需要各种类型数据按照一定的规则在空间上排列，而不是顺序的一个接一个的排放，这就是对齐。 为什么要进行字节对齐？ 某些平台只能在特定的地址处访问特定类型的数据; 最根本的原因是效率问题，字节对齐能提⾼存取数据的速度。 比如有的平台每次都是从偶地址处读取数据,对于一个int型的变量,若从偶地址单元处存放,则只需一个读取周期即可读取该变量，但是若从奇地址单元处存放,则需要2个读取周期读取该变量。 字节对齐的原则 数据成员对齐规则：结构(struct)(或联合(union))的数据成员，第一个数据成员放在 offset 为0的地方，以后每个数据成员存储的起始位置要从该成员大小或者成员的子成员大小（只要该成员有子成员，比如说是数组，结构体等）的整数倍开始(比如int在32位机为4字节,则要从4的整数倍地址开始存储。 结构体作为成员:如果一个结构里有某些结构体成员,则结构体成员要从其内部最大元素大小的整数倍地址开始存储。(struct a里存有struct b,b里有char,int ,double等元素,那b应该从8的整数倍开始存储。) 收尾工作:结构体的总大小，也就是sizeof的结果，必须是其内部最大成员的整数倍，不足的要补齐。 参考资料 百度百科：冯·诺依曼体系结构 二进制原码、反码、补码 什么是位、字节、字、KB、MB? 定点数与浮点数 大小字节序 浅谈字节序(Byte Order)及其相关操作 大小端字节序的判断 百度百科：字节对齐 五分钟搞定内存对齐 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/arch/OS.html":{"url":"basic/arch/OS.html","title":"操作系统基础","keywords":"","body":"操作系统提供的服务 操作系统的五大功能，分别为：作业管理、文件管理、存储管理、输入输出设备管理、进程及处理机管理 中断与系统调用 中断 所谓的中断就是在计算机执行程序的过程中，由于出现了某些特殊事情，使得CPU暂停对程序的执行，转而去执行处理这一事件的程序。等这些特殊事情处理完之后再回去执行之前的程序。中断一般分为三类： 由计算机硬件异常或故障引起的中断，称为内部异常中断； 由程序中执行了引起中断的指令而造成的中断，称为软中断（这也是和我们将要说明的系统调用相关的中断）； 由外部设备请求引起的中断，称为外部中断。简单来说，对中断的理解就是对一些特殊事情的处理。 与中断紧密相连的一个概念就是中断处理程序了。当中断发生的时候，系统需要去对中断进行处理，对这些中断的处理是由操作系统内核中的特定函数进行的，这些处理中断的特定的函数就是我们所说的中断处理程序了。 另一个与中断紧密相连的概念就是中断的优先级。中断的优先级说明的是当一个中断正在被处理的时候，处理器能接受的中断的级别。中断的优先级也表明了中断需要被处理的紧急程度。每个中断都有一个对应的优先级，当处理器在处理某一中断的时候，只有比这个中断优先级高的中断可以被处理器接受并且被处理。优先级比这个当前正在被处理的中断优先级要低的中断将会被忽略。 典型的中断优先级如下所示： 机器错误 > 时钟 > 磁盘 > 网络设备 > 终端 > 软件中断 当发生软件中断时，其他所有的中断都可能发生并被处理；但当发生磁盘中断时，就只有时钟中断和机器错误中断能被处理了。 系统调用 在讲系统调用之前，先说下进程的执行在系统上的两个级别：用户级和核心级，也称为用户态和系统态(user mode and kernel mode)。 程序的执行一般是在用户态下执行的，但当程序需要使用操作系统提供的服务时，比如说打开某一设备、创建文件、读写文件等，就需要向操作系统发出调用服务的请求，这就是系统调用。 Linux系统有专门的函数库来提供这些请求操作系统服务的入口，这个函数库中包含了操作系统所提供的对外服务的接口。当进程发出系统调用之后，它所处的运行状态就会由用户态变成核心态。但这个时候，进程本身其实并没有做什么事情，这个时候是由内核在做相应的操作，去完成进程所提出的这些请求。 系统调用和中断的关系就在于，当进程发出系统调用申请的时候，会产生一个软件中断。产生这个软件中断以后，系统会去对这个软中断进行处理，这个时候进程就处于核心态了。 那么用户态和核心态之间的区别是什么呢？（以下区别摘至《UNIX操作系统设计》） 用户态的进程能存取它们自己的指令和数据，但不能存取内核指令和数据（或其他进程的指令和数据）。然而，核心态下的进程能够存取内核和用户地址 某些机器指令是特权指令，在用户态下执行特权指令会引起错误 对此要理解的一个是，在系统中内核并不是作为一个与用户进程平行的估计的进程的集合，内核是为用户进程运行的。 参考资料 Linux系统的中断、系统调用于调度概述 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/arch/Concurrency.html":{"url":"basic/arch/Concurrency.html","title":"并发技术","keywords":"","body":"多任务 在上古时代，CPU 资源十分昂贵，如果让 CPU 只能运行一个程序，那么当 CPU 空闲下来（例如等待 I/O 时），CPU 就空闲下来了。为了让 CPU 得到更好的利用，人们编写了一个监控程序，如果发现某个程序暂时无须使用 CPU 时，监控程序就把另外的正在等待 CPU 资源的程序启动起来，以充分利用 CPU 资源。这种方法被称为 多道程序（Multiprogramming）。 对于多道程序来说，最大的问题是程序之间不区分轻重缓急，对于交互式程序来说，对于 CPU 计算时间的需求并不多，但是对于响应速度却有比较高的要求。而对于计算类程序来说则正好相反，对于响应速度要求低，但是需要长时间的 CPU 计算。想象一下我们同时在用浏览器上网和听音乐，我们希望浏览器能够快速响应，同时也希望音乐不停掉。这时候多道程序就没法达到我们的要求了。于是人们改进了多道程序，使得每个程序运行一段时间之后，都主动让出 CPU 资源，这样每个程序在一段时间内都有机会运行一小段时间。这样像浏览器这样的交互式程序就能够快速地被处理，同时计算类程序也不会受到很大影响。这种程序协作方式被称为 分时系统（Time-Sharing System）。 在分时系统的帮助下，我们可以边用浏览器边听歌了，但是如果某个程序出现了错误，导致了死循环，不仅仅是这个程序会出错，整个系统都会死机。为了避免这种情况，一个更为先进的操作系统模式被发明出来，也就是我们现在很熟悉的 多任务（Multi-tasking）系统。操作系统从最底层接管了所有硬件资源。所有的应用程序在操作系统之上以 进程（Process） 的方式运行，每个进程都有自己独立的地址空间，相互隔离。CPU 由操作系统统一进行分配。每个进程都有机会得到 CPU，同时在操作系统控制之下，如果一个进程运行超过了一定时间，就会被暂停掉，失去 CPU 资源。这样就避免了一个程序的错误导致整个系统死机。如果操作系统分配给各个进程的运行时间都很短，CPU 可以在多个进程间快速切换，就像很多进程都同时在运行的样子。几乎所有现代操作系统都是采用这样的方式支持多任务，例如 Unix，Linux，Windows 以及 macOS。 进程 进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。 进程的概念主要有两点：第一，进程是一个实体。每一个进程都有它自己的地址空间，一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。第二，进程是一个“执行中的程序”。程序是一个没有生命的实体，只有处理器赋予程序生命时，它才能成为一个活动的实体，我们称其为进程。 进程的基本状态 等待态：等待某个事件的完成； 就绪态：等待系统分配处理器以便运行； 运行态：占有处理器正在运行。 运行态→等待态 往往是由于等待外设，等待主存等资源分配或等待人工干预而引起的。 等待态→就绪态 则是等待的条件已满足，只需分配到处理器后就能运行。 运行态→就绪态 不是由于自身原因，而是由外界原因使运行状态的进程让出处理器，这时候就变成就绪态。例如时间片用完，或有更高优先级的进程来抢占处理器等。 就绪态→运行态 系统按某种策略选中就绪队列中的一个进程占用处理器，此时就变成了运行态 进程调度 调度种类 高级、中级和低级调度作业从提交开始直到完成，往往要经历下述三级调度： 高级调度：(High-Level Scheduling)又称为作业调度，它决定把后备作业调入内存运行； 中级调度：(Intermediate-Level Scheduling)又称为在虚拟存储器中引入，在内、外存对换区进行进程对换。 低级调度：(Low-Level Scheduling)又称为进程调度，它决定把就绪队列的某进程获得CPU； 非抢占式调度与抢占式调度 非抢占式 分派程序一旦把处理机分配给某进程后便让它一直运行下去，直到进程完成或发生进程调度进程调度某事件而阻塞时，才把处理机分配给另一个进程。 抢占式 操作系统将正在运行的进程强行暂停，由调度程序将CPU分配给其他就绪进程的调度方式。 调度策略的设计 响应时间: 从用户输入到产生反应的时间 周转时间: 从任务开始到任务结束的时间 CPU任务可以分为交互式任务和批处理任务，调度最终的目标是合理的使用CPU，使得交互式任务的响应时间尽可能短，用户不至于感到延迟，同时使得批处理任务的周转时间尽可能短，减少用户等待的时间。 调度算法 FIFO或First Come, First Served (FCFS) 调度的顺序就是任务到达就绪队列的顺序。 公平、简单(FIFO队列)、非抢占、不适合交互式。未考虑任务特性，平均等待时间可以缩短 Shortest Job First (SJF) 最短的作业(CPU区间长度最小)最先调度。 可以证明，SJF可以保证最小的平均等待时间。 Shortest Remaining Job First (SRJF) SJF的可抢占版本，比SJF更有优势。 SJF(SRJF): 如何知道下一CPU区间大小？根据历史进行预测: 指数平均法。 优先权调度 每个任务关联一个优先权，调度优先权最高的任务。 注意：优先权太低的任务一直就绪，得不到运行，出现“饥饿”现象。 FCFS是RR的特例，SJF是优先权调度的特例。这些调度算法都不适合于交互式系统。 Round-Robin(RR) 设置一个时间片，按时间片来轮转调度（“轮叫”算法） 优点: 定时有响应，等待时间较短；缺点: 上下文切换次数较多； 如何确定时间片？ 时间片太大，响应时间太长；吞吐量变小，周转时间变长；当时间片过长时，退化为FCFS。 多级队列调度 按照一定的规则建立多个进程队列 不同的队列有固定的优先级（高优先级有抢占权） 不同的队列可以给不同的时间片和采用不同的调度方法 存在问题1：没法区分I/O bound和CPU bound； 存在问题2：也存在一定程度的“饥饿”现象； 多级反馈队列 在多级队列的基础上，任务可以在队列之间移动，更细致的区分任务。 可以根据“享用”CPU时间多少来移动队列，阻止“饥饿”。 最通用的调度算法，多数OS都使用该方法或其变形，如UNIX、Windows等。 进程同步 临界资源与临界区 在操作系统中，进程是占有资源的最小单位（线程可以访问其所在进程内的所有资源，但线程本身并不占有资源或仅仅占有一点必须资源）。但对于某些资源来说，其在同一时间只能被一个进程所占用。这些一次只能被一个进程所占用的资源就是所谓的临界资源。典型的临界资源比如物理上的打印机，或是存在硬盘或内存中被多个进程所共享的一些变量和数据等(如果这类资源不被看成临界资源加以保护，那么很有可能造成丢数据的问题)。 对于临界资源的访问，必须是互斥进行。也就是当临界资源被占用时，另一个申请临界资源的进程会被阻塞，直到其所申请的临界资源被释放。而进程内访问临界资源的代码被成为临界区。 对于临界区的访问过程分为四个部分： 进入区:查看临界区是否可访问，如果可以访问，则转到步骤二，否则进程会被阻塞 临界区:在临界区做操作 退出区:清除临界区被占用的标志 剩余区：进程与临界区不相关部分的代码 解决临界区问题可能的方法： 一般软件方法 关中断方法 硬件原子指令方法 信号量方法 信号量 信号量是一个确定的二元组（s，q），其中s是一个具有非负初值的整形变量，q是一个初始状态为空的队列，整形变量s表示系统中某类资源的数目： 当其值 ≥ 0 时，表示系统中当前可用资源的数目 当其值 ＜ 0 时，其绝对值表示系统中因请求该类资源而被阻塞的进程数目 除信号量的初值外，信号量的值仅能由P操作和V操作更改，操作系统利用它的状态对进程和资源进行管理 P操作： P操作记为P(s)，其中s为一信号量，它执行时主要完成以下动作： s.value = s.value - 1； /*可理解为占用1个资源，若原来就没有则记帐“欠”1个*/ 若s.value ≥ 0，则进程继续执行，否则（即s.value 说明：实际上，P操作可以理解为分配资源的计数器，或是使进程处于等待状态的控制指令 V操作： V操作记为V(s)，其中s为一信号量，它执行时，主要完成以下动作： s.value = s.value + 1；/*可理解为归还1个资源，若原来就没有则意义是用此资源还1个欠帐*/ 若s.value > 0，则进程继续执行，否则（即s.value ≤ 0）,则从信号量s的等待队s.queue中移出第一个进程，使其变为就绪状态，然后返回原进程继续执行 说明：实际上，V操作可以理解为归还资源的计数器，或是唤醒进程使其处于就绪状态的控制指令 信号量方法实现：生产者 − 消费者互斥与同步控制 semaphore fullBuffers = 0; /*仓库中已填满的货架个数*/ semaphore emptyBuffers = BUFFER_SIZE;/*仓库货架空闲个数*/ semaphore mutex = 1; /*生产-消费互斥信号*/ Producer() { while(True) { /*生产产品item*/ emptyBuffers.P(); mutex.P(); /*item存入仓库buffer*/ mutex.V(); fullBuffers.V(); } } Consumer() { while(True) { fullBuffers.P(); mutex.P(); /*从仓库buffer中取产品item*/ mutex.V(); emptyBuffers.V(); /*消费产品item*/ } } 死锁 死锁: 多个进程因循环等待资源而造成无法执行的现象。 死锁会造成进程无法执行，同时会造成系统资源的极大浪费(资源无法释放)。 死锁产生的4个必要条件： 互斥使用(Mutual exclusion) 指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 不可抢占(No preemption) 指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 请求和保持(Hold and wait) 指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 循环等待(Circular wait) 指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 死锁避免——银行家算法 思想: 判断此次请求是否造成死锁若会造成死锁，则拒绝该请求 进程间通信 本地进程间通信的方式有很多，可以总结为下面四类： 消息传递（管道、FIFO、消息队列） 同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） 共享内存（匿名的和具名的） 远程过程调用（Solaris门和Sun RPC） 线程 多进程解决了前面提到的多任务问题。然而很多时候不同的程序需要共享同样的资源（文件，信号量等），如果全都使用进程的话会导致切换的成本很高，造成 CPU 资源的浪费。于是就出现了线程的概念。 线程，有时被称为轻量级进程(Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针(PC），寄存器集合和堆栈组成。 线程具有以下属性： 轻型实体 线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。 独立调度和分派的基本单位。 在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。 可并发执行。 在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。 共享进程资源。 在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的地址空间（进程的地址空间），这意味着，线程可以访问该地址空间的每一个虚地址；此外，还可以访问进程所拥有的已打开文件、定时器、信号量等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。 线程共享的环境包括：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。 使用 pthread 线程库实现的生产者－消费者模型： #include #include #include #define BUFFER_SIZE 10 static int buffer[BUFFER_SIZE] = { 0 }; static int count = 0; pthread_t consumer, producer; pthread_cond_t cond_producer, cond_consumer; pthread_mutex_t mutex; void* consume(void* _){ while(1){ pthread_mutex_lock(&mutex); while(count == 0){ printf(\"empty buffer, wait producer\\n\"); pthread_cond_wait(&cond_consumer, &mutex); } count--; printf(\"consume a item\\n\"); pthread_mutex_unlock(&mutex); pthread_cond_signal(&cond_producer); //pthread_mutex_unlock(&mutex); } pthread_exit(0); } void* produce(void* _){ while(1){ pthread_mutex_lock(&mutex); while(count == BUFFER_SIZE){ printf(\"full buffer, wait consumer\\n\"); pthread_cond_wait(&cond_producer, &mutex); } count++; printf(\"produce a item.\\n\"); pthread_mutex_unlock(&mutex); pthread_cond_signal(&cond_consumer); //pthread_mutex_unlock(&mutex); } pthread_exit(0); } int main() { pthread_mutex_init(&mutex, NULL); pthread_cond_init(&cond_consumer, NULL); pthread_cond_init(&cond_producer, NULL); int err = pthread_create(&consumer, NULL, consume, (void*)NULL); if(err != 0){ printf(\"consumer thread created failed\\n\"); exit(1); } err = pthread_create(&producer, NULL, produce, (void*)NULL); if(err != 0){ printf(\"producer thread created failed\\n\"); exit(1); } pthread_join(producer, NULL); pthread_join(consumer, NULL); //sleep(1000); pthread_cond_destroy(&cond_consumer); pthread_cond_destroy(&cond_producer); pthread_mutex_destroy(&mutex); return 0; } 锁 这里讨论的主要是多线程编程中需要使用的锁，网上有关于锁的文章实在是非常多而且乱套，让新手不知道从何下手。这里我们不去钻名词和概念的牛角尖，而是直接从本质上试图解释一下锁这个很常用的多线程编程工具。 锁要解决的是线程之间争取资源的问题，这个问题大概有下面几个角度： 资源是否是独占（独占锁 - 共享锁） 抢占不到资源怎么办（互斥锁 - 自旋锁） 自己能不能重复抢（重入锁 - 不可重入锁） 竞争读的情况比较多，读可不可以不加锁（读写锁） 上面这几个角度不是互相独立的，在实际场景中往往要它们结合起来，才能构造出一个合适的锁。 独占锁 - 共享锁 当一个共享资源只有一份的时候，通常我们使用独占锁，常见的即各个语言当中的 Mutex。当共享资源有多份时，可以使用前面提到的 Semaphere。 互斥锁 - 自旋锁 对于互斥锁来说，如果一个线程已经锁定了一个互斥锁，第二个线程又试图去获得这个互斥锁，则第二个线程将被挂起（即休眠，不占用 CPU 资源）。 在计算机系统中，频繁的挂起和切换线程，也是有成本的。自旋锁就是解决这个问题的。 自旋锁，指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 容易看出，当资源等待的时间较长，用互斥锁让线程休眠，会消耗更少的资源。当资源等待的时间较短时，使用自旋锁将减少线程的切换，获得更高的性能。 较新版本的 Java 中的 synchornized 和 .NET 中的 lock（Monitor） 的实现，是结合了两种锁的特点。简单说，它们在发现资源被抢占之后，会先试着自旋等待一段时间，如果等待时间太长，则会进入挂起状态。通过这样的实现，可以较大程度上挖掘出锁的性能。 重入锁 - 不可重入锁 可重入锁（ReetrantLock），也叫做递归锁，指的是在同一线程内，外层函数获得锁之后，内层递归函数仍然可以获取到该锁。换一种说法：同一个线程再次进入同步代码时，可以使用自己已获取到的锁。 使用可重入锁时，在同一线程中多次获取锁，不会导致死锁。使用不可重入锁，则会导致死锁发生。 Java 中的 synchornized 和 .NET 中的 lock（Monitor） 都是可重入的。 读写锁 有些情况下，对于共享资源读竞争的情况远远多于写竞争，这种情况下，对读操作每次都进行加速，是得不偿失的。读写锁就是为了解决这个问题。 读写锁允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。简单可以总结为，读读不互斥，读写互斥，写写互斥。 对读写锁来说，有一个升级和降级的概念，即当前获得了读锁，想把当前的锁变成写锁，称为升级，反之称为降级。锁的升降级本身也是为了提升性能，通过改变当前锁的性质，避免重复获取锁。 协程 协程，又称微线程，纤程。英文名 Coroutine。 协程可以理解为用户级线程，协程和线程的区别是：线程是抢占式的调度，而协程是协同式的调度，协程避免了无意义的调度，由此可以提高性能，但也因此，程序员必须自己承担调度的责任，同时，协程也失去了标准线程使用多CPU的能力。 使用协程(python)改写生产者-消费者问题： import time def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) time.sleep(1) r = '200 OK' def produce(c): next(c) #python 3.x中使用next(c)，python 2.x中使用c.next() n = 0 while n 可以看到，使用协程不再需要显式地对锁进行操作。 IO多路复用 基本概念 IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合： 当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用。 当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。 如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。 如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。 如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 常见的IO复用实现 select(Linux/Windows/BSD) epoll(Linux) kqueue(BSD/Mac OS X) 参考资料 浅谈进程同步与互斥的概念 进程间同步——信号量 百度百科：线程 进程、线程和协程的理解 协程 IO多路复用之select总结 银行家算法 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/arch/Memory-Management.html":{"url":"basic/arch/Memory-Management.html","title":"内存管理","keywords":"","body":"内存管理基础 程序可执行文件的结构 一个程序的可执行文件在内存中的结果，从大的角度可以分为两个部分：只读部分和可读写部分。只读部分包括程序代码（.text）和程序中的常量（.rodata）。可读写部分（也就是变量）大致可以分成下面几个部分： .data： 初始化了的全局变量和静态变量 .bss： 即 Block Started by Symbol， 未初始化的全局变量和静态变量（这个我感觉上课真的没讲过啊我去。。。） heap： 堆，使用 malloc, realloc, 和 free 函数控制的变量，堆在所有的线程，共享库，和动态加载的模块中被共享使用 stack： 栈，函数调用时使用栈来保存函数现场，自动变量（即生命周期限制在某个 scope 的变量）也存放在栈中。 下面就各个分区具体解释一下： data 和 bss 区 这两个区经常放在一起说，因为他们都是用来存储全局变量和静态变量的，区别在于 data 区存放的是初始化过的， bss 区存放的是没有初始化过的，例如： int val = 3; char string[] = \"Hello World\"; 这两个变量的值会一开始被存储在 .text 中（因为值是写在代码里面的），在程序启动时会拷贝到 .data 去区中。 而不初始化的话，像下面这样： static int i; 这个变量就会被放在 bss 区中。 答疑一 静态变量和全局变量 这两个概念都是很常见的概念，又经常在一起使用，很容易造成混淆。 全局变量：在一个代码文件（具体说应该一个 translation unit/compilation unit)）当中，一个变量要么定义在函数中，要么定义在在函数外面。当定义在函数外面时，这个变量就有了全局作用域，成为了全局变量。全局变量不光意味着这个变量可以在整个文件中使用，也意味着这个变量可以在其他文件中使用（这种叫做 external linkage）。当有如下两个文件时； a.c #include int a; int compute(void); int main() { a = 1; printf(\"%d %d\\n\", a, compute()); return 0; } b.c int a; int compute(void) { a = 0; return a; } 在 Link 过程中会产生重复定义错误，因为有两个全局的 a 变量，Linker 不知道应该使用哪一个。为了避免这种情况，就需要引入 static。 静态变量： 指使用 static 关键字修饰的变量，static 关键字对变量的作用域进行了限制，具体的限制如下： 在函数外定义：全局变量，但是只在当前文件中可见（叫做 internal linkage） 在函数内定义：全局变量，但是只在此函数内可见（同时，在多次函数调用中，变量的值不会丢失） （C++）在类中定义：全局变量，但是只在此类中可见 对于全局变量来说，为了避免上面提到的重复定义错误，我们可以在一个文件中使用 static，另一个不使用。这样使用 static 的就会使用自己的 a 变量，而没有用 static 的会使用全局的 a 变量。当然，最好两个都使用 static，避免更多可能的命名冲突。 注意：'静态'这个中文翻译实在是有些莫名其妙，给人的感觉像是不可改变的，而实际上 static 跟不可改变没有关系，不可改变的变量使用 const 关键字修饰，注意不要混淆。 Bonus 部分 —— extern： extern 是 C 语言中另一个关键字，用来指示变量或函数的定义在别的文件中，使用 extern 可以在多个源文件中共享某个变量，例如这里的例子。 extern 跟 static 在含义上是“水火不容”的，一个表示不能在别的地方用，一个表示要去别的地方找。如果同时使用的话，有两种情况，一种是先使用 static，后使用 extern ，即： static int m; extern int m; 这种情况，后面的 m 实际上就是前面的 m 。如果反过来： extern int m; static int m; 这种情况的行为是未定义的，编译器也会给出警告。 答疑二 程序在内存和硬盘上不同的存在形式 这里我们提到的几个区，是指程序在内存中的存在形式。和程序在硬盘上存储的格式不是完全对应的。程序在硬盘上存储的格式更加复杂，而且是和操作系统有关的，具体可以参考这里。一个比较明显的例子可以帮你区分这个差别：之前我们提到过未定义的全局变量存储在 .bss 区，这个区域不会占用可执行文件的空间（一般只存储这个区域的长度），但是却会占用内存空间。这些变量没有定义，因此可执行文件中不需要存储（也不知道）它们的值，在程序启动过程中，它们的值会被初始化成 0 ，存储在内存中。 栈 栈是用于存放本地变量，内部临时变量以及有关上下文的内存区域。程序在调用函数时，操作系统会自动通过压栈和弹栈完成保存函数现场等操作，不需要程序员手动干预。 栈是一块连续的内存区域，栈顶的地址和栈的最大容量是系统预先规定好的。能从栈获得的空间较小。如果申请的空间超过栈的剩余空间时，例如递归深度过深，将提示stackoverflow。 栈是机器系统提供的数据结构，计算机会在底层对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。 堆 堆是用于存放除了栈里的东西之外所有其他东西的内存区域，当使用malloc和free时就是在操作堆中的内存。对于堆来说，释放工作由程序员控制，容易产生memory leak。 堆是向高地址扩展的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。 对于堆来讲，频繁的new/delete势必会造成内存空间的不连续，从而造成大量的碎片，使程序效率降低。对于栈来讲，则不会存在这个问题，因为栈是先进后出的队列，永远都不可能有一个内存块从栈中间弹出。 堆都是动态分配的，没有静态分配的堆。栈有2种分配方式：静态分配和动态分配。静态分配是编译器完成的，比如局部变量的分配。动态分配由alloca函数进行分配，但是栈的动态分配和堆是不同的，他的动态分配是由编译器进行释放，无需我们手工实现。 计算机底层并没有对堆的支持，堆则是C/C++函数库提供的，同时由于上面提到的碎片问题，都会导致堆的效率比栈要低。 内存分配 虚拟地址：用户编程时将代码（或数据）分成若干个段，每条代码或每个数据的地址由段名称 + 段内相对地址构成，这样的程序地址称为虚拟地址 逻辑地址：虚拟地址中，段内相对地址部分称为逻辑地址 物理地址：实际物理内存中所看到的存储地址称为物理地址 逻辑地址空间：在实际应用中，将虚拟地址和逻辑地址经常不加区分，通称为逻辑地址。逻辑地址的集合称为逻辑地址空间 线性地址空间：CPU地址总线可以访问的所有地址集合称为线性地址空间 物理地址空间：实际存在的可访问的物理内存地址集合称为物理地址空间 MMU(Memery Management Unit内存管理单元)：实现将用户程序的虚拟地址（逻辑地址） → 物理地址映射的CPU中的硬件电路 基地址：在进行地址映射时，经常以段或页为单位并以其最小地址（即起始地址）为基值来进行计算 偏移量：在以段或页为单位进行地址映射时，相对于基地址的地址值 虚拟地址先经过分段机制映射到线性地址，然后线性地址通过分页机制映射到物理地址。 虚拟内存 请求调页，也称按需调页，即对不在内存中的“页”，当进程执行时要用时才调入，否则有可能到程序结束时也不会调入 页面置换算法 FIFO算法 先入先出，即淘汰最早调入的页面。 OPT(MIN)算法 选未来最远将使用的页淘汰，是一种最优的方案，可以证明缺页数最小。 可惜，MIN需要知道将来发生的事，只能在理论中存在，实际不可应用。 LRU(Least-Recently-Used)算法 用过去的历史预测将来，选最近最长时间没有使用的页淘汰(也称最近最少使用)。 LRU准确实现：计数器法，页码栈法。 由于代价较高，通常不使用准确实现，而是采用近似实现，例如Clock算法。 内存抖动现象：页面的频繁更换，导致整个系统效率急剧下降，这个现象称为内存抖动（或颠簸）。抖动一般是内存分配算法不好，内存太小引或者程序的算法不佳引起的。 Belady现象：对有的页面置换算法，页错误率可能会随着分配帧数增加而增加。 FIFO会产生Belady异常。 栈式算法无Belady异常，LRU，LFU（最不经常使用），OPT都属于栈式算法。 参考资料 https://en.wikipedia.org/wiki/Data_segment https://stackoverflow.com/questions/12798486/bss-segment-in-c/12799389#12799389 https://stackoverflow.com/questions/7837190/c-c-global-vs-static-global https://stackoverflow.com/questions/572547/what-does-static-mean-in-a-c-program/572550#572550 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/arch/Disk-And-File.html":{"url":"basic/arch/Disk-And-File.html","title":"磁盘与文件","keywords":"","body":"磁盘调度 磁盘访问延迟 = 队列时间 + 控制器时间 + 寻道时间 + 旋转时间 + 传输时间 磁盘调度的目的是减小延迟，其中前两项可以忽略，寻道时间是主要矛盾。 磁盘调度算法 FCFS 先进先出的调度策略，这个策略具有公平的优点，因为每个请求都会得到处理，并且是按照接收到的顺序进行处理。 SSTF(Shortest-seek-time First 最短寻道时间优先) 选择使磁头从当前位置开始移动最少的磁盘I/O请求，所以 SSTF 总是选择导致最小寻道时间的请求。 总是选择最小寻找时间并不能保证平均寻找时间最小，但是能提供比 FCFS 算法更好的性能，会存在饥饿现象。 SCAN SSTF+中途不回折，每个请求都有处理机会。 SCAN 要求磁头仅仅沿一个方向移动，并在途中满足所有未完成的请求，直到它到达这个方向上的最后一个磁道，或者在这个方向上没有其他请求为止。 由于磁头移动规律与电梯运行相似，SCAN 也被称为电梯算法。 SCAN 算法对最近扫描过的区域不公平，因此，它在访问局部性方面不如 FCFS 算法和 SSTF 算法好。 C-SCAN SCAN+直接移到另一端，两端请求都能很快处理。 把扫描限定在一个方向，当访问到某个方向的最后一个磁道时，磁道返回磁盘相反方向磁道的末端，并再次开始扫描。 其中“C”是Circular（环）的意思。 LOOK 和 C-LOOK 釆用SCAN算法和C-SCAN算法时磁头总是严格地遵循从盘面的一端到另一端，显然，在实际使用时还可以改进，即磁头移动只需要到达最远端的一个请求即可返回，不需要到达磁盘端点。这种形式的SCAN算法和C-SCAN算法称为LOOK和C-LOOK调度。这是因为它们在朝一个给定方向移动前会查看是否有请求。 文件系统 分区表 MBR：支持最大卷为2 TB（Terabytes）并且每个磁盘最多有4个主分区（或3个主分区，1个扩展分区和无限制的逻辑驱动器） GPT：支持最大卷为18EB（Exabytes）并且每磁盘的分区数没有上限，只受到操作系统限制（由于分区表本身需要占用一定空间，最初规划硬盘分区时，留给分区表的空间决定了最多可以有多少个分区，IA-64版Windows限制最多有128个分区，这也是EFI标准规定的分区表的最小尺寸。另外，GPT分区磁盘有备份分区表来提高分区数据结构的完整性。 RAID 技术 磁盘阵列（Redundant Arrays of Independent Disks，RAID），独立冗余磁盘阵列之。原理是利用数组方式来作磁盘组，配合数据分散排列的设计，提升数据的安全性。 RAID 0 RAID 0是最早出现的RAID模式，需要2块以上的硬盘，可以提高整个磁盘的性能和吞吐量。 RAID 0没有提供冗余或错误修复能力，其中一块硬盘损坏，所有数据将遗失。 RAID 1 RAID 1就是镜像，其原理为在主硬盘上存放数据的同时也在镜像硬盘上写一样的数据。 当主硬盘（物理）损坏时，镜像硬盘则代替主硬盘的工作。因为有镜像硬盘做数据备份，所以RAID 1的数据安全性在所有的RAID级别上来说是最好的。 但无论用多少磁盘做RAID 1，仅算一个磁盘的容量，是所有RAID中磁盘利用率最低的。 RAID 2 这是RAID 0的改良版，以汉明码（Hamming Code）的方式将数据进行编码后分区为独立的比特，并将数据分别写入硬盘中。因为在数据中加入了错误修正码（ECC，Error Correction Code），所以数据整体的容量会比原始数据大一些，RAID2最少要三台磁盘驱动器方能运作。 RAID 3 采用Bit－interleaving（数据交错存储）技术，它需要通过编码再将数据比特分割后分别存在硬盘中，而将同比特检查后单独存在一个硬盘中，但由于数据内的比特分散在不同的硬盘上，因此就算要读取一小段数据资料都可能需要所有的硬盘进行工作，所以这种规格比较适于读取大量数据时使用。 RAID 4 它与RAID 3不同的是它在分区时是以区块为单位分别存在硬盘中，但每次的数据访问都必须从同比特检查的那个硬盘中取出对应的同比特数据进行核对，由于过于频繁的使用，所以对硬盘的损耗可能会提高。（块交织技术，Block interleaving） RAID 2/3/4 在实际应用中很少使用 RAID 5 RAID Level 5是一种储存性能、数据安全和存储成本兼顾的存储解决方案。它使用的是Disk Striping（硬盘分区）技术。 RAID 5至少需要三块硬盘，RAID 5不是对存储的数据进行备份，而是把数据和相对应的奇偶校验信息存储到组成RAID5的各个磁盘上，并且奇偶校验信息和相对应的数据分别存储于不同的磁盘上。 RAID 5 允许一块硬盘损坏。 实际容量 Size = (N-1) * min(S1, S2, S3 ... SN) RAID 6 与RAID 5相比，RAID 6增加第二个独立的奇偶校验信息块。两个独立的奇偶系统使用不同的算法，数据的可靠性非常高，即使两块磁盘同时失效也不会影响数据的使用。 RAID 6 至少需要4块硬盘。 实际容量 Size = (N-2) * min(S1, S2, S3 ... SN) RAID 10/01（RAID 1+0，RAID 0+1） RAID 10是先镜射再分区数据，再将所有硬盘分为两组，视为是RAID 0的最低组合，然后将这两组各自视为RAID 1运作。 RAID 01则是跟RAID 10的程序相反，是先分区再将数据镜射到两组硬盘。它将所有的硬盘分为两组，变成RAID 1的最低组合，而将两组硬盘各自视为RAID 0运作。 当RAID 10有一个硬盘受损，其余硬盘会继续运作。RAID 01只要有一个硬盘受损，同组RAID 0的所有硬盘都会停止运作，只剩下其他组的硬盘运作，可靠性较低。如果以六个硬盘建RAID 01，镜射再用三个建RAID 0，那么坏一个硬盘便会有三个硬盘脱机。因此，RAID 10远较RAID 01常用，零售主板绝大部份支持RAID 0/1/5/10，但不支持RAID 01。 RAID 10 至少需要4块硬盘，且硬盘数量必须为偶数。 常见文件系统 Windows: FAT, FAT16, FAT32, NTFS Linux: ext2/3/4, btrfs, ZFS Mac OS X: HFS+ Linux文件权限 Linux文件采用10个标志位来表示文件权限，如下所示： -rw-r--r-- 1 skyline staff 20B 1 27 10:34 1.txt drwxr-xr-x 5 skyline staff 170B 12 23 19:01 ABTableViewCell 第一个字符一般用来区分文件和目录，其中： d：表示是一个目录，事实上在ext2fs中，目录是一个特殊的文件。 －：表示这是一个普通的文件。 l: 表示这是一个符号链接文件，实际上它指向另一个文件。 b、c：分别表示区块设备和其他的外围设备，是特殊类型的文件。 s、p：这些文件关系到系统的数据结构和管道，通常很少见到。 第2～10个字符当中的每3个为一组，左边三个字符表示所有者权限，中间3个字符表示与所有者同一组的用户的权限，右边3个字符是其他用户的权限。 这三个一组共9个字符，代表的意义如下： r(Read，读取)：对文件而言，具有读取文件内容的权限；对目录来说，具有浏览目录的权限 w(Write,写入)：对文件而言，具有新增、修改文件内容的权限；对目录来说，具有删除、移动目录内文件的权限。 x(eXecute，执行)：对文件而言，具有执行文件的权限；对目录来说该用户具有进入目录的权限。 权限的掩码可以使用十进制数字表示： 如果可读，权限是二进制的100，十进制是4； 如果可写，权限是二进制的010，十进制是2； 如果可运行，权限是二进制的001，十进制是1； 具备多个权限，就把相应的 4、2、1 相加就可以了： 若要 rwx 则 4+2+1=7 若要 rw- 则 4+2=6 若要 r-x 则 4+1=5 若要 r-- 则 =4 若要 -wx 则 2+1=3 若要 -w- 则 =2 若要 --x 则 =1 若要 --- 则 =0 默认的权限可用umask命令修改，用法非常简单，只需执行umask 777命令，便代表屏蔽所有的权限，因而之后建立的文件或目录，其权限都变成000， 依次类推。通常root帐号搭配umask命令的数值为022、027和 077，普通用户则是采用002，这样所产生的权限依次为755、750、700、775。 chmod命令 chmod命令非常重要，用于改变文件或目录的访问权限。用户用它控制文件或目录的访问权限。 该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 文字设定法 chmod ［who］ ［+ | - | =］ ［mode］ 文件名 命令中各选项的含义为： 操作对象who可是下述字母中的任一个或者它们的组合： u 表示“用户（user）”，即文件或目录的所有者。 g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。 o 表示“其他（others）用户”。 a 表示“所有（all）用户”。它是系统默认值。 操作符号可以是： 添加某个权限。 取消某个权限。 = 赋予给定权限并取消其他所有权限（如果有的话）。 设置mode所表示的权限可用下述字母的任意组合： r 可读。 w 可写。 x 可执行。 X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加x 属性。 s 在文件执行时把进程的属主或组ID置为该文件的文件属主。方式“u＋s”设置文件的用户ID位，“g＋s”设置组ID位。 t 保存程序的文本到交换设备上。 u 与文件属主拥有一样的权限。 g 与和文件属主同组的用户拥有一样的权限。 o 与其他用户拥有一样的权限。 文件名：以空格分开的要改变权限的文件列表，支持通配符。 在一个命令行中可给出多个权限方式，其间用逗号隔开。例如：chmod g+r，o+r example 使同组和其他用户对文件example 有读权限。 数字设定法 直接使用数字表示的权限来更改： 例： $ chmod 644 mm.txt chgrp命令 功能：改变文件或目录所属的组。 语法：chgrp ［选项］ group filename 例：$ chgrp - R book /opt/local /book 改变/opt/local /book/及其子目录下的所有文件的属组为book。 chown命令 功能：更改某个文件或目录的属主和属组。这个命令也很常用。例如root用户把自己的一个文件拷贝给用户xu，为了让用户xu能够存取这个文件，root用户应该把这个文件的属主设为xu，否则，用户xu无法存取这个文件。 语法：chown ［选项］ 用户或组 文件 说明：chown将指定文件的拥有者改为指定的用户或组。用户可以是用户名或用户ID。组可以是组名或组ID。文件是以空格分开的要改变权限的文件列表，支持通配符。 例：把文件shiyan.c的所有者改为wang。 chown wang shiyan.c 参考资料 操作系统中的磁盘调度算法 百度百科：磁盘阵列 维基百科：RAID Linux文件权限详解 修改Linux文件权限命令:chmod Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/compiler/":{"url":"basic/compiler/","title":"编译原理","keywords":"","body":"欢迎补充内容 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/compiler/Compiler-Arch.html":{"url":"basic/compiler/Compiler-Arch.html","title":"编译器架构","keywords":"","body":"词法分析器 语法分析器 语义分析及中间代码生成 代码优化 代码生成 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/design_pattern/":{"url":"basic/design_pattern/","title":"设计模式","keywords":"","body":"工厂模式 工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 主要解决：主要解决接口选择的问题。 何时使用：我们明确地计划不同条件下创建不同实例时。 优点： 一个调用者想创建一个对象，只要知道其名称就可以了。 扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 屏蔽产品的具体实现，调用者只关心产品的接口。 缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 创建步骤： 步骤 1 创建一个接口: Shape.java public interface Shape { void draw(); } 步骤 2 创建实现接口的实体类： Rectangle.java public class Rectangle implements Shape { @Override public void draw() { System.out.println(\"Inside Rectangle::draw() method.\"); } } Square.java， Circle.java 同上 步骤 3 创建一个工厂，生成基于给定信息的实体类的对象: ShapeFactory.java public class ShapeFactory { //使用 getShape 方法获取形状类型的对象 public Shape getShape(String shapeType){ if(shapeType == null){ return null; } if(shapeType.equalsIgnoreCase(\"CIRCLE\")){ return new Circle(); } else if(shapeType.equalsIgnoreCase(\"RECTANGLE\")){ return new Rectangle(); } else if(shapeType.equalsIgnoreCase(\"SQUARE\")){ return new Square(); } return null; } } 步骤 4 使用该工厂，通过传递类型信息来获取实体类的对象: FactoryPatternDemo.java public class FactoryPatternDemo { public static void main(String[] args) { ShapeFactory shapeFactory = new ShapeFactory(); //获取 Circle 的对象，并调用它的 draw 方法 Shape shape1 = shapeFactory.getShape(\"CIRCLE\"); //调用 Circle 的 draw 方法 shape1.draw(); //获取 Rectangle 的对象，并调用它的 draw 方法 Shape shape2 = shapeFactory.getShape(\"RECTANGLE\"); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取 Square 的对象，并调用它的 draw 方法 Shape shape3 = shapeFactory.getShape(\"SQUARE\"); //调用 Square 的 draw 方法 shape3.draw(); } } 步骤 5 执行程序，输出结果： Inside Circle::draw() method. Inside Rectangle::draw() method. Inside Square::draw() method. 建造者模式 建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。 意图：将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。 主要解决：主要解决在软件系统中，有时候面临着\"一个复杂对象\"的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。 何时使用：一些基本部件不会变，而其组合经常变化的时候。 优点： 1、建造者独立，易扩展。 2、便于控制细节风险。 缺点：1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。 用mealbuilder创建含有不同meal的对象 创建步骤： 步骤 1 创建一个表示食物条目和食物包装的接口。 步骤 2 创建实现 Packing 接口的实体类。 步骤 3 创建实现 Item 接口的抽象类，该类提供了默认的功能。 步骤 4 创建扩展了 Burger 和 ColdDrink 的实体类。 步骤 5 创建一个 Meal 类，带有上面定义的 Item 对象。 步骤 6 创建一个 MealBuilder 类，实际的 builder 类负责创建 Meal 对象。 步骤 7 BuiderPatternDemo 使用 MealBuider 来演示建造者模式（Builder Pattern）。 步骤 8 执行程序，输出结果： 适配器模式 适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种类型的设计模式属于结构型模式，它结合了两个独立接口的功能。 意图：将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 主要解决：主要解决在软件系统中，常常要将一些\"现存的对象\"放到新的环境中，而新环境要求的接口是现对象不能满足的。 何时使用：一些基本部件不会变，而其组合经常变化的时候。 优点： 1、可以让任何两个没有关联的类一起运行。 2、提高了类的复用。 3、增加了类的透明度。 4、灵活性好。 缺点： 1、过多地使用适配器，会让系统非常零乱，不易整体进行把握。2.由于JAVA至多继承一个类，所以至多只能适配一个适配者类，而且目标类必须是抽象类。 注意事项：适配器不是在详细设计时添加的，而是解决正在服役的项目的问题。 用audioPlayer调用，实现播放不同格式的文件 创建步骤： 步骤 1 为媒体播放器和更高级的媒体播放器创建接口：MediaPlayer.java，AdvancedMediaPlayer.java 步骤 2 创建实现了 AdvancedMediaPlayer 接口的实体类：VlcPlayer.java，Mp4Player.java 步骤 3 创建实现了 MediaPlayer 接口的适配器类：MediaAdapter.java 步骤 4 创建实现了 MediaPlayer 接口的实体类：AudioPlayer.java 步骤 5 使用 AudioPlayer 来播放不同类型的音频格式：AdapterPatternDemo.java 步骤 6 执行程序，输出结果： 单例模式 这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 注意： 1、单例类只能有一个实例。 2、单例类必须自己创建自己的唯一实例。 3、单例类必须给所有其他对象提供这一实例。 意图：保证一个类仅有一个实例，并提供一个访问它的全局访问点。 主要解决：一个全局使用的类频繁地创建与销毁。 何时使用：当您想控制实例数目，节省系统资源的时候。判断系统是否已经有这个单例，如果有则返回，如果没有则创建。 优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）2、避免对资源的多重占用（比如写文件操作）。 缺点： 没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 注意事项：getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。 构造函数为 private，这样该类就不会被实例化 单例模式的实现 懒汉模式是需要的时候创建对象，后面用到就判断有无现成的对象用，如果有则使用，没有则创建 饿汉模式是在编译时就创建对象，不管用没用到。有可能产生浪费资源的情况。 懒汉式，线程不安全 是否 Lazy 初始化：是 是否多线程安全：否 实现难度：易 描述：这种方式是最基本的实现方式，这种实现最大的问题就是不支持多线程。因为没有加锁synchronized，所以严格意义上它并不算单例模式。这种方式 lazy loading 很明显，不要求线程安全，在多线程不能正常工作。 public class Singleton { private static Singleton instance; private Singleton (){} public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 懒汉式，线程安全 是否 Lazy 初始化：是 是否多线程全：是 实现难度：易 描述：这种方式具备很好的 lazy loading，能够在多线程中很好的工作，但是，效率很低，99% 情况下不需要同步。 优点：第一次调用才初始化，避免内存浪费。 缺点：必须加锁 synchronized 才能保证单例，但加锁会影响效率。getInstance()的性能对应用程序不是很关键（该方法使用不太频繁） public class Singleton { private static Singleton instance; private Singleton (){} public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 饿汉式 是否 Lazy 初始化：否 是否多线程安全：是 实现难度：易 描述：这种方式比较常用，但容易产生垃圾对象。 优点：没有加锁，执行效率会提高。 缺点：类加载时就初始化，浪费内存。 它基于 classloader 机制避免了多线程的同步问题，不过，instance在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用 getInstance 方法， 但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化 instance 显然没有达到 lazy loading 的效果。 public class Singleton { private static Singleton instance = new Singleton(); private Singleton (){} public static Singleton getInstance() { return instance; } } 双检锁/双重校验锁（DCL，即 double-checked locking） JDK 版本：JDK1.5 起 是否 Lazy 初始化：是 是否多线程安全：是 实现难度：较复杂 描述：这种方式采用双锁机制，安全且在多线程情况下能保持高性能。getInstance() 的性能对应用程序很关键。 public class Singleton { private volatile static Singleton singleton; private Singleton (){} public static Singleton getSingleton() { if (singleton == null) { synchronized (Singleton.class) { if (singleton == null) { singleton = new Singleton(); } } } return singleton; } } 装饰器模式 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。 我们通过下面的实例来演示装饰器模式的用法。其中，我们将把一个形状装饰上不同的颜色，同时又不改变形状类。 意图：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。 主要解决：一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀 何时使用：在不想增加很多子类的情况下扩展类。 优点： 装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。 缺点：多层装饰比较复杂。 注意事项：可代替继承。 创建步骤： 步骤 1 创建一个接口：Shape.java 步骤 2 创建实现接口的实体类：Rectangle.java，Circle.java 步骤 3 创建实现了 Shape 接口的抽象装饰类：ShapeDecorator.java 步骤 4 创建扩展了 ShapeDecorator 类的实体装饰类：RedShapeDecorator.java 步骤 5 使用 RedShapeDecorator 来装饰 Shape 对象：DecoratorPatternDemo.java 步骤 6 执行程序，输出结果 * [设计模式参考] (https://www.runoob.com/design-pattern/design-pattern-tutorial.html) Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/design_pattern/OO-Basic.html":{"url":"basic/design_pattern/OO-Basic.html","title":"面向对象基础","keywords":"","body":"面向对象的基本特征 面向对象的三个基本特征是：封装、继承、多态 封装 封装最好理解了。封装是面向对象的特征之一，是对象和类概念的主要特性。封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。 继承 继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。通过继承创建的新类称为“子类”或“派生类”，被继承的类称为“基类”、“父类”或“超类”。 要实现继承，可以通过“继承”（Inheritance）和“组合”（Composition）来实现。 多态性 多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。 实现多态，有两种方式，覆盖和重载。覆盖和重载的区别在于，覆盖在运行时决定，重载是在编译时决定。并且覆盖和重载的机制不同，例如在 Java 中，重载方法的签名必须不同于原先方法的，但对于覆盖签名必须相同。 参考资料 面向对象的三个基本特征 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/design_pattern/GOP.html":{"url":"basic/design_pattern/GOP.html","title":"设计模式","keywords":"","body":"设计模式 简介 在 1994 年，由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides 四人合著出版了一本名为 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 的书，该书首次提到了软件开发中设计模式的概念。 四位作者合称 GOF（四人帮，全拼 Gang of Four）。他们所提出的设计模式主要是基于以下的面向对象设计原则。 对接口编程而不是对实现编程。 优先使用对象组合而不是继承。 设计模式六大原则 单一职责原则：即一个类应该只负责一项职责 里氏替换原则：所有引用基类的地方必须能透明地使用其子类的对象 依赖倒转原则：高层模块不应该依赖低层模块，二者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象 接口隔离原则：客户端不应该依赖它不需要的接口；一个类对另一个类的依赖应该建立在最小的接口上 迪米特法则：一个对象应该对其他对象保持最少的了解 开闭原则：对扩展开放，对修改关闭 设计模式归纳 参考： 设计模式六大原则 23中设计模式 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/":{"url":"basic/database/","title":"数据库","keywords":"","body":"数据库封皮 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/Definition.html":{"url":"basic/database/Definition.html","title":"基本概念","keywords":"","body":"数据库基本概念 SQL：结构化查询语言的简称， 是关系数据库的标准语言。SQL 是一种通用的、功能极强的关系数据库语言，是对关系数据存取的标准接口， 也是不同数据库系统之间互操作的基础。集数据查询、数据操作、数据定义、和数据控制功能于一体。 数据定义：数据定义功能包括模式定义、表定义、视图和索引的定义。 嵌套查询：指将一个查询块嵌套在另一个查询块的 WHERE 子句或 HAVING 短语的条件中的查询。 SQL 数据定义语句的操作对象有：模式、表、视图和索引。 索引可以分为唯一索引、非唯一索引和聚簇索引三种类型。索引相关介绍可参考：索引详解 触发器： 触发器是用户定义在基本表上的一类由事件驱动的特殊过程。由服务器自动激活，能执行更为复杂的检查和操作，具有更精细和更强大的数据控制能力。使用 CREATE TRIGGER 命令建立触发器。 数据库设计的6个基本步骤：需求分析，概念结构设计，逻辑结构设计，物理结构设计，数据库实施，数据库运行和维护。 数据库模式：是对数据库中全体数据的逻辑结构（数据项的名字、类型、取值范围等）和特征（数据之间的联系以及数据有关的安全性、完整性要求）的描述。 数据库的三级系统结构：外模式、模式和内模式。 数据库内模式：又称为存储模式，是对数据库物理结构和存储方式的描述，是数据在数据库内部的表示方式。一个数据库只有一个内模式。 数据库外模式：又称为子模式或用户模式，它是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视。通常是模式的子集。一个数据库可有多个外模式。 数据库的二级映像：外模式/模式映像、模式/内模式映像。 表相关： 8.1 主键：关系型数据库中的一条记录中有若干个属性，若其中某一个属性组(注意是组)能唯一标识一条记录，该属性组就可以成为一个主键。若表中单独一列无法成为唯一标识， 需要多个列共同标识，那么多个列的属性组称为这个表的主键。 主键约束： 可以在建表的时候设置， 也可以对已存在的表设置：PRIMARY KEY约束为表创建主键 每个表只能有一个主键。参与主键的所有列必须定义为NOT NULL.当一张表,把某个列设为主键的时候,则该列就是主键索引 CREATE TABLE table_name ( pk_column_1 data_type, pk_column_2 data type, ... PRIMARY KEY (pk_column_1, pk_column_2) 8.2 外键：这个表中存在的其他表的主键，即为这个表的外键。外键约束： 在引用表(定义外键的表)与被引用表(外键引用的表)之间创建依赖关系，被引用列必须是主键或者唯一约束，主键和外键可以是同一列。和主键不同，每个表中的外键数目不限制唯一性，在每个表中可以有0~253个外键。唯一的限制是一个给定的列只能引用一个外键。然而，一个外键可以涉及多个列。一个给定的被外键引用的列也可以被很多表引用。简单说，就是A表中的一列是B表的主键。 8.3 约束：对表中的列进行约束， 目前支持的约束类型有：主键约束，外键约束，唯一约束，默认约束，和检查约束。主键约束和外键约束对应上面的8.1和8.2。唯一约束确保表中的一列数据没有相同的值。与主键约束类似，唯一约束也强制唯一性，但唯一约束用于非主键的一列或者多列的组合，且一个表可以定义多个唯一约束。若在表中定义了默认值约束，用户在插入新的数据行时，如果该行没有指定数据，那么系统将默认值赋给该列，如果我们不设置默认值，系统默认为NULL。Check约束通过逻辑表达式来判断数据的有效性，用来限制输入一列或多列的值的范围。在列中更新数据时，所要输入的内容必须满足 Check 约束的条件，否则将无法正确输入。比如只能输入男或者女。 区别 主键 外键 索引 定义 唯一标识一条记录，不能有重复的，不允许为空 表的外键是另一表的主键, 外键可以有重复的, 可以是空值 该字段没有重复值，但可以有一个空值 作用 用来保证数据完整性 用来和其他表建立联系用的 是提高查询排序的速度 个数 主键只能有一个 一个表可以有多个外键 一个表只允许有一个唯一聚集索引。 数据库的三级模式 模式 模式又称概念模式或逻辑模式，对应于概念级。它是由数据库设计者综合所有用户的数据，按照统一的观点构造的全局逻辑结构，是对数据库中全部数据的逻辑结构和特征的总体描述，是所有用户的公共数据视图(全局视图)。它是由数据库管理系统提供的数据模式描述语言(DataDescription Language，DDL)来描述、定义的，体现、反映了数据库系统的整体观。 外模式 外模式又称子模式或用户模式，对应于用户级。它是某个或某几个用户所看到的数据库的数据视图，是与某一应用有关的数据的逻辑表示。外模式是从模式导出的一个子集，包含模式中允许特定用户使用的那部分数据。用户可以通过外模式描述语言来描述、定义对应于用户的数据记录(外模式)，也可以利用数据操纵语言(DataManipulationLanguage，DML)对这些数据记录进行。外模式反映了数据库的用户观。 内模式 内模式又称存储模式，对应于物理级，它是数据库中全体数据的内部表示或底层描述，是数据库最低一级的逻辑描述，它描述了数据在存储介质上的存储方式和物理结构，对应着实际存储在外存储介质上的数据库。内模式由内模式描述语言来描述、定义，它是数据库的存储观。在一个数据库系统中，只有唯一的数据库，因而作为定义、描述数据库存储结构的内模式和定义、描述数据库逻辑结构的模式，也是唯一的，但建立在数据库系统之上的应用则是非常广泛、多样的，所以对应的外模式不是唯一的，也不可能是唯一的。 数据库的二级映象与数据库独立性 外模式/模式映象 模式描述的是数据的全局逻辑结构，外模式描述的是数据的局部逻辑结构。当模式改变时，由数据库管理员对各个外模式/模式的映象作相应改变，可以使外模式保持不变，从而应用程序不必修改，保证了数据与程序的逻辑独立性，简称数据的逻辑独立性 模式/内模式映象 数据库中只有一个模式，也只有一个内模式，所以模式/内模式映象是唯一的，它定义了数据库全局逻辑结构与存储结构之间的对应关系。当数据库的存储结构改变了，由数据库管理员对模式/内模式映象作相应改变，可以使模式保持不变，从而应用程序也不必改变。保证了数据与程序的物理独立性，简称数据的物理独立性。 超键、候选键、主键、外键 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 视图 及 应用场景 视图是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改不影响基本表。它使得我们获取数据更容易，相比多表查询。 只暴露部分字段给访问者，所以就建一个虚表，就是视图。 查询的数据来源于不同的表，而查询者希望以统一的方式查询，这样也可以建立一个视图，把多个表查询结果联合起来，查询者只需要直接从视图中获取数据，不必考虑数据来源于不同表所带来的差异 游标(cursor) 游标是一段私有的SQL工作区,也就是一段内存区域,用于暂时存放受SQL语句影响到的数据。通俗理解就是将受影响的数据暂时放到了一个内存区域的虚表中，而这个虚表就是游标。 游标是一种能从包括多条数据记录的结果集中每次提取一条记录的机制。即游标用来逐行读取结果集。游标充当指针的作用。尽管游标能遍历结果中的所有行，但他一次只指向一行。 游标的一个常见用途就是保存查询结果，以便以后使用。游标的结果集是由SELECT语句产生，如果处理过程需要重复使用一个记录集，那么创建一次游标而重复使用若干次，比重复查询数据库要快的多。 游标的作用 大家都知道数据库中的事物可以回滚，而游标在其中起着非常重要的作用，由于对数据库的操作我们会暂时放在游标中，只要不提交，我们就可以根据游标中内容进行回滚，在一定意义有利于数据库的安全。 另外，在Oracle中PL/SQL只能返回单行数据，而游标弥补了这个不足。相当于ADO.NET中的Data table吧。 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/SQL_basic.html":{"url":"basic/database/SQL_basic.html","title":"SQL基础","keywords":"","body":"部分内容：PL/SQL，存储过程 详见对应页面内容 []加在字段名上，解决数据库保留字的问题，申明其不是保留字 内连接 关键字：inner join on 语句：select * from a_table a inner join b_table b on a.a_id = b.b_id; 说明：组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集（阴影）部分。 左连接（左外连接） 关键字：left join on / left outer join on 语句：select * from a_table a left join b_table b on a.a_id = b.b_id; 说明：left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。 右连接（右外连接） 关键字：right join on / right outer join on 语句：select * from a_table a right outer join b_table b on a.a_id = b.b_id; 说明：right join是right outer join的简写，它的全称是右外连接，是外连接中的一种。与左(外)连接相反，右(外)连接，左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来。左表记录不足的地方均为NULL。 全连接（全外连接）- MySQL目前不支持此种方式，可以用其他方式替代解决。Oracle支持。 关键字： full outer join 或者 full join 语句： SELECT * FROM TESTA FULL OUTER JOIN TESTB ON TESTA.A=TESTB.A 全外连接的等价写法，对同一表先做左连接，然后右连接 SELECT TESTA.*,TESTB.* FROM TESTA LEFT OUTER JOIN TESTB ON TESTA.A=TESTB.A UNION SELECT TESTA.*,TESTB.* FROM TESTB LEFT OUTER JOIN TESTA ON TESTA.A=TESTB.A 说明：全外连接是在等值连接的基础上将左表和右表的未匹配数据都加上 SQL 基础 WHERE 使用WHERE子句是选定返回数据的条件，可以和表的联合一起使用. WHERE有时候和从属运算IN或BETWEEN一起使用： SELECT * FROM people WHERE state `IN` ('CA','NY'); SELECT * FROM inventory WHERE prince `BETWEEN` 50 AND 100; 请注意BETWEEN操作将包括边界值（50和100） WHERE也可以和LIKE表达式一起用： 在LIKE表达式中，%是一种通配符，表示可能的模糊信息。如果想查找在某一确定的位置上有字符的数据时，可以使用另一个通配符——下划线: SELECT * FROM people WHERE firstname LIKE '_o%' 返回firstname中第二个字符为o的数据, 类似的还有`STARTING WITH`：STARTING WITH子句附加于WHERE子句上，它的作用与LIKE（%）相似。 ORDER BY 查询输出的结果按一定的排序规则来显示，ORDER BY可以使用多个字段，在ORDER BY后边的DESC表示用降序排列来代替默认的升序排列。 SELECT * FROM customers ORDER BY consumption;# DESC假如你已经知道了你想要进行排序的列是表中的第一列的话，那么你可以用ORDER BY 1 来代替输入列的名字。 GROUP BY SQL无法把正常的列和汇总函数结合在一起，这时就需要GROUP BY子句，它可以对SELECT的结果进行分组后在应用汇总函数。当要求分组结果返回多个数值时不能在SELECT子句中使用除分组列以外的列,这将会导致错误的返回值，但是你可以使用在SELECT中未列出的列进行分组。无论在什么情况下进行分组，SELECT语句中出现的字段只能是在GROUP BY中出现过的才可以。 select [columns] from table_name [where..] group by [columns] [having ...] HAVING Having字句与where子句一样可以进行条件判断的，另外Having 子句通常用来筛选满足条件的组，即在分组之后过滤数据。条件中经常包含聚合函数，使用having 条件过滤出特定的组，也可以使用多个分组标准进行分组。 SELECT goods_type, COUNT(goods_type) FROM tb_goods GROUP BY goods_type HAVING avg(price) > 100 //这里使用聚合函数 NION与UNION ALL合并 在数据库中，union和union all关键字都是将两个结果集合并为一个，但这两者从使用和效率上来说都有所不同。 union在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。在SQL运行时先取出两个表的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。 而union all只是简单的将两个结果合并后就返回。这样，如果返回的两个结果集中有重复的数据，那么返回的结果集就会包含重复的数据了。 使用 union 组合查询的结果集有两个最基本的规则： 所有查询中的列数和列的顺序必须相同。 数据类型必须兼容 select * from test_union1 union select * from test_union2 select * from test_union1 union all select * from test_union2 INTERSECT相交 INTERSECT运算符是一个集合运算符，它只返回两个查询或更多查询的交集。 INTERSECT运算符比较两个查询的结果，并返回由左和右查询输出的不同行记录。 要将INTERSECT运算符用于两个查询，应用以下规则： 列的顺序和数量必须相同。 相应列的数据类型必须兼容或可转换。 左侧查询产生一个结果集(1,2,3)，右侧查询返回一个结果集(2,3,4)。 INTERSECT操作符返回包含(2,3)，也就是两个结果集的相叉的行记录。与UNION运算符不同，INTERSECT运算符返回两个集合之间的交点。 注意，SQL标准有三个集合运算符，包括UNION，INTERSECT和MINUS。 不幸的是，MySQL不支持INTERSECT操作符。 但是我们可以模拟INTERSECT操作符。但可以通过使用内连接和DISTINCT进行模拟，实现同样的效果 MINUS相减 MINUS运算符，用于从另一个结果集中减去一个结果集。 要使用MINUS运算符，可以编写单独的SELECT语句并将MINUS运算符放在它们之间。 MINUS运算符返回第一个查询生成的唯一行，但不返回第二个查询生成的唯一行。 下图是MINUS运算符的说明。 为了获得结果集，数据库系统执行两个查询并从第二个查询中减去第一个查询的结果集。要使用MINUS运算符，SELECT子句中的列必须匹配，并且必须具有相同或至少可转换的数据类型。 我们经常在ETL中使用MINUS运算符。 ETL是数据仓库系统中的软件组件。 ETL代表Extract，Transform和Load。 ETL负责将数据从源系统加载到数据仓库系统。 完成加载数据后，可以使用MINUS运算符通过从源系统中的数据中减去目标系统中的数据来确保数据已完全加载. SELECT employee_id FROM employees MINUS SELECT employee_id FROM dependents ORDER BY employee_id; SQL分页（返回前几行) 记住这两句，再加上order by column_name (desc) 就能应付“消费第二多的客户”、“点击量第5到20名”之类的问题的。 select * from table_name limit 3,1; # 跳过前3条数据，从数据库中第4条开始查询，取一条数据，即第4条数据 select * from table_name limit 3 offset 1;# 从数据库中的第2条数据开始查询3条数据，即第2条到第4条 条件语句 MySQL里常用的条件语句是Case。Case语句分为两种：简单Case函数和Case搜索函数。 简单Case函数： CASE gender WHEN '0' THEN 'male' WHEN '1' THEN 'female' ELSE 'others' END Case搜索函数： CASE WHEN age Case语句只返回第一个符合条件的结果，剩下的条件会被自动忽略，比如上例中一个数据的age为16，那么它就在第一个case中被返回，不会进入第二个when中进行判断，因此返回'未成年人'而不是'成年人'。 关于空值（ISNULL 和 IS NOT NULL） NULL 值的处理方式与其他值不同。 NULL 用作未知的或不适用的值的占位符。 注释：无法比较 NULL 和 0；它们是不等价的。 提示：请始终使用 IS NULL 来查找 NULL 值。 我们如何仅仅选取在 \"Address\" 列中带有 NULL 值的记录呢？ 我们必须使用 IS NULL 操作符： SELECT LastName,FirstName,Address FROM Persons WHERE Address IS NULL 我们如何选取在 \"Address\" 列中不带有 NULL 值的记录呢？ 我们必须使用 IS NOT NULL 操作符： SELECT LastName,FirstName,Address FROM Persons WHERE Address IS NOT NULL 关于窗口函数 (Online Anallytical Processing) 窗口函数是为了对一组数据进行统计之后返回结果和基础信息，比如姓名，班级。普通的avg()方法等只能返回一行统计数据，不能包含基础信息。的位置，可以放以下两种函数： 1） 专用窗口函数，包括后面要讲到的rank, dense_rank, row_number等专用窗口函数。 2） 聚合函数，如sum. avg, count, max, min等 因为窗口函数是对where或者group by子句处理后的结果进行操作，所以窗口函数原则上只能写在select子句中。 一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于Order by字句之前。 over (partition by order by ) MySQL本身是不支持Window Function的（一般翻译为统计分析函数）,现在大部分的数据库语言都支持window function。 从SQL Server 2005起，SQL Server开始支持窗口函数 (Window Function)，以及到SQL Server 2012，窗口函数功能增强，目前为止支持以下几种窗口函数： 排序函数 (Ranking Function) -> rank()等； 排序函数中，ROW_NUMBER()较为常用，可用于去重、分页、分组中选择数据，生成数字辅助表等等； 排序函数在语法上要求OVER子句里必须含ORDER BY，否则语法不通过，对于不想排序的场景可以变通。 例子： 以班级“1”为例，这个班级的成绩“95”排在第1位，这个班级的“83”排在第4位。上面这个结果确实按我们的要求在每个班级内，按成绩排名了。 所得到的的SQL语句如下： select *, rank() over (partition by 班级 # Tip: partition by用来对表分组。在这个例子中，所以我们指定了按“班级”分组（partition by 班级） order by 成绩 desc) as ranking # Tip: order by子句的功能是对分组后的结果进行排序，默认是按照升序（asc）排列。在本例中（order by 成绩 desc）是按成绩这一列排序，加了desc关键词表示降序排列。 from 班级表 聚合函数 (Aggregate Function) -> over()； 聚合函数 over(partition by 字段）— 分区 聚合函数 over(order by 字段) — 框架字句 partition by字句的优点是：在同一个select语句中，一个窗口函数的计算独立于按其他列分区的其他窗口函数的计算。 例如下面的查询，返回每个员工、他的部门、他的部门中的员工数、他的职位以及跟他相同职位的员工数： select first_name,department_id,count(*) over (partition by department_id) as dept_cnt, job_id, count(*) over(partition by job_id) as job_cnt from employees *order by 2 框架字句：当在窗口函数over字句中使用order by 字句时，就指定了两件事： 分区中的行如何排序 在计算中包含哪些行 通过框架字句,允许定义数据的不同“子窗口”，以便在计算中使用，有很多方式可以指定这样的子窗口。如： range between unbounded preceding and current row 指定计算当前行开始、当前行之前的所有值； rows between 1 preceding and current row 指定计算当前行的前一行开始，其范围一直延续到当前行； range between current row and unbounded following 指定计算从当前行开始，包括它后面的所有行； rows between current row and 1 following 指定计算当前行和它后面的一行； select department_id,first_name,salary, sum(salary) over (order by hire_date range between unbounded preceding and current row) as run_total1, sum(salary) over(order by hire_date rows between 1 preceding and current row) as run_total2, sum(salary) over(order by hire_date range between current row and unbounded following) as run_total3, sum(salary) over(order by hire_date rows between current row and 1 following) as run_total4 from employees * where department_id=30 最终在显示中，每个 sum 都是一列，列名是 as 后面的字符。 分析函数 (Analytic Function) ； 分析函数是以一定的方法在一个与当前行相关的结果子集中进行计算，也称为窗口函数。 一般结构为： Function(arg1 , arg2 ……) over(partition by clause order by clause windowing clause ) Windowing clause : rows | range between start_expr and end_expr Start_expr is unbounded preceding | current row | n preceding | n following End_expr is unbounded following | current row | n preceding | n following function：是所调用的接收0个或多个参数的分析函数。分析函数包括Lag、Lead、First_value、Last_value、Rank、Dense_rank、Row_number、Percentile_cont、Ntile、Listagg等。 NEXT VALUE FOR Function， 这是给sequence专用的一个函数； 窗口函数参考内容 相关子查询 -> sql嵌套 与 exist 独立子查询：顾名思义：就是子查询和外层查询不存在任何联系，是独立于外层查询的 相关子查询：顾名思义：就是子查询里面的条件依赖于外层查询的数据 高级子查询： 业务要求： 查询出 order 表面的orderid 以及其 对应的 相邻的前面的和相邻的后面的 orderid（注意由于是订单表，可能前后的订单之间的大小并不是相差1）：使用相关子查询： select orderid, ( select MAX(orderid) from [Sales.Orders] as innerOrder where innerOrder.orderid outerOrder.orderid ) as lastOrderId from [Sales.Orders] as outerOrder 连续聚合（使用相关子查询） 业务要求：对orderid实现 累加的结果作为一个查询字段进行输出 select orderid, ( select SUM(orderid) from [Sales.Orders] as innerOrder where innerOrder.orderid 随机抽样 数据量较小可以简单实使用： SELECT * FROM table WHERE field=x ORDER BY RAND() LIMIT n； 套用结构： SELECT * FROM `lz_adv` WHERE `status` = 1 ORDER BY RAND() LIMIT 1; 对于百万千万级表，以下可做到高效查询： SELECT * FROM `tableName` WHERE id >= (SELECT floor(RAND() * ((SELECT MAX(id) FROM`tableName`) - (SELECT MIN(id) FROM `tableName`)) + (SELECT MIN(id) FROM `tableName`))) ORDER BY id LIMIT N 操作数据 - 增删查改 增： INSERT INTO Persons (LastName, Address) VALUES ('Wilson', 'Champs-Elysees‘） 删：DELETE FROM table_name或DELETE * FROM table_name（注意，删除行，并不删除表）； 查：SELECT * FROM Persons WHERE City LIKE 'N%' 改：UPDATE Person SET Address = 'Zhongshan 23', City = 'Nanjing' WHERE LastName = 'Wilson' sum函数：SELECT SUM(OrderPrice) AS OrderTotal FROM Orders；as表示生成的数据的列名是OrderTotal，Sum只返回统计值 count函数：SELECT COUNT(Customer) AS CustomerNilsen FROM Orders WHERE Customer='Carter'；返回指定列的值的数目（NULL 不计入） COUNT(*) 函数返回表中的记录数，即表中有多少条记录 COUNT(DISTINCT column_name) 函数返回指定列的不同值的数目 group by:SELECT Customer,OrderDate,SUM(OrderPrice) FROM Orders GROUP BY Customer,OrderDate 多表联合查询：select column1,column2,column3 from table_name1,talbe_name2 where table_name1.column = table_name2.column； 注意：多表联合查询两个表中要有一个记录相同信息的列 column。 创建和操作表 创建数据库： CREATE DATABASE 数据库名; 连接数据库： USE test_sql; 删除数据库： DROP DATABASE test_sql; 删除表： DROP TABLE t_student; 创建表： CREATE TABLE ( []); CREATE TABLE t_student( student_name VARCHAR(10), student_birthday DATETIME, student_phone INT, student_score FLOAT); 删除表： DROP TABLE t_student; 复制表： 注意：复制表的同时表的约束并不能复制过来。只复制内容与结构。 CREATE TABLE copy_student SELECT * FROM t_student; 修改表 添加新列：ALTER TABLE t_student ADD student_address VARCHAR(50); 更改列 ：ALTER TABLE t_student CHANGE student_birthday student_age INT; 删除列 ：ALTER TABLE t_student DROP COLUMN student_score; 数据库完整性 实体完整性--主键约束，唯一约束 (保证一行数据是有效的)： PRIMARY KEY 主键列不能为空也不能重复，通常加在表的id列中。 CREATE TABLE t_student( student_id INT PRIMARY KEY, student_name VARCHAR(10), student_birthday DATETIME, student_phone INT, student_score FLOAT); UNIQUE 唯一约束是指给定列的值必须唯一，与主键约束不同的是它可以为空。通常加在表中不能重复的信息中，如电话号码。 CREATE TABLE t_student( student_id INT PRIMARY KEY, student_name VARCHAR(10), student_birthday DATETIME, student_phone INT UNIQUE, student_score FLOAT); 域完整性 --非空约束，默认约束 (保证一列数据是有效的)： NOT NULL - 非空约束 非空约束可以加在诸如姓名等列上。 CREATE TABLE t_student( student_id INT PRIMARY KEY, student_name VARCHAR(10) NOT NULL, student_birthday DATETIME, student_phone INT UNIQUE, student_score FLOAT); 设定默认值后，可以在添加此列时不指定值,数据库会自动填充设定的默认值。 - 默认约束 DEFAULT CREATE TABLE t_student( student_id INT PRIMARY KEY, student_name VARCHAR(10) NOT NULL, student_sex VARCHAR(5) DEFAULT '男', student_birthday DATETIME, student_phone INT UNIQUE, student_score FLOAT); 引用完整性--外键约束， (保证引用的编号是有效的)： CREATE TABLE t_student( student_id INT PRIMARY KEY, s_c_id INT REFERENCES t_class(class_id), student_name VARCHAR(10) NOT NULL, student_sex VARCHAR(5) DEFAULT '男', student_birthday DATETIME, student_phone INT UNIQUE, student_score FLOAT CONSTRAINT FOREIGN KEY(s_c_id) REFERENCES t_class(class_id); 用户自定义完整性--主键约束 (保证自定义规则)： SQL函数 内建 SQL 函数的语法是： SELECT function(列) FROM 表 函数的基本类型是： Aggregate 函数 Aggregate 函数的操作面向一系列的值，并返回一个单一的值。 Scalar 函数 Scalar 函数的操作面向某个单一的值，并返回基于输入值的一个单一的值。 函数类型 函数 描述 Aggregate functions AVG(column) 返回某列的平均值 Aggregate functions AVG(column) 返回某列的平均值 Aggregate functions COUNT(column) 返回某列的行数 (不包括 NULL 值) Aggregate functions COUNT(*) 返回被选行数 Aggregate functions FIRST(column) 返回在指定的域中第一个记录的值 Aggregate functions LAST(column) 返回在指定的域中最后一个记录的值 Aggregate functions MAX(column) 返回某列的最高值 Aggregate functions MIN(column) 返回某列的最低值 Scalar 函数 UCASE() 将某个域转换为大写 Scalar 函数 LCASE() 将某个域转换为小写 Scalar 函数 MID(c,start[,end]) 从某个文本域提取字符 Scalar 函数 LEN() 返回某个文本域的长度 Scalar 函数 INSTR(c,char) 返回在某个文本域中指定字符的数值位置 Scalar 函数 LEFT(c,number_of_char) 返回某个被请求的文本域的左侧部分 Scalar 函数 RIGHT(c,number_of_char) 返回某个被请求的文本域的右侧部分 Scalar 函数 ROUND(c,decimals) 对某个数值域进行指定小数位数的四舍五入 Scalar 函数 MOD(x,y) 返回除法操作的余数 Scalar 函数 NOW() 返回当前的系统日期 Scalar 函数 FORMAT(c,format) 改变某个域的显示方式 Scalar 函数 DATEDIFF(d,date1,date2) 用于执行日期计算 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/SQL_adv.html":{"url":"basic/database/SQL_adv.html","title":"SQL优化","keywords":"","body":"SQL 优化思路 对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描。 备注、描述、评论之类的可以设置为 NULL，其他的，最好不要使用NULL。 不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。 应尽量避免在 where 子句中使用 != 或 <> 操作符，否则将引擎放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描 可以这样查询： select id from t where num = 10 union all select id from t where Name = 'admin' in 和 not in 也要慎用，否则会导致全表扫描.对于连续的数值，能用 between 就不要用 in 了.很多时候用 exists 代替 in 是一个好的选择。 select id from t where num in(1,2,3) select id from t where num between 1 and 3 select num from a where num in(select num from b) select num from a where exists(select 1 from b where num=a.num) 模糊查询也将导致全表搜索 如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。 可以改为强制查询使用索引： select id from t with(index(索引名)) where num = @num 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2 = 100; 应改为： select id from t where num = 100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3) = ’abc’ -–name以abc开头的id select id from t where datediff(day,createdate,’2005-11-30′) = 0 -–‘2005-11-30’ --生成的id 应改为： select id from t where name like 'abc%' select id from t where createdate >= '2005-11-30' and createdate 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 Update 语句，如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。 对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。 select count(*) from table；这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。 应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连 接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 任何地方都不要使用 select from t ，用具体的字段列表代替“”，不要返回用不到的任何字段。 尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 避免频繁创建和删除临时表，以减少系统表资源的消耗。临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件， 最好使用导出表。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 尽量避免大事务操作，提高系统并发能力。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 实际案例分析：拆分大的 DELETE 或INSERT 语句，批量提交SQL语句 如果你需要在一个在线的网站上去执行一个大的 DELETE 或 INSERT 查询，你需要非常小心，要避免你的操作让你的整个网站停止相应。因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。 Apache 会有很多的子进程或线程。所以，其工作起来相当有效率，而我们的服务器也不希望有太多的子进程，线程和数据库链接，这是极大的占服务器资源的事情，尤其是内存。 如果你把你的表锁上一段时间，比如30秒钟，那么对于一个有很高访问量的站点来说，这30秒所积累的访问进程/线程，数据库链接，打开的文件数，可能不仅仅会让你的WEB服务崩溃，还可能会让你的整台服务器马上挂了。 所以，如果你有一个大的处理，你一定把其拆分，使用 LIMIT oracle(rownum),sqlserver(top)条件是一个好的方法。下面是一个mysql示例： while(1){ 　　//每次只做1000条 　　 mysql_query(“delete from logs where log_date Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/Index.html":{"url":"basic/database/Index.html","title":"索引 和 锁","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/Procedure.html":{"url":"basic/database/Procedure.html","title":"存储过程","keywords":"","body":"存储过程的概念（Stored Procedure） 存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集。经编译后存储在数据库中。 存储过程是数据库中的一个重要对象，用户通过指定存储过程的名字并给出参数（可以有参数，也可以没有）来执行它。 存储过程是由 流控制 和 SQL语句书写的过程，这个过程经编译和优化后存储在数据库服务器中。 存储过程 可由应用程序通过一个调用来执行，而且允许用户声明变量。 同时，存储过程可以接收和输出参数、返回执行存储过程的状态值，也可以嵌套调用。 存储过程的优点 存储过程的使用大大增强了SQL语言的功能和灵活性。 存储过程可以用流控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 可保证数据的安全性和完整性。 通过存储过程可以使没有权限的用户在控制之下间接地存取数据库，从而保证数据的安全。 通过存储过程可以使相关的动作在一起发生，从而可以维护数据库的完整性。（就像事务的原子性：要么事务内的所有SQL语句全部执行成功，要么全部不成功） 在运行存储过程前，数据库已对其进行了语法和句法分析，并给出了优化执行方案。 这种已经编译好的过程可极大地改善SQL语句的性能。 由于执行SQL语句的大部分工作已经完成（因为已经提前经过编译），所以存储过程能以极快的速度执行。 可以降低网络的通信量。 客户端调用存储过程只需要传存储过程名和相关参数即可，与传输SQL语句相比自然数据量少了很多（在远程访问时体现出来）。 存储过程只在创造时进行编译，以后每次执行存储过程都不需再重新编译，而一般SQL语句每执行一次就编译一次,所以使用存储过程可提高数据库执行速度。 当对数据库进行复杂操作时(如对多个表进行Update,Insert,Query,Delete时)，可将此复杂操作用存储过程封装起来与数据库提供的事务处理结合一起使用。 比如每一步对数据库的操作用一个事务来完成，把这些事务全都放在一个存储过程中。 存储过程可以重复使用,可减少数据库开发人员的工作量。 安全性高,可设定只有某些用户才具有对指定存储过程的使用权 存储过程缺点 调试麻烦：但是用 PL/SQL Developer 调试很方便！弥补这个缺点。 移植问题：数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。 重新编译问题：因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。 比如A存储过程调用B存储过程，使用B的返回值作为参数，如果B的参数或返回值发生改变时，会对调用她的A产生影响，此时存储过程就要重新编译，设置成运行时刻自动编译。 维护比较困难：如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。 存储过程的特性 存储过程与函数的区别 返回值：函数只能返回一个变量，而存储过程可以返回多个。对于存储过程来说可以返回参数，如记录集，而函数只能返回值或者表对象 存储过程一般是作为一个独立的部分来执行（ EXECUTE 语句执行），而函数可以作为查询语句的一个部分来调用（SELECT调用），由于函数可以返回一个表对象，因此它可以在查询语句中位于FROM关键字的后面。 SQL语句中不可用存储过程，而可以使用函数。 存储过程实现的功能要复杂一点，而函数的实现的功能针对性比较强，比较单一。 存储过程与事务的区别 存储位置：事务在程序中被调用，保存在调用以及实现它的代码中，存储过程可以在数据库客户端直接被调用，经编译后存储在数据库中。 运行方式：事务在每次被调用的时候执行其中的SQL语句，存储过程预先经过编译，并不是每次被调用时都会执行一遍其中的SQL语句。 事务有严格的一致性和原子性，使用的安全性高，存储过程则没有这些特性，在进行一些复杂的操作时，为了保证操作的准确性，可以在存储过程中调用事务，然后判断事务的执行结果是否成功来确保操作的准确性。 触发器 概念及作用 触发器是一种特殊类型的存储过程，它不同于我们前面介绍过的存储过程。触发器主要是通过事件进行触发而被执行的，而存储过程可以通过存储过程名字而被直接调用。当对某一表进行诸如Update、 Insert、 Delete 这些操作时，SQL Server就会自动执行触发器所定义的SQL 语句，从而确保对数据的处理必须符合由这些SQL 语句所定义的规则。 功能 触发器的主要作用就是其能够实现由主键和外键所不能保证的复杂的参照完整性和数据的一致性。 除此之外，触发器还有其它许多不同的功能： 强化约束(Enforce restriction) 触发器能够实现比CHECK 语句更为复杂的约束。 跟踪变化(Auditing changes) 触发器可以侦测数据库内的操作，从而不允许数据库中未经许可的指定更新和变化。 级联运行(Cascaded operation)。 触发器可以侦测数据库内的操作，并自动地级联影响整个数据库的各项内容。例如，某个表上的触发器中包含有对另外一个表的数据操作(如删除，更新，插入)而该操作又导致该表上触发器被触发。 存储过程的调用(Stored procedure invocation)。 为了响应数据库更新,触发器可以调用一个或多个存储过程，甚至可以通过外部过程的调用而在DBMS(数据库管理系统)本身之外进行操作。 由此可见，触发器可以解决高级形式的业务规则或复杂行为限制以及实现定制记录等一些方面的问题。例如，触发器能够找出某一表在数据修改前后状态发生的差异，并根据这种差异执行一定的处理。此外一个表的同一类(Insert、 Update、Delete)的多个触发器能够对同一种数据操作采取多种不同的处理。 总体而言，触发器性能通常比较低。当运行触发器时，系统处理的大部分时间花费在参照其它表的这一处理上，因为这些表既不在内存中也不在数据库设备上，而删除表和插入表总是位于内存中。可见触发器所参照的其它表的位置决定了操作要花费的时间长短。 存储过程的语法和参数 --------------创建存储过程----------------- CREATE PROC [ EDURE ] procedure_name [ ; number ] [ { @parameter data_type } [ VARYING ] [ = default ] [ OUTPUT ] ] [ ,...n ] [ WITH { RECOMPILE | ENCRYPTION | RECOMPILE , ENCRYPTION } ] [ FOR REPLICATION ] AS sql_statement [ ...n ] --------------调用存储过程----------------- EXECUTE Procedure_name '' --存储过程如果有参数，后面加参数格式为：@参数名=value，也可直接为参数值value --------------删除存储过程----------------- drop procedure procedure_name --在存储过程中能调用另外一个存储过程，而不能删除另外一个存储过程 存储过程例子 只返回单一记录集的存储过程 结果：相当于运行 select * from UserAccount 这行代码，结果为整个表的数据。 -------------创建名为GetUserAccount的存储过程---------------- create Procedure GetUserAccount as select * from UserAccount go -------------执行上面的存储过程---------------- exec GetUserAccount 没有输入输出的存储过程 结果：相当于运行 insert into UserAccount (UserName,[PassWord],RegisterTime,RegisterIP) values(9,9,'2013-01-02',9) 这行代码。 -------------创建名为GetUserAccount的存储过程---------------- create Procedure inUserAccount as insert into UserAccount (UserName,[PassWord],RegisterTime,RegisterIP) values(9,9,'2013-01-02',9) go -------------执行上面的存储过程---------------- exec inUserAccount 有返回值的存储过程 解释：这里的@@rowcount为执行存储过程影响的行数，执行的结果是不仅插入了一条数据，还返回了一个值即 return value =1 ，这个可以在程序中获取 -------------创建名为GetUserAccount的存储过程---------------- create Procedure inUserAccountRe as insert into UserAccount (UserName,[PassWord],RegisterTime,RegisterIP) values(10,10,'2013-01-02',10) return @@rowcount go -------------执行上面的存储过程---------------- exec inUserAccountRe 有输入参数和输出参数的存储过程 解释：@UserName为输入参数，@UserID为输出参数。 运行结果为@userID为COOUT（*）即 =1。 -------------创建名为GetUserAccount的存储过程---------------- create Procedure GetUserAccountRe @UserName nchar(20), @UserID int output as if(@UserName>5) select @UserID=COUNT(*) from UserAccount where UserID>25 else set @UserID=1000 go -------------执行上面的存储过程---------------- exec GetUserAccountRe '7',null 同时具有返回值、输入参数、输出参数的存储过程 结果：@userID为COOUT（*）即 =1，Retun Value=1。 -------------创建名为GetUserAccount的存储过程---------------- create Procedure GetUserAccountRe1 @UserName nchar(20), @UserID int output as if(@UserName>5) select @UserID=COUNT(*) from UserAccount where UserID>25 else set @UserID=1000 return @@rowcount go -------------执行上面的存储过程---------------- exec GetUserAccountRe1 '7',null 返回多个记录集的存储过程 结果：返回两个结果集，一个为 select from UserAccount，另一个为 select from UserAccount where UserID>5 。 -------------创建名为GetUserAccount的存储过程---------------- create Procedure GetUserAccountRe3 as select * from UserAccount select * from UserAccount where UserID>5 go -------------执行上面的存储过程---------------- exec GetUserAccountRe3 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/Transaction.html":{"url":"basic/database/Transaction.html","title":"事务","keywords":"","body":"事务概念 事务（Transaction）是指构成单一逻辑工作单元的操作集合，要么完整地执行，要么完全不执行。 如果事务中有的操作没有成功完成，则事务中的所有操作都需要被回滚，回到事务执行前的状态（要么全执行，要么全都不执行）； 同时，该事务对数据库或者其他事务的执行无影响，所有的事务都好像在独立的运行。 事务四大特性(ACID) 原子性（Atomicity）: 事务要么全部完成，要么全部取消。 如果事务崩溃，状态回到事务之前（事务回滚）。 原子性，确保不管交易过程中发生了什么意外状况（服务器崩溃、网络中断等），不能出现A账户少了一个亿，但B账户没到帐，或者A账户没变，但B账户却凭空收到一个亿（数据不一致）。A和B账户的金额变动要么同时成功，要么同时失败(保持原状)。 隔离性（Isolation）: 如果2个事务 T1 和 T2 同时运行，事务 T1 和 T2 最终的结果是相同的，不管 T1和T2谁先结束。 隔离性，如果A在转账1亿给B（T1），同时C又在转账3亿给A（T2），不管T1和T2谁先执行完毕，最终结果必须是A账户增加2亿，而不是3亿，B增加1亿，C减少3亿。 持久性（Durability）: 一旦事务提交，不管发生什么（崩溃或者出错），数据要保存在数据库中。 持久性，确保如果 T1 刚刚提交，数据库就发生崩溃，T1执行的结果依然会保持在数据库中。 一致性（Consistency）: 只有合法的数据（依照关系约束和函数约束）才能写入数据库。 一致性，确保钱不会在系统内凭空产生或消失， 依赖原子性和隔离性。 事务的隔离级别 如果两个事务同时对一个表进行操作，就需要给表加上互斥锁。所以，在事务中更新某条数据获得的互斥锁，只有在事务提交或失败之后才会释放，在此之前，其他事务是只能读，不能写这条数据的。 Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 Read committed (读已提交)：可避免脏读的发生。 Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 幻读：T1, T2两个事务同时对表进行操作。T1写入一条数据，由于两个事务同时操作一个表，T2两次读表，得到的结果不同，即为幻读。不隔离新增的数据。 不可重复读：不可重复读是一个事务执行过程中，另一事务提交并修改了当前事务正在读取的数据，此时产生冲突。 不可重复读是两个并发的事务，“事务A：消费”、“事务B：老婆网上转账”，事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。 产生脏读：事务执行过程中，T1发生回滚，此时T2读到的数据没有任意义。这个叫脏读。 脏读是两个并发的事务，“事务A：领导发工资”、“事务B：我查询工资账户”，事务B读取了事务A尚未提交的数据。 如何保证持久性 成功提交的事务，数据会保存到磁盘 未提交的事务，相应的数据会回滚 事务日志 LSN：一个按时间顺序分配的唯一日志序列号，靠后的操作的LSN比靠前的大。 TransID：产生操作的事务ID。 PageID：被修改的数据在磁盘上的位置，数据以页为单位存储。 PrevLSN：同一个事务产生的上一条日志记录的指针。 UNDO：取消本次操作的方法，按照此方法回滚。 REDO：重复本次操作的方法，如有必要，重复此方法保证操作成功。 磁盘上每个页（保存数据的，不是保存日志的）都记录着最后一个修改该数据操作的LSN。数据库会通过解析事务日志，将修改真正落到磁盘上(写盘)，随后清理事务日志(正常情况下)。 将数据的变更以事务日志的方式，按照时间先后追加到日志缓冲区，由特定算法写入事务日志，这是顺序IO，性能较好 通过数据管理器解析事务日志，由特定的算法择机进行写盘 数据库恢复 当数据库从崩溃中恢复时，会有以下几个步骤,经过这几个阶段，在数据库恢复后，可以达到奔溃前的状态，也保证了数据的一致性： 解析存在的事务日志，分析哪些事务需要回滚，哪些需要写盘(还没来得及写盘，数据库就崩溃了)。 Redo，进行写盘。检测对应数据所在数据页的LSN，如果数据页的LSN>=事务操作的LSN，说明已经写过盘，不然进行写盘操作。 Undo, 按照LSN倒序进行回滚 事务的编写 BEGIN TRANSACTION：事务的起始点 COMMIT TRANSACTION ：提交事务 ROLLBACK TRANSACTION ：滚到事务的起始点或事务内的某个保存点 --@@TRANCOUNT 函数记录当前事务的嵌套级。 --每一次Begin Transaction都会引起@@TranCount加1。 --而每一次Commit Transaction都会使@@TranCount减1。 --而RollBack Transaction会回滚所有的嵌套事务包括已经提交的事务和未提交的事务，而使@@TranCount置0。 BEGIN TRAN TestTran; BEGIN TRY INSERT INTO TranTest VALUES (1); INSERT INTO TranTest VALUES (NULL); INSERT INTO TranTest VALUES (2); END TRY BEGIN CATCH IF @@TRANCOUNT > 0 ROLLBACK TRAN TestTran; END CATCH IF @@TRANCOUNT > 0 COMMIT TRAN TestTran; GO Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/database/Formula.html":{"url":"basic/database/Formula.html","title":"范式","keywords":"","body":"数据库四大范式 满足范式要求的数据库设计是结构清晰的，同时可避免数据冗余和操作异常。这并意味着不符合范式要求的设计一定是错误的，在数据库表中存在1：1或1：N关系这种较特殊的情况下，合并导致的不符合范式要求反而是合理的。 第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式 数据库的设计范式是数据库设计所需要满足的规范，满足这些规范的数据库是简洁的、结构明晰的，同时，不会发生插入（insert）、删除（delete）和更新（update）操作异常。 第一范式（1NF）：数据库表中的字段都是单一属性的，不可再分。确保每列保持原子性 第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。 上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式（2NF）：确保表中的每列都和主键相关 第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键，如下表所示。 这样就产生一个问题：这个表中是以订单编号和商品编号作为联合主键。这样在该表中商品名称、单位、商品价格等信息不与该表的主键相关，而仅仅是与商品编号相关。所以在这里违反了第二范式的设计原则。 而如果把这个订单信息表进行拆分，把商品信息分离到另一个表中，把订单项目表也分离到另一个表中，就非常完美了。如下所示。 这样设计，在很大程度上减小了数据库的冗余。如果要获取订单的商品信息，使用商品编号到商品信息表中查询即可。 第三范式（3NF）：确保每列都和主键列直接相关,而不是间接相关。 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。 比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。如下面这两个表所示的设计就是一个满足第三范式的数据库表。 这样在查询订单信息的时候，就可以使用客户编号来引用客户信息表中的记录，也不必在订单信息表中多次输入客户信息的内容，减小了数据冗余。 鲍依斯-科得范式（BCNF）：在第三范式的基础上，数据库表中如果不存在任何字段对任一候选关键字段的传递函数依赖则符BCNF范式。 Boyce-Codd Normal Form。没有新的规范加入，是对第二范式和第三范式的一个加强。 所有的非主属性对每一个码都是完全函数依赖 （暗含 主关键字里面可能有多个码可以将实体区分） 所有的主属性对每一个不包含它的码也是完全函数依赖（即所选码与未选择的码之间也是完全函数依赖的） 没有任何属性完全函数依赖于非码的任何一组属性（即非主属性之间不能函数依赖） 解释： 例如关系模式 S(Sno,Sname,Sdept,Sage) 假设 Sname具有唯一性 解释条件1：非主属性 （Sdept,Sage） 不仅依赖于Sno,而且依赖于Sname,因为不仅可以通过学号知道学生的信息，还可以通过姓名知道学生的信息。 解释条件2：Sno 与Sname之间也是完全函数依赖关系 解释条件3：没有任何一个属性函数依赖于Sdept和Sage Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/scm/":{"url":"basic/scm/","title":"版本控制","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/scm/Git.html":{"url":"basic/scm/Git.html","title":"Git","keywords":"","body":" https://progit.org/ http://think-like-a-git.net/ Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"basic/scm/SVN.html":{"url":"basic/scm/SVN.html","title":"SVN","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/C++/":{"url":"Programming_language/C++/","title":"C++ 基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/C++/basic_C++.html":{"url":"Programming_language/C++/basic_C++.html","title":"语法基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Java/":{"url":"Programming_language/Java/","title":"Java 基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Java/Java_basic.html":{"url":"Programming_language/Java/Java_basic.html","title":"语法基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Java/Java_interview.html":{"url":"Programming_language/Java/Java_interview.html","title":"Java 面试","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Python/":{"url":"Programming_language/Python/","title":"Python 基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Python/Python_basic.html":{"url":"Programming_language/Python/Python_basic.html","title":"语法基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Python/Python_interview.html":{"url":"Programming_language/Python/Python_interview.html","title":"Python 面试","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Matlab/":{"url":"Programming_language/Matlab/","title":"Matlab 基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Matlab/Matlab_basic.html":{"url":"Programming_language/Matlab/Matlab_basic.html","title":"语法基础","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Programming_language/Matlab/Matlab_interview.html":{"url":"Programming_language/Matlab/Matlab_interview.html","title":"Matlab 面试","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Big_data/":{"url":"Big_data/","title":"大数据","keywords":"","body":" Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/AI_intro.html":{"url":"Artificial_Intelligence/AI_intro.html","title":"AI杂谈面试问题","keywords":"","body":"机器学习经典算法应用 分类问题： 逻辑回归(工业界最常用) 支持向量机 随机森林 朴素贝叶斯(NLP中常用) 深度神经网络(视频、图片、语音等多媒体数据中使用) 处理回归问题的常用算法包括： 线性回归 普通最小二乘回归（Ordinary Least Squares Regression） 逐步回归（Stepwise Regression） 多元自适应回归样条（Multivariate Adaptive Regression Splines） 处理聚类问题的常用算法包括： K均值（K-means） 基于密度聚类 LDA (Latent Dirichlet Allocation) 等等。 Linear Discriminant Analysis, 以下简称LDA 降维算法： 主成分分析（PCA） 奇异值分解（SVD） 推荐系统： 协同过滤算法 模型融合(model ensemble)和提升(boosting)的算法包括： bagging adaboost GBDT GBRT 工业界使用相关： 数据处理： Hadoop: 基本上是工业界的标配了。一般用来做特征清洗、特征处理的相关工作。 Spark: 提供了MLlib这样的大数据机器学习平台，实现了很多常用算法。但可靠性、稳定性上有待提高。 基本工作流程： 将问题抽象成数学问题： 明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。 获取数据： 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。 数据要有代表性，否则必然会过拟合。 对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。 要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。 特征预处理与特征选择 良好的数据要能够提取出良好的特征才能真正发挥效力。 特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。 训练模型与调优 调参，调参，调参。。。 模型诊断 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。 误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题…… 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。 模型融合 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。 上线运行 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。 机器学习理论类： 1. 写出全概率公式&贝叶斯公式 2. 模型训练为什么要引入偏差(bias)和方差(variance)？ 证 3. CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型 4. 如何解决过拟合问题？ 5. One-hot的作用是什么？为什么不直接使用数字作为表示 6. 决策树和随机森林的区别是什么？ 7. 朴素贝叶斯为什么“朴素naive”？ 8. kmeans初始点除了随机选取之外的方法 9. LR明明是分类模型为什么叫回归 10. 梯度下降如何并行化 11. LR中的L1/L2正则项是啥 12. 简述决策树构建过程 13. 解释Gini系数 14. 决策树的优缺点 15. 出现估计概率值为 0 怎么处理 16. 随机森林的生成过程 17. 介绍一下Boosting的思想 18. gbdt的中的tree是什么tree？有什么特征 19. xgboost对比gbdt/boosting Tree有了哪些方向上的优化 20. 什么叫最优超平面 21. 什么是支持向量 22. SVM如何解决多分类问题 23. 核函数的作用是啥 特征工程类： 1. 怎么去除DataFrame里的缺失值？ 2. 特征无量纲化的常见操作方法 3. 如何对类别变量进行独热编码？ 4. 如何把“年龄”字段按照我们的阈值分段？ 5. 如何根据变量相关性画出热力图？ 6. 如何把分布修正为类正态分布？ 7. 怎么简单使用PCA来划分数据且可视化呢？ 8. 怎么简单使用LDA来划分数据且可视化呢？ 深度学习类： 1. 你觉得batch-normalization过程是什么样的 2. 激活函数有什么用？常见的激活函数的区别是什么？ 3. Softmax的原理是什么？有什么作用？ CNN的平移不变性是什么？如何实现的？ 4. VGG，GoogleNet，ResNet等网络之间的区别是什么？ 5. 残差网络为什么能解决梯度消失的问题 6. LSTM为什么能解决梯度消失/爆炸的问题 7. Attention对比RNN和CNN，分别有哪点你觉得的优势 8. 写出Attention的公式 9. Attention机制，里面的q,k,v分别代表什么 10. 为什么self-attention可以替代seq2seq 自然语言处理（NLP）类： 1. GolVe的损失函数 2. 为什么GolVe会用的相对比W2V少 3. 层次softmax流程 4. 负采样流程 5. 怎么衡量学到的embedding的好坏 6. 阐述CRF原理 7. 详述LDA原理 8. LDA中的主题矩阵如何计算 9. LDA和Word2Vec区别？LDA和Doc2Vec区别 10. Bert的双向体现在什么地方 11. Bert的是怎样预训练的 12. 在数据中随机选择 15% 的标记，其中80%被换位[mask]，10%不变、10%随机替换其他单词，原因是什么 13. 为什么BERT有3个嵌入层，它们都是如何实现的 14. 手写一个multi-head attention 推荐系统类： 1. DNN与DeepFM之间的区别 2. 你在使用deepFM的时候是如何处理欠拟合和过拟合问题的 3. deepfm的embedding初始化有什么值得注意的地方吗 4. YoutubeNet 变长数据如何处理的 5. YouTubeNet如何避免百万量级的softmax问题的 6. 推荐系统有哪些常见的评测指标？ 7. MLR的原理是什么？做了哪些优化？ 计算机视觉（CV）类： 1. 常见的模型加速方法 2. 目标检测里如何有效解决常见的前景少背景多的问题 3. 目标检测里有什么情况是SSD、YOLOv3、Faster R-CNN等所不能解决的，假设网络拟合能力无限强 4. ROIPool和ROIAlign的区别 5. 介绍常见的梯度下降优化方法 6. Detection你觉的还有哪些可做的点 7. mini-Batch SGD相对于GD有什么优点 8. 人体姿态估计主流的两个做法是啥？简单介绍下 9. 卷积的实现原理以及如何快速高效实现局部weight sharing的卷积操作方式 10. CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/math.html":{"url":"Artificial_Intelligence/math.html","title":"机器学习数学基础","keywords":"","body":"机器学习数学基础 目录 概率统计 概率核心理论 核心的几种随机变量的分布以及变量之间的关系 参数估计理论 随机理论的相关概念 信息论 随机过程初步理论和应用 时间序列分析 线性代数 矩阵基本计算 行列式 矩阵的逆矩阵 矩阵求导 概率统计 概率核心理论 主要参考 及 例子 经典分布 分布参考 均匀分布（连续）伯努利分布（离散）二项分布（离散）多伯努利分布，分类分布（离散）多项式分布（离散） β分布（连续）Dirichlet 分布（连续）伽马分布（连续）指数分布（连续）高斯分布（连续）正态分布（连续） 卡方分布（连续）t 分布（连续） 期望 方差 条件概率 和 全概率 联合概率 和 边缘概率 贝叶斯公式 肝癌检测报告 例子 朴素贝叶斯 朴素贝叶斯参考 朴素贝叶斯参考 朴素贝叶斯参考 朴素贝叶斯参考 核心的几种随机变量的分布以及变量之间的关系 分布的期望、方差等数字特征，了解概率密度函数和累积分布函数。对多组不同的变量，熟悉协方差以及相关性的意义和计算方法。 参数估计理论 需要重点掌握最小偏差无偏估计、最大似然估计和贝叶斯估计的相关内容。并且学习EM算法。 随机理论的相关概念 掌握蒙特卡罗方法的基本思想。同时巩固贝叶斯的思想方法，接触一下马尔科夫蒙特卡洛（MCMC）算法，找一找处理实际问题的感觉。 信息论 学习关于熵的一些理论，联合熵、条件熵、交叉熵、相对熵、互信息等概念，以及最大熵模型。 参考 随机过程初步理论和应用 首先马尔科夫链是必须学习的，了解状态转移矩阵、多步转移、几种不同的状态分类、平稳分布等最基本的内容。然后在此概念基础上，学习隐马尔科夫链的相关内容，聚焦其基本概念，以及概率计算和参数学习的一些方法。 时间序列分析 重点是移动平均、相关性以及预测等内容。 线性代数 矩阵基本计算 行列式 矩阵的逆矩阵 矩阵求导 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-28 09:59:12 "},"Artificial_Intelligence/Machine_Learning/ML_intro.html":{"url":"Artificial_Intelligence/Machine_Learning/ML_intro.html","title":"Machine learning","keywords":"","body":" Machine learning method Supervised machine learning algorithms： 用于有标签的，已分类的数据。用已知的知识去预测分类新的知识。 unsupervised machine learning algorithms ： 用于训练的数据既没有分类也没有标记。无监督学习研究如何从未标记的数据中推断出隐藏结构。无监督学习无法找出正确的输出，但可以浏览数据并得到推论，以描述未标记数据中的结构。 Semi-supervised machine learning algorithms ：半监督学习算法。介于有监督和无监督学习之间，同时使用标记和未标记数据进行训练。通常使用少量有标签数据和大量无标签数据进行训练，这种方法可以大大提高准确性。通常，当需要标记的数据需要相关和熟练的资源以供其训练时，选择半监督学习。半监督学习方法的性能依赖于所用的半监督假设。 Reinforcement machine learning algorithms： 强化机器学习。 通过产生动作并发现错误或奖励来与环境互动。反复试验和延迟奖励是强化学习的特征。强化学习允许机器和软件代理自动确定特定上下文的理想行为，以使其性能最大化。代理需要反馈，以了解那种行动最好，这就是增强信号。 Machine learning method Supervised machine learning algorithms： 用于有标签的，已分类的数据。用已知的知识去预测分类新的知识。 unsupervised machine learning algorithms ： 用于训练的数据既没有分类也没有标记。无监督学习研究如何从未标记的数据中推断出隐藏结构。无监督学习无法找出正确的输出，但可以浏览数据并得到推论，以描述未标记数据中的结构。 Semi-supervised machine learning algorithms ：半监督学习算法。介于有监督和无监督学习之间，同时使用标记和未标记数据进行训练。通常使用少量有标签数据和大量无标签数据进行训练，这种方法可以大大提高准确性。通常，当需要标记的数据需要相关和熟练的资源以供其训练时，选择半监督学习。半监督学习方法的性能依赖于所用的半监督假设。 Reinforcement machine learning algorithms： 强化机器学习。 通过产生动作并发现错误或奖励来与环境互动。反复试验和延迟奖励是强化学习的特征。强化学习允许机器和软件代理自动确定特定上下文的理想行为，以使其性能最大化。代理需要反馈，以了解那种行动最好，这就是增强信号。 Supervised machine learning algorithms: Analytical learning - 分析学习 Artificial neural network - 人工神经网络 Backpropagation - 反向传播 Boosting (meta-algorithm) - 元算法 Bayesian statistics - 贝叶斯统计 Case-based reasoning - 基于案例的推理 Decision tree learning - 决策树学习 Inductive logic programming - 归纳逻辑编程 Gaussian process regression - 高斯过程回归 Genetic Programming - 遗传算法 Group method of data handling - 数据处理的分组方法 Kernel estimators - 核估计器 Learning Automata - 学习自动状态机 Learning Classifier Systems - 学习分类器系统 Minimum message length (decision trees, decision graphs, etc.) - 最小消息长度（决策树，决策图） Multilinear subspace learning - 多线性子空间学习 Naive Bayes classifier - 朴素贝叶斯分类 Maximum entropy classifier - 最大熵分类器 Conditional random field - 随机条件场 Nearest Neighbor Algorithm - 最大临近算法 Probably approximately correct learning (PAC) learning - 大概近似学习（PCA） Ripple down rules, a knowledge acquisition methodology - 波动降低规则，一种知识获取方法 Symbolic machine learning algorithms - 符号机器学习算法 Subsymbolic machine learning algorithms - 亚符号机器学习算法 Support vector machines - 支持向量机 Minimum Complexity Machines (MCM) - 最低复杂度机器 Random Forests - 随机森林 Ensembles of Classifiers - 分类器集合 Ordinal classification - 顺序分类 Data Pre-processing - 数据预处理 Handling imbalanced datasets - 处理不平衡的数据集 Statistical relational learning - 统计关系学习 Proaftn, a multicriteria classification algorithm - Proaftn 多准则分类算法 Machine learning method Supervised machine learning algorithms： 用于有标签的，已分类的数据。用已知的知识去预测分类新的知识。 unsupervised machine learning algorithms ： 用于训练的数据既没有分类也没有标记。无监督学习研究如何从未标记的数据中推断出隐藏结构。无监督学习无法找出正确的输出，但可以浏览数据并得到推论，以描述未标记数据中的结构。 Semi-supervised machine learning algorithms ：半监督学习算法。介于有监督和无监督学习之间，同时使用标记和未标记数据进行训练。通常使用少量有标签数据和大量无标签数据进行训练，这种方法可以大大提高准确性。通常，当需要标记的数据需要相关和熟练的资源以供其训练时，选择半监督学习。半监督学习方法的性能依赖于所用的半监督假设。 Reinforcement machine learning algorithms： 强化机器学习。 通过产生动作并发现错误或奖励来与环境互动。反复试验和延迟奖励是强化学习的特征。强化学习允许机器和软件代理自动确定特定上下文的理想行为，以使其性能最大化。代理需要反馈，以了解那种行动最好，这就是增强信号。 Supervised machine learning algorithms: Analytical learning - 分析学习 Artificial neural network - 人工神经网络 Backpropagation - 反向传播 Boosting (meta-algorithm) - 元算法 Bayesian statistics - 贝叶斯统计 Case-based reasoning - 基于案例的推理 Decision tree learning - 决策树学习 Inductive logic programming - 归纳逻辑编程 Gaussian process regression - 高斯过程回归 Genetic Programming - 遗传算法 Group method of data handling - 数据处理的分组方法 Kernel estimators - 核估计器 Learning Automata - 学习自动状态机 Learning Classifier Systems - 学习分类器系统 Minimum message length (decision trees, decision graphs, etc.) - 最小消息长度（决策树，决策图） Multilinear subspace learning - 多线性子空间学习 Naive Bayes classifier - 朴素贝叶斯分类 Maximum entropy classifier - 最大熵分类器 Conditional random field - 随机条件场 Nearest Neighbor Algorithm - 最大临近算法 Probably approximately correct learning (PAC) learning - 大概近似学习（PCA） Ripple down rules, a knowledge acquisition methodology - 波动降低规则，一种知识获取方法 Symbolic machine learning algorithms - 符号机器学习算法 Subsymbolic machine learning algorithms - 亚符号机器学习算法 Support vector machines - 支持向量机 Minimum Complexity Machines (MCM) - 最低复杂度机器 Random Forests - 随机森林 Ensembles of Classifiers - 分类器集合 Ordinal classification - 顺序分类 Data Pre-processing - 数据预处理 Handling imbalanced datasets - 处理不平衡的数据集 Statistical relational learning - 统计关系学习 Proaftn, a multicriteria classification algorithm - Proaftn 多准则分类算法 Unsupervised learning Unspervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs. 无监督学习是在没有打标的数据中寻找没有检测到的模式。无监督学习允许对输入进行概率密度建模。 无监督学习中两种主要的方式是 主成分分析（PCA） 和 聚类分析。 被称为无监督学习策略的唯一要求是通过最大化某些目标函数或最小化某些损失函数来学习一个捕获原始空间特征的新特征空间。因此，生成协方差矩阵并不是学习的非监督方法，而是采用协方差矩阵的特征向量，因为特征分解线性代数运算能最大程度地提高方差。同样，进行数据集的对数变换也不是无监督学习，而是将输入数据通过多个S形函数传递，同时最小化生成的数据与结果数据之间的距离函数，这被称为自动编码器。 Unspevised leaning algorithms: Some of the most common algorithms used in unsupervised learning include: Clustering - 聚类 Anomaly detection - 异常检测 Neural Networks - 神经网络 Approaches for learning latent variable models. - 学习潜在变量的模型方法 Each approach uses several methods as follows: Clustering: hierarchical clustering - 分级聚类 k-means mixture models - 混合模型 DBSCAN - 基于密度的带噪声应用程序空间聚类 OPTICS algorithm - OPTICS算法 Anomaly detection Local Outlier Factor - 局部离群因子 Isolation Forest - 隔离森林 Neural Networks Autoencoders - 自动编码器 Deep Belief Nets - 深信网 Hebbian Learning - 赫布理论 Generative adversarial networks - 生成对抗网络 Self-organizing map - 自组织映射 ANN——人工神经网络 CNN——卷积神经网络 RNN——循环神经网络 Long Short-Term Memory - LSTM 时间递归神经网络 Approaches for learning latent variable models such as Expectation–maximization algorithm (EM) - 期望最大化算法 Method of moments - 矩量法 Blind signal separation techniques - 盲信号分离 Principal component analysis - PCA主成分分析 Independent component analysis - 独立成分分析 Non-negative matrix factorization - 非负矩阵分解 Singular value decomposition - 奇异值分解 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/Machine_Learning/Supervised.html":{"url":"Artificial_Intelligence/Machine_Learning/Supervised.html","title":"Supervised machine learning algorithms","keywords":"","body":"本页目录 有监督学习 线性回归和逻辑回归比较 回归算法 Linear regression - 线性回归 线性回归参数求解方法一：解析法 线性回归参数求解方法二：梯度下降 分类算法 - 预测的变量是离散的 Logistic regression - 逻辑回归 逻辑回归 - 简介 逻辑回归的 cost function 线性回归参数求解方法二：梯度下降 逻辑回归参数求解 （最小化 cost function, 或 最大化 极大似然函数) 方法一：梯度下降 梯度下井推导过程 梯度下降的可行性 解释 方法二：牛顿法 牛顿法推导 求解正则化 L1正则化 L2正则化 并行化 求解 梯度下降或牛顿法 计算步骤 Softmax regression （SMR） - 可用于多分类 SMR 一般步骤 Softmax定义 Softmax模型定义 Softmax cost function - 定义为 交叉熵 Softmax Regression 多分类例子 Softmax Regression 与 logistic regression 的联系 Softmax Regression 过拟合问题 - 处理方法为减少特征数量或正则化 朴素贝叶斯 朴素贝叶斯算法过程 朴素贝叶斯详解 朴素贝叶斯例子 朴素贝叶斯优缺点 回归分类都可以的算法 决策树 决策树关键问题 决策树学习 决策树与概率分布 决策树的构造算法 决策树的特征选择方法 ID3 算法 ID3 算法的不足 C4.5 算法 C4.5 算法改进 C4.5 算法的不足 CART树 CART 分类树树 特征选择方法 CART分类树对于连续特征和离散特征的处理 CART分类树算法 CART回归树算法 CART树算法的剪枝 CART回归树算法 CART中 类别不平衡 问题 CART树缺点 多变量决策树-斜决策树(OC1算法)) 决策树总结 决策树例子 SVM SVM 例子(KCL-course work) SVM详解 k-Nearest Neighbors (KNN) KNN算法解析 KNN算法实现 KNN算法拓展 KNN算法优缺点 有监督学习 回归算法 - 预测的变量是连续的 linear regression - 线性回归 分类算法 - 预测的变量是离散的 logistic regression - 逻辑回归 - （统计方法） Softmax regression - 可用于多分类 Decision Tree - 决策树 SVM - 支持向量 - （几何方法） 线性回归和逻辑回归比较 线性回归只能用于回归问题，逻辑回归虽然名字叫回归，但是更多用于分类问题 线性回归要求因变量是连续性数值变量，而逻辑回归要求因变量是离散的变量 线性回归要求自变量和因变量呈线性关系，而逻辑回归不要求自变量和因变量呈线性关系 线性回归可以直观的表达自变量和因变量之间的关系，逻辑回归则无法表达变量之间的关系 Linear regression - 线性回归 给定数据集D={(x1, y1), (x2, y2), ...}，我们试图从此数据集中学习得到一个线性模型，这个模型尽可能准确地反应x(i)和y(i)的对应关系。这里的线性模型，就是属性(x)的线性组合的函数，可表示为： 通俗的理解：x(i)就是一个个属性（例如西瓜书中的色泽，根蒂；Andrew ng示例中的房屋面积，卧室数量等），theta(或者w/b)，就是对应属性的参数（或者权重），我们根据已有数据集来求得属性的参数（相当于求得函数的参数），然后根据模型来对于新的输入或者旧的输入来进行预测（或者评估）。 从下图来直观理解一下线性回归优化的目标——图中线段距离（平方）的平均值，也就是最小化到分割面的距离和。 也就是说我们尽量使得f(xi)接近于yi，那么问题来了，我们如何衡量二者的差别？常用的方法是均方误差，也就是（均方误差的几何意义就是欧氏距离）最小二乘法。最小二乘法即为线性回归的损失函数 （loss function）。 这里，将上式中的 b 也向量化，吸纳到 w 中。 此时，线性回归的目标就是求得 这里，argmin是指求得最小值时的w,b的取值 这里可以采用两种方式完成求解，当维度不高时可以采用解析法。当处理高纬度数据时可以使用梯度下降求解。 解析法 E是关于 (w,b)的凸函数，只有一个最小值。此时，所求即为E为最小是的 (w,b) 取值。 对于凸函数E关于w,b导数都为零时，就得到了最优解。 若是一维数据，可分别对w,b求导，令等式为0，即可计算。 若是多维数据，则使用矩阵。 此时，解析解为： 以下是多元线性回归 最小二乘法 推导过程 梯度下降 由上一步，根据最小二乘法得到 cost function (损失函数)，这里使用梯度下降方法求解损失函数最小值时的参数值。（最优化问题） 梯度下降有不同的方法，分别为 batch gradient descent - 批梯度下降: 权重的更新通过计算全部训练集的数据 Stochastic Gradient Descent, （SGD） - 随机梯度下降: 当数据量特别大时，在计算最快下降方向时，随机选择一个数据进行计算，而不是扫描所有的训练数据，这样就加快了迭代速度。 随机梯度下降并不是沿着 cost function 下降最快的方向进行，是以震荡的方式趋向极小点。 批量梯度下降 随机梯度下降 Logistic regression - 逻辑回归 Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。 逻辑回归的思路是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率 它输出一个 0 到 1 之间的离散二值结果。简单来说，它的结果不是 1 就是 0。 Logistic 回归通过使用其固有的 logistic 函数估计概率，来衡量因变量（我们想要预测的标签）与一个或多个自变量（特征）之间的关系。 逻辑回归优点 直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题； 不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用； 对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现； 缺点 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换； 简介 逻辑回归的 cost function 逻辑回归参数求解 （最小化 cost function, 或 最大化 极大似然函数） 方法一：梯度下降 梯度下井推导过程 梯度下降的可行性 解释 方法二：牛顿法 牛顿法推导 求解正则化 正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。 L1正则化 拉普拉斯分布 L2正则化 正态分布 并行化求解梯度下降或牛顿法 逻辑回归的并行化最主要的就是对目标函数梯度计算的并行化。 并行 LR 实际上就是在求解损失函数最优解的过程中，针对寻找损失函数下降方向中的梯度方向计算作了并行化处理，而在利用梯度确定下降方向的过程中也可以采用并行化。 我们看到目标函数的梯度向量计算中只需要进行向量间的点乘和相加，可以很容易将每个迭代过程拆分成相互独立的计算步骤，由不同的节点进行独立计算，然后归并计算结果。 样本矩阵按行划分，将样本特征向量分布到不同的计算节点，由各计算节点完成自己所负责样本的点乘与求和计算，然后将计算结果进行归并，则实现了按行并行的 LR。按行并行的 LR 解决了样本数量的问题，但是实际情况中会存在针对高维特征向量进行逻辑回归的场景，仅仅按行进行并行处理，无法满足这类场景的需求，因此还需要按列将高维的特征向量拆分成若干小的向量进行求解。 计算步骤 并行计算总共会被分为两个并行化计算步骤和两个结果归并步骤 Softmax regression （SMR） - 可用于多分类 使用对数线性模型 softmax 回归是逻辑回归的一般形式,当类别数为 2 时，softmax 回归退化为逻辑回归 softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！ 在神经网络中的最后一层隐含层和输出层就可以看成是logistic回归或softmax回归模型，之前的层只是从原始输入数据从学习特征，然后把学习得到的特征交给logistic回归或softmax回归处理。 Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。 一般步骤 Step 1: add up the evidence of our input being in certain classes; Step 2: convert that evidence into probabilities. Softmax定义 Softmax模型定义 假设有 K 个分类 Softmax cost function - 定义为 交叉熵 得到costfunction后的步骤可用梯度下降完成, 根据学习率，进行参数求解 Softmax Regression 多分类例子 原文地址https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html Softmax Regression 与 logistic regression 的联系 Softmax Regression 过拟合问题 - 处理方法为 减少特征数量 或 正则化 朴素贝叶斯分类 朴素贝叶斯算法过程 朴素贝叶斯详解 朴素贝叶斯例子 朴素贝叶斯优缺点 决策树 基本思想：采用自顶向下的递归方法，以信息熵（或其他度量条件）为度量来构造一课熵值下降最快的数，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一个类。 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。 决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据集进行分割，使得对各个子数据集有一个最好的分类过程，这一过程对应着对特征空间的划分，也对应着决策树的构建。 决策树是在已知各种情况发生概率((各个样本数据出现中，不同特征出现的概率))的基础上,通过构建决策树来进行分析的一种方式。常用算法有ID3、C4.5、CART。 决策树的主要优势就在于数据形式非常容易理解。 决策树关键问题 决策树学习 决策树与概率分布 决策树的构造算法： 分类解决离散问题，回归解决连续问题。 根结点包含了所有的训练样本。 内部结点包含对应的属性测试，每个内部结点包含的样本集合，依据属性测试的结果，被划分到它的子结点。 叶子结点对应于决策结界 从根结点到每个叶子结点的路径对应了一条决策规则。 基本算法遵循自顶向下、分而治之的策略 选择最好的属性作为测试属性并创建树的根结点 为测试属性每个可能的取值产生一个分支 训练样本划分到适当的分支形成子结点 对每个子节点重复上面的过程，直到所有结点都是叶子结点。 决策树构造过程 step1: 数据预处理,缺失值处理 Step2：选取一个属性作为决策树的根结点，然后就这个属性所有的取值创建树的分支.（划分特征） Step3：用这棵树来对训练数据集进行分类：（构建决策树） 如果一个叶结点的所有实例都属于同一类，则以该类为标记标识此叶 结点. 如果所有的叶结点都有类标记，则算法 终止. Step4：否则，选取 一个从该结点到根路径中没有出现过的属性作为标记标识该结点，然后就这个属性的所有取值继续创建树的分支；重复算法 步骤3. step5: 决策分类。在构建好决策树之后，最终就可以使用未知样本进行预测分类。 决策树的特征选择方法 特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。 具体选取的时候会用到两个准则：信息增益或信息增益比。 ID3 算法 ID3 算法的不足 C4.5 算法 C4.5 算法改进 C4.5 算法的不足 CART树 CART(Classification and Regression Tree)树既可以做分类，也可以做回归。 CART 分类树树 特征选择方法 CART分类树对于连续特征和离散特征的处理 CART分类树算法 CART回归树算法 CART树算法的剪枝 CART中 类别不平衡 问题 CART树缺点 多变量决策树_斜决策树(OC1算法) 决策树总结 决策树例子 例1：https://www.cnblogs.com/bonheur/p/12469858.html https://blog.csdn.net/jeryjeryjery/article/details/78884324 例2： SVM(support vector machine) 支持向量机 SVM 例子(KCL-course work) SVM详解 KNN(k-nearest neighbors) KNN算法解析 KNN算法实现 KNN算法拓展 KNN算法优缺点 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-25 17:31:17 "},"Artificial_Intelligence/Machine_Learning/Unsupervised.html":{"url":"Artificial_Intelligence/Machine_Learning/Unsupervised.html","title":"Unsupervised machine learning algorithms","keywords":"","body":"本页目录 无监督学习 [EM（Expectation-Maximum）算法也称期望最大化算法] 受限波尔兹曼机 聚类算法 K均值聚类K-means 层次聚类 降维算法 主成分分析 – PCA 奇异值分解 – SVD 自编码器（Auto-encoder） GAN-深度学习 K均值聚类K-means Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-12-10 01:49:37 "},"Artificial_Intelligence/Machine_Learning/Boosting.html":{"url":"Artificial_Intelligence/Machine_Learning/Boosting.html","title":"模型优化-集成学习(模型融合)","keywords":"","body":"本页目录 集成学习 Boosting 方法 - 模型融合 Boosting 简介 Forward Stagewise Additive Modeling - 前向分步加法模型 四大 Boosting 算法 Adaboost (Adaptive Boosting) 算法 adaboost 算法流程 adaboost 误差界分析 adaboost 案例分析 GBDT(Gradient Boosting Decision Tree) 梯度提升 GBDT特征选择 GBDT回归树 GBDT分类树 GBDT 例题 XGBoost) GBDT特征选择 XGBoost 与 GBDT 区别于联系 XGBoost原理 XGBoost推导 XGBoost 例题 XGBoost 高频题 Bagging方法 集成学习 集成学习会挑选一些简单的基础模型进行组装，组装这些基础模型的思路主要有 2 种方法： bagging（bootstrap aggregating的缩写，也称作“套袋法”） 并行式集成学习方法最著名的代表 Bagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。 大部分情况下，经过 bagging 得到的结果方差（variance）更小。 在 bagging 的方法中，最广为熟知的就是随机森林了：bagging + 决策树 = 随机森林 具体过程 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的） 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） boosting Boosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。 大部分情况下，经过 boosting 得到的结果偏差（bias）更小 在 boosting 的方法中，比较主流的有 Adaboost 和 Gradient boosting 。 具体过程 通过加法模型将基础模型进行线性的组合。 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。 Boosting 和 bagging 的异同 样本选择上： Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 样例权重： Bagging：使用均匀取样，每个样例的权重相等 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 预测函数： Bagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 并行计算： Bagging：各个预测函数可以并行生成 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 Boosting 方法 - 模型融合 Boosting 简介 对于一个复杂任务来说，将多个专家的判断进行适当（按照一定权重）的综合（例如线性组合加法模型）所得出的判断，要比其中任何一个专家单独的判断好 在概率近似正确（probably approximately correct，PAC）学习框架中： 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的； 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。 强可学习和弱可学习是等价的。 也就是说，在PAC学习的框架下，一个概念是强可学习的 充分必要条件 是这个概念是弱可学习的。 这样一来，问题就转化为了，在算法训练建模中，如果已经发现了“弱可学习算法”（即当前分类效果并不优秀，甚至仅仅比随机预测效果要好），就有可能将其boosting（提升）为强可学习算法，这其中最具代表性的方法就是AdaBoosting（AdaBoosting algorithm） Boosting 要做的事情就是把弱可学习方法变成强可学习方法，提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权重分布） Boosting 思路： 对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。 常见的模型组合方法有： 简单平均（Averaging） 投票（voting） Bagging（randomforest） boosting（GBDT） stacking blending 等 Boosting设计的问题： 在每一轮学习之前，如何改变训练数据的权值分布？ (Adaboost算法的做法是：提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。) 如何将一组弱分类器组合成一个强分类器？ (AdaBoost采取加权多数表决的方法。具体地：加大分类误差率小 的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。) Forward Stagewise Additive Modeling - 前向分步加法模型 四大 Boosting 算法 L2Boosting全称：Least Squares Boosting；该算法由Buhlmann和Yu在2003年提出。 Adaboost (Adaptive Boosting) 算法 由Yoav Freund和Robert Schapire在1995年提出 它的自适应在于： 前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 算法步骤 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 adaboost 算法流程 adaboost 误差界分析 adaboost 案例分析 GBDT(Gradient Boosting Decision Tree) 梯度提升树 GBDT主要由三个概念组成： Regression Decistion Tree（即DT)， Gradient Boosting（即GB)， Shrinkage (算法的一个重要演进分枝，目前大部分源码都按该版本实现） 理解它如何用于搜索排序则需要额外理解RankNet概念，之后便功德圆满。 GBDT通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。 弱分类器一般会选择为CART TREE（也就是分类回归树）。 每一轮预测和实际值有残差，下一轮根据残差再进行预测，最后将所有预测相加，就是结果。 GBDT中的树都是回归树，不是分类树，这点对理解GBDT相当重要（尽管GBDT调整后也可用于分类但不代表GBDT的树是分类树）。 GBDT特征选择 GBDT回归树 GBDT分类树 多分类例题及代码实现 GBDT 例题 例题1 李航 统计学习例题： XGBoost(eXtreme Gradient Boosting) XGBoost 与 GBDT 区别于联系 XGBoost原理 (https://blog.csdn.net/qq_22238533/article/details/79477547) XGBoost推导 (https://zhuanlan.zhihu.com/p/92837676) XGBoost 例题 (https://www.jianshu.com/p/ac1c12f3fba1) XGBoost 高频题 Bagging方法 Bagging是并行式集成学习方法最著名的代表。 它基于自助采样法(bootstrap sampling)。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回到初始数据集，使得下次采样时该样本仍然由可能被采到。这样，经过m次随机采样操作，我们可以得到含有m个样本的采样集。初始训练集中有的样本被多次采到，有的则一次也没有被采样到，初始样本集中大约会有63.2%的样本出现在采样集中。 通过上述的自助采样法可以得到T个集合，每个集合包含m个样本。然后，基于每个集合训练出一个基学习器，再将这些基学习器进行集合。 在对各个基学习器的预测结果进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选一个，也可以进一步考察投票的置信度来确定最终胜者。 从方差—偏差分解的角度看，Bagging主要关注降低方差，（高方差对应的是过拟合问题），因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更为明显。 Random Forest(RF) 随机森林 随机森林简介 随机森林（Random Forest，RF）是bagging的一个扩展变体。RF在 以决策树为基学习器构建Bagging集成 的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时，是在当前节点的属性集合（假定由d个集合）中选择一个最优属性；而在RF中，对基决策树的每个节点，先从该节点属性中随机选择一个包含k个属性的属性子集，然后从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度，若k=d，则基决策树的构建与传统的决策树相同；若k=1，则是随机选择一个属性用于划分；一般情况下，推荐k=log2d。 这种随机选择属性也会使得RF的训练效率比bagging更高。 随机森林简单，容易实现，计算开销小，令人惊奇的是它在很多现实任务中展现出了很强大的性能，被誉为“代表集成学习技术水平方法”。 随机森林法仅仅对bagging做了小改动，但是，与bagging中基学习器的“多样性”仅仅通过样本扰动（通过对初始训练集bootstrap采样）而来不同，RF中基学习器的多样性不仅仅来自样本扰动，而且还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提高。 随机森林特点 在当前所有算法中，具有极好的准确率/It is unexcelled in accuracy among current algorithms； 能够有效地运行在大数据集上/It runs efficiently on large data bases； 能够处理具有高维特征的输入样本，而且不需要降维/It can handle thousands of input variables without variable deletion； 能够评估各个特征在分类问题上的重要性/It gives estimates of what variables are important in the classification； 在生成过程中，能够获取到内部生成误差的一种无偏估计/It generates an internal unbiased estimate of the generalization error as the forest building progresses； 对于缺省值问题也能够获得很好得结果/It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing 它就相当于机器学习领域的Leatherman（多面手），你几乎可以把任何东西扔进去，它基本上都是可供使用的。在估计推断映射方面特别好用，以致都不需要像SVM那样做很多参数的调试。 随机森林算法流程(bagging + 决策树 = 随机森林) Bagging Bagging是并行式集成学习方法最著名的代表。它直接基于自助采样法(bootstrap sampling) 从原始样本集中有放回随机抽取n个训练样本，独立进行k轮抽取，得到k个训练集 独立训练k个模型（基学习器可以是：决策树，ANN等） 分类问题：投票产生分类结果； 回归问题：取k个模型预测结果的均值 预测函数可以并行生成 随机森林算法优缺点 随机森林算法例子 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-12-09 21:39:41 "},"Artificial_Intelligence/Machine_Learning/Optimization.html":{"url":"Artificial_Intelligence/Machine_Learning/Optimization.html","title":"模型优化-Optimization","keywords":"","body":" 贝叶斯优化(Bayesian Optimization) https://www.cnblogs.com/marsggbo/p/9866764.html Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-28 23:57:43 "},"Artificial_Intelligence/Deep_Learning/Deep_learning.html":{"url":"Artificial_Intelligence/Deep_Learning/Deep_learning.html","title":"Deep learning","keywords":"","body":"Deep Learning 深度学习 - 神经网络 Specific artificial neural networks 学习方式 LDF (Linear Discriminant Functions) 线性二分方程 Perceptron Learning Minimum Squared Error Learning (Widrow-Hoff) - 最小二乘错误学习 Delta Learning Rule （supervised） Hebbian learning rule （unsupervised） 必备网络 神经网络发展分类 Specific artificial neural networks generic artificial neural network - 遗传神经网络 Linear Threshold Unit (or Perceptron) - 感知机 又称 基于LTN的人工神经元 Competitive Learning Networks - 竞争学习神经网络 Inhibitory Feedback Networks - 抑制反馈网络 Autoencoder Networks - 自编码网络 Multilayer Perceptrons - 多层感知机 Radial Basis Function Networks - 径向基函数神经网络 Convolutional Neural Networks - 卷积神经网络 Restricted Boltzmann Machines - 受限玻尔兹曼机 Hopfield Networks - 一种单层反馈神经网络 Kohonen Networks - 自组织特征映射 SOM(Self-organizing feature Map） Capsule Networks - 胶囊网络 学习方式 LDF (Linear Discriminant Functions) 线性二分方程： Perceptron Learning 感知机学习的数据是线性可分的，若给定一组含有正负样本的数据，若可以找到一个超平面，将其完美的分为两类，即一侧为正样本，一侧为负样本，那么这组数据即为线性可分的。 学习策略 首先假设数据是线性可分的，那么我们就需要找到一个超平面(一组 w,b )将数据分开。 定义损失函数，则希望这个损失函数越小越好，即完美分割的平面损失函数值最小。 误分类的点离这个超平面越近，则说明误分类的程度越小，故可将损失函数定义为误分类点到超平面的距离和 然而此时我们只在这只值的大小，不考虑分母未知 w 的L2范数也是可以的，显然， L(w,b) 为非负函数。即定义为 算法形式 明确优化目标 感知机学习算法是误分类驱动的，可以采用梯度下降算法。假定误分类点集合 M 是固定的，损失函数的 L(w,b) 的梯度由下面两个式子确定。 对于误分类点，进行一下迭代更新 其中 η 为步长，也被称为学习率，这样迭代可以使得损失函数 L(w,b) 不但减小，直至为0为止。 使用梯度下降更新模型，也只是对那些误分类的样本更新，对已经正确分类的样本则不用更新。 梯度下降的种类 （具体算法在启发式算法中） 批量梯度下降（Batch Gradient Descent） 批量梯度下降中，单次迭代使用全部的训练集样本来更新模型。当训练集非常大的时候，会十分耗时，他的优点也非常明显，收敛快，迭代次数少。 随机梯度下降（Stochastic gradient descent） 为了克服BGD带来的训练速度慢的问题，提出了SGD方法。随机梯度下降中，单次迭代随机选择训练集中的一个样本来更新模型，同时也带来了问题，由于有噪音的问题，单次迭代可能不会朝着优化的方向进行，但是可以说总体是朝着更优的方向移动的。 小批量梯度下降（Mini-Batch Gradient Descent) 小批量梯度下降是为了折中上面的两种算法，即每次既不用一个，也不用全部训练集来更新模型，而是选择一部分来更新模型。这样既可以保证模型的训练速度，又可以保证参数的准确性。 口袋算法（Pocket Algorithm） 感知机模型是针对可以线性可分的数据进行的，若不可线性可分，则会算法会一直运行下去。为了解决这个问题，STEPHEN I. GALLANT在论文《Perceptron-Based Learning Algorithms》中提出了Pocket Algorithm。 Pocket Algorithm的基本思想为，在感知机求解超平面的过程中，保存当前的最优解，即使得测试集误分类样本最少的 [公式] 的值，当超过迭代次数后，算法输出保存的最优解。为了求解出这个解，则必须在单次迭代后，遍历一遍训练集，找出对于当前的模型有多少误分类的样本数，增大了开销。 相比之下，感知机自动收敛到最优结果（当然要求线性可分，否则停不下来），使用口袋算法的感知机则设定最大迭代次数，输出迭代过程中的最优结果。 Minimum Squared Error Learning (Widrow-Hoff) - 最小二乘错误学习 MSE是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。 Delta Learning Rule （supervised） Sequential Delta Learning Algorithm 例子 Hebbian learning rule （unsupervised） 必备网络 感知机 卷积神经网络 循环神经网络 长短期记忆网络 玻尔兹曼机网络 深度信念网络 深度自动编码器 BP神经网络 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-12-15 15:48:04 "},"Artificial_Intelligence/Deep_Learning/Specific.html":{"url":"Artificial_Intelligence/Deep_Learning/Specific.html","title":"Specific artificial neural networks","keywords":"","body":"Deep Learning 深度学习 - 神经网络 Basic knowledge Specific artificial neural networks Linear Threshold Unit (or Perceptron) - 感知机 又称 基于LTN的人工神经元 Multilayer Perceptrons - 多层感知机 Back Propagation Neural Network - 反向传播神经网络 激活函数 bp算法 反向传播神经网络计算过程 反向传播总结 Convolutional Neural Networks - 卷积神经网络 基本结构 CNN训练方法 CNN的卷积层 CNN的池化层 - 数据降维,避免过拟合 CNN全连接网络 CNN 公式计算 CNN 例子 CNN 框架发展 AlexNet Restricted Boltzmann Machines - 受限玻尔兹曼机 RBM 计算 RBM 数学原理 RBM 例子 Deep Belief Network 深度置信网络 Autoencoder Networks - 自编码网络 稀疏自编码器推导 generic artificial neural network - 遗传神经网络 Recurrent Neural Network 循环神经网络 循环神经网络推导求解 推导理解 基于时间的反向传播理解 tanh梯度消失的问题 Long short-term memory(LSTM) 长短时记忆网络 推导 Recursive Neural Network 递归神经网络 Graph Neural Networks(GNN) 图神经网络 Competitive Learning Networks - 竞争学习神经网络 Attention机制 Attention Mechanism 原理 Attention Mechanism 模块图解 Attention 分类 Soft Attention 和 Hard Attention Soft Attention 和 Hard Attention Global Attention 和 Local Attention Self Attention 及计算 Multi-Head Attention 及计算 Attention 的其他组合应用 Hierarchical Attention Attention over Attention Multi-step Attention Multi-dimension Attention Memory-based Attention Attention 在图像领域的应用 学习权重分部 精细分类 图像分类 图像分割 看图说话 任务聚焦/解耦 图像分割 Attention 计算过程 待补充 Inhibitory Feedback Networks - 抑制反馈网络 Radial Basis Function Networks - 径向基函数神经网络 Hopfield Networks - 一种单层反馈神经网络 Kohonen Networks - 自组织特征映射 SOM(Self-organizing feature Map） Capsule Networks - 胶囊网络 Basic knowledge Layers can be: visible – receive inputs from, or send outputs to, the external environment hidden – only receive inputs and send output to other processing units input layers: receive signals from the environment – for classification, this is the feature vector which is to be classified output layers: which send signals to the environment – for classification, this is the predicted class label associated with the feature vector Weights - Connection weights can be defined by Setting weights explicitly using prior knowledge. Optimising connectivity to achieve some objective (e.g. using a genetic algorithm). Training the network by feeding it training data and allowing it to adapt the connection weights. supervised - Delta Learning Rule unsupervised - Hebbian learning rule Specific artificial neural networks Linear Threshold Unit (or Perceptron) - 感知机 又称 基于LTN的人工神经元 Each unit receives a vector of inputs, x (from other units or the external environment). Each input is associated with a weight, w, which determines the strength of influence of each input. Each w can be either +ve (excitatory) or -ve (inhibitory). (restricted case, where weights and activations are binary, known as Threshold Logic Unit, or McCulloch-Pitts neuron) This response function often split into two component parts: A transfer function that determines how the inputs are integrated. An activation function that determines the output the neuron produces. w and θ definea hyperplane that divides the input space into two parts This hyperplane is called the “decision boundary.” Multilayer Perceptrons - 多层感知机 Back Propagation Neural Network - 反向传播神经网络 激活函数 bp算法 反向传播神经网络计算过程 反向传播总结 Convolutional Neural Networks - 卷积神经网络 https://cs231n.github.io/convolutional-networks/ 基本结构 CNN训练方法 CNN的卷积层 CNN的池化层 - 数据降维,避免过拟合 池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。 CNN全连接网络 CNN 公式计算 训练方式为反向传播 CNN 例子 CNN 框架发展 AlexNet Restricted Boltzmann Machines - 受限玻尔兹曼机 https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf RBM 计算 RBM 数学原理 RBM 例子 Deep Belief Network 深度置信网络 https://s-top.github.io/blog/2018-01-19-dbn Autoencoder Networks - 自编码网络 https://zhuanlan.zhihu.com/p/54032437 稀疏自编码器推导 generic artificial neural network - 遗传神经网络 An environment in which the system operates (inputs to the network, outputs from the network). A set of processing units (‘neurons’, ‘cells’, ‘nodes’). A set of weighted connections between units, wji, which determines the influence of unit i on unit j. A transfer function that determines how the inputs to a unit are integrated. An activation function that determines the output the neuron produces. An activation state, yj, for every unit (‘response’, ‘output’). A method for setting/changing the connection weights. Recurrent Neural Network 循环神经网络 https://zhuanlan.zhihu.com/p/32755043 循环神经网络推导求解 https://www.cnblogs.com/YiXiaoZhou/p/6058890.html 推导理解 基于时间的反向传播理解 tanh梯度消失的问题 Long short-term memory(LSTM) 长短时记忆网络 循环神经网络(RNN) 的优化算法 推导 ![LSTM 32(img/lstm_32.png) ![LSTM 33(img/lstm_33.png) Recursive Neural Network 递归神经网络 Graph Neural Networks(GNN) 图神经网络 清华大学GNN paper list: https://github.com/thunlp/GNNPapers/blob/master/README.md#survey-papers Competitive Learning Networks - 竞争学习神经网络 所谓竞争学习神经网络，是在输出层对多个节点返回的数据做选择。选择方法可以是投票可以是算平均值等。 Attention机制 Capsule Networks - 胶囊网络 https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/ 各层参数计算 胶囊网络-动态路由算法 矩阵胶囊： https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/ 胶囊网络实现 https://blog.csdn.net/bhneo/article/details/79391469 Inhibitory Feedback Networks - 抑制反馈网络 Radial Basis Function Networks - 径向基函数神经网络 Hopfield Networks - 一种单层反馈神经网络 Kohonen Networks - 自组织特征映射 SOM(Self-organizing feature Map） Capsule Networks - 胶囊网络 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-15 20:23:27 "},"Artificial_Intelligence/Deep_Learning/Optimization.html":{"url":"Artificial_Intelligence/Deep_Learning/Optimization.html","title":"Network Optimization","keywords":"","body":"深度学习 模型优化方法 CNN 模型优化 两个卷积层 激活函数 => 线性修正单元(ReLU) 正则化 => Batch Normalization 批标准正则化 拓展数据集 => 参考AlexNet 可实现数据百倍拓展 继续插入额外的全连接层 Dropout 方法 参数设置方法待补充 通过训练多个网络，采用投票的方式组合网络 提高准确率 冻结部分参数，训练另一部分参数 通过检查点保存模型，可以实现在检查点状态继续训练 激活函数 激活函数，并不是去激活什么，而是指如何把“激活的神经元的特征”通过函数把特征保留并映射出来（保留特征，去除一些数据中是的冗余），这是神经网络能解决非线性问题关键。 激活函数作用 激活函数的主要作用是提供网络的非线性建模能力。在卷积层中，我们主要采用了卷积的方式来处理，也就是对每个像素点赋予一个权值，这个操作显然就是线性的。但是对于我们样本来说，不一定是线性可分的，为了解决这个问题，我们可以进行线性变化，或者我们引入非线性因素，解决线性模型所不能解决的问题。如果没有激活函数，那么该网络仅能够表达线性映射，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此也可以认为，只有加入了激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。 激活函数还可以构建稀疏矩阵，也就是稀疏性，这个特性可以去除数据中的冗余，最大可能保留数据的特征，也就是大多数为0的稀疏矩阵来表示（这个特性主要是对于Relu） 待补充 冻结部分参数 加载预训练模型后，比如在resnet的基础上连接了新的模块，那么resnet部分将不需要参与训练。 具体代码待补充 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-04 17:55:36 "},"Artificial_Intelligence/Deep_Learning/Pytorch.html":{"url":"Artificial_Intelligence/Deep_Learning/Pytorch.html","title":"Deep leearning demo in Pytorch","keywords":"","body":"Pytorch tutorial CNN 常用包 import torch from torch import nn, optim import torch.nn.functional as F from torch.autograd import Variable from torch.utils.data import DataLoader from torchvision import transforms from torchvision import datasets from log1 import Logged 激活函数 常用：Rectified Linear Unit, ReLU 线性整流函数 又称修正线性单元 最优化参数 常用： Adam 梯度下降 CNN 定义 以下是有一个样例 import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader import torchvision.transforms as transforms import torchvision.datasets import numpy as np PART 1：加载数据，设置超参 超参设置 num_epochs = 5 num_classes = 10 batch_size = 100 learning_rate = 0.001 pytorch 将会下载MNIST数据保存到DATA_PATH 也会将训练好的模型保存到MODEL_STORE_PATH DATA_PATH = \"E:/DL/CNN/dataset\" MODEL_STORE_PATH = \"E:/DL/CNN/model\" transforms to apply to the data transforms.Compose函数来自于torchvision包 用Compose可以将各种transforms有序组合到一个list中 首先指定一个转换transforms.ToTensor()，将数据转换为pytorch tensor pytorch tensor是pytorch中特殊的数据类型，用于网络中数据和权重的操作，本质上是多维矩阵 接下来用transforms.Normalize对数据进行归一化，参数为数据的平均数和标准差 MNIST数据是单通道的，多通道就需要提供每个通道的均值和方差 trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) MNIST dataset，这里创建了train_dataset和test_dataset对象 root：train.pt和test.pt数据文件位置；train：指定获取train.pt或者test.pt数据 tranform：对创建的数据进行操作transform操作；download：从线上下载MNIST数据 train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True) test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans) pytorch中的DataLoader对象，可以对数据洗牌，批处理数据，多处理来并行加载数据 train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) PART 2：创建CNN类 神经网络的结构： 输入图片为 28*28 单通道 第一次卷积：32 channels of 5 x 5 convolutional filters，a ReLU activation followed by 2 x 2 max pooling(stride = 2，this gives a 14 x 14 output) 第二次卷积：64 channels of 5 x 5 convolutional filters 2 x 2 max pooling (stride = 2，produce a 7 x 7 output) 展开需要节点：7 x 7 x 64 = 3164 个，接上全连接层（含1000个节点） 最后对10个输出节点进行softmax操作，产生分类概率 class ConvNet(nn.Module): # 初始化定义网络的结构：也就是定义网络的层 def __init__(self): super(ConvNet,self).__init__() # Sequential方法使我们有序的创建网络层 # Conv2d nn.Module的方法，该方法创建一组卷积滤波器， # 第一个参数为输入的channel数，第二个参数为输出的channel数 # kernel_size：卷积滤波器的尺寸，这里卷积滤波器为5*5，所以参数设置为5 # 如果卷积滤波器为 x*y，参数就是一个元组(x,y) self.layer1 = nn.Sequential( # 卷积操作&池化操作的维度变化公式: width_of_output = (width_of_input - filter_size + 2*padding)/stride + 1 # 卷积操作时维度变化：28-5+2*2+1 =28，我希望卷积的输出和输出维度一样，所以加了2 padding nn.Conv2d(1,32,kernel_size=5,stride=1,padding=2), # 激活函数 nn.ReLU(), # kernel_size：pooling size，stride：down-sample nn.MaxPool2d(kernel_size=2,stride=2)) self.layer2 = nn.Sequential( nn.Conv2d(32,64,kernel_size=5,stride=1,padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2)) self.drop_out = nn.Dropout() # 后面两个全连接层，分别有1000个节点，10个节点对应10种类别 # 接全连接层的意义是：将神经网络输出的丰富信息加到标准分类器中 self.fc1 = nn.Linear(7*7*64,1000) self.fc2 = nn.Linear(1000,10) # 定义网络的前向传播,该函数会覆盖 nn.Module 里的forward函数 # 输入x,经过网络的层层结构，输出为out def forward(self,x): out = self.layer1(x) out = self.layer2(out) # flattens the data dimensions from 7 x 7 x 64 into 3164 x 1 # 左行右列，-1在哪边哪边固定只有一列 out = out.reshape(out.size(0),-1) # 以一定概率丢掉一些神经单元，防止过拟合 out = self.drop_out(out) out = self.fc1(out) out = self.fc2(out) return out PART 3：创建一个CNN实例 model = ConvNet() 该函数包含了 SoftMax activation 和 cross entorpy，所以在神经网络结构定义的时候不需要定义softmax activation criterion = nn.CrossEntropyLoss() 第一个参数:我们想要训练的参数。 在nn.Module类中，方法 nn.parameters()可以让pytorch追踪所有CNN中需要训练的模型参数，让他知道要优化的参数是哪些 optimizer = optim.Adam(model.parameters(),lr = learning_rate) PART 4：训练模型 训练数据集长度 total_step = len(train_loader) loss_list = [] acc_list = [] for epoch in range(num_epochs): # 遍历训练数据(images,label) for i,(images,labels) in enumerate(train_loader): # 向网络中输入images，得到output,在这一步的时候模型会自动调用model.forward(images)函数 outputs = model(images) # 计算这损失 loss = criterion(outputs,labels) loss_list.append(loss.item()) # 反向传播，Adam优化训练 # 先清空所有参数的梯度缓存，否则会在上面累加 optimizer.zero_grad() # 计算反向传播 loss.backward() # 更新梯度 optimizer.step() # 记录精度 total = labels.size(0) # torch.max(x,1) 按行取最大值 # output每一行的最大值存在_中，每一行最大值的索引存在predicted中 # output的每一行的每个元素的值表示是这一类的概率，取最大概率所对应的类作为分类结果 # 也就是找到最大概率的索引 _,predicted = torch.max(outputs.data,1) # .sum()计算出predicted和label相同的元素有多少个，返回的是一个张量，.item()得到这个张量的数值(int型) correct = (predicted == labels).sum().item() acc_list.append(correct/total) if (i+1) % 100 == 0: print('Epoch[{}/{}],Step[{},{}],Loss:{:.4f},Accuracy:{:.2f}%' .format(epoch+1,num_epochs,i+1,total_step,loss.item(),(correct/total)*100)) PART 5：测试模型 将模型设为评估模式，在模型中禁用dropout或者batch normalization层 model.eval() 在模型中禁用autograd功能，加快计算 with torch.no_grad(): correct = 0 total = 0 for images,labels in test_loader: outputs = model(images) _,predicted = torch.max(outputs.data,1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Test Accuracy of the model on the 1w test images:{} %'.format((correct / total) * 100)) save the model torch.save(model.state_dict(),MODEL_STORE_PATH + 'conv_net_model.ckpt') Demo 目标检测任务 ， 分类任务 和 目标跟踪 先来一个全状态的CNN，附带反向传播 再来一个图神经网络的CNN， Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-07 14:49:48 "},"Artificial_Intelligence/Deep_Learning/Object_Detection/Detection.html":{"url":"Artificial_Intelligence/Deep_Learning/Object_Detection/Detection.html","title":"目标检测","keywords":"","body":"目标检测 重要算法模型 必备模型 一个完整的目标检测网络主要由三部分构成：detector=backbone+neck+head 新的监测网络使用 Transformer ，如： Detection Transformer RetinaNet resent senet frp， rpn， nms， roi align ， sppnet， rcnn系列， yolov系列， ssd系列 Single Shot MultiBox Detector (SSD) https://zhuanlan.zhihu.com/p/33544892 https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/ObjectDetections/SSD self.ssd_params = SSDParams(img_shape=(300, 300), # 输入图片大小 num_classes=21, # 类别数+背景 no_annotation_label=21, feat_layers=[\"block4\", \"block7\", \"block8\", \"block9\", \"block10\", \"block11\"], # 要进行检测的特征图name feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)], # 特征图大小 anchor_size_bounds=[0.15, 0.90], # 特征图尺度范围 anchor_sizes=[(21., 45.), (45., 99.), (99., 153.), (153., 207.), (207., 261.), (261., 315.)], # 不同特征图的先验框尺度（第一个值是s_k，第2个值是s_k+1） anchor_ratios=[[2, .5], [2, .5, 3, 1. / 3], [2, .5, 3, 1. / 3], [2, .5, 3, 1. / 3], [2, .5], [2, .5]], # 特征图先验框所采用的长宽比（每个特征图都有2个正方形先验框） anchor_steps=[8, 16, 32, 64, 100, 300], # 特征图的单元大小 anchor_offset=0.5, # 偏移值，确定先验框中心 normalizations=[20, -1, -1, -1, -1, -1], # l2 norm prior_scaling=[0.1, 0.1, 0.2, 0.2] # variance ) def _built_net(self): \"\"\"Construct the SSD net\"\"\" self.end_points = {} # record the detection layers output self._images = tf.placeholder(tf.float32, shape=[None, self.ssd_params.img_shape[0], self.ssd_params.img_shape[1], 3]) with tf.variable_scope(\"ssd_300_vgg\"): # original vgg layers # block 1 net = conv2d(self._images, 64, 3, scope=\"conv1_1\") net = conv2d(net, 64, 3, scope=\"conv1_2\") self.end_points[\"block1\"] = net net = max_pool2d(net, 2, scope=\"pool1\") # block 2 net = conv2d(net, 128, 3, scope=\"conv2_1\") net = conv2d(net, 128, 3, scope=\"conv2_2\") self.end_points[\"block2\"] = net net = max_pool2d(net, 2, scope=\"pool2\") # block 3 net = conv2d(net, 256, 3, scope=\"conv3_1\") net = conv2d(net, 256, 3, scope=\"conv3_2\") net = conv2d(net, 256, 3, scope=\"conv3_3\") self.end_points[\"block3\"] = net net = max_pool2d(net, 2, scope=\"pool3\") # block 4 net = conv2d(net, 512, 3, scope=\"conv4_1\") net = conv2d(net, 512, 3, scope=\"conv4_2\") net = conv2d(net, 512, 3, scope=\"conv4_3\") self.end_points[\"block4\"] = net net = max_pool2d(net, 2, scope=\"pool4\") # block 5 net = conv2d(net, 512, 3, scope=\"conv5_1\") net = conv2d(net, 512, 3, scope=\"conv5_2\") net = conv2d(net, 512, 3, scope=\"conv5_3\") self.end_points[\"block5\"] = net print(net) net = max_pool2d(net, 3, stride=1, scope=\"pool5\") print(net) # additional SSD layers # block 6: use dilate conv net = conv2d(net, 1024, 3, dilation_rate=6, scope=\"conv6\") self.end_points[\"block6\"] = net #net = dropout(net, is_training=self.is_training) # block 7 net = conv2d(net, 1024, 1, scope=\"conv7\") self.end_points[\"block7\"] = net # block 8 net = conv2d(net, 256, 1, scope=\"conv8_1x1\") net = conv2d(pad2d(net, 1), 512, 3, stride=2, scope=\"conv8_3x3\", padding=\"valid\") self.end_points[\"block8\"] = net # block 9 net = conv2d(net, 128, 1, scope=\"conv9_1x1\") net = conv2d(pad2d(net, 1), 256, 3, stride=2, scope=\"conv9_3x3\", padding=\"valid\") self.end_points[\"block9\"] = net # block 10 net = conv2d(net, 128, 1, scope=\"conv10_1x1\") net = conv2d(net, 256, 3, scope=\"conv10_3x3\", padding=\"valid\") self.end_points[\"block10\"] = net # block 11 net = conv2d(net, 128, 1, scope=\"conv11_1x1\") net = conv2d(net, 256, 3, scope=\"conv11_3x3\", padding=\"valid\") self.end_points[\"block11\"] = net # class and location predictions predictions = [] logits = [] locations = [] for i, layer in enumerate(self.ssd_params.feat_layers): cls, loc = ssd_multibox_layer(self.end_points[layer], self.ssd_params.num_classes, self.ssd_params.anchor_sizes[i], self.ssd_params.anchor_ratios[i], self.ssd_params.normalizations[i], scope=layer+\"_box\") predictions.append(tf.nn.softmax(cls)) logits.append(cls) locations.append(loc) return predictions, logits, locations 对于特征图的检测，这里单独定义了一个组合层ssd_multibox_layer，其主要是对特征图进行两次卷积，分别得到类别置信度与边界框位置： # multibox layer: get class and location predicitions from detection layer def ssd_multibox_layer(x, num_classes, sizes, ratios, normalization=-1, scope=\"multibox\"): pre_shape = x.get_shape().as_list()[1:-1] pre_shape = [-1] + pre_shape with tf.variable_scope(scope): # l2 norm if normalization > 0: x = l2norm(x, normalization) print(x) # numbers of anchors n_anchors = len(sizes) + len(ratios) # location predictions loc_pred = conv2d(x, n_anchors*4, 3, activation=None, scope=\"conv_loc\") loc_pred = tf.reshape(loc_pred, pre_shape + [n_anchors, 4]) # class prediction cls_pred = conv2d(x, n_anchors*num_classes, 3, activation=None, scope=\"conv_cls\") cls_pred = tf.reshape(cls_pred, pre_shape + [n_anchors, num_classes]) return cls_pred, loc_pred 对于先验框，可以基于numpy生成，定义在ssd_anchors.py文件中，结合先验框与检测值，对边界框进行过滤与解码： classes, scores, bboxes = self._bboxes_select(predictions, locations) 这里将得到过滤得到的边界框，其中classes, scores, bboxes分别表示类别，置信度值以及边界框位置。 基于训练好的权重文件在这里下载，这里对SSD进行测试： (https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1snhuTsT) ssd_net = SSD() classes, scores, bboxes = ssd_net.detections() images = ssd_net.images() sess = tf.Session() # Restore SSD model. ckpt_filename = './ssd_checkpoints/ssd_vgg_300_weights.ckpt' sess.run(tf.global_variables_initializer()) saver = tf.train.Saver() saver.restore(sess, ckpt_filename) img = cv2.imread('./demo/dog.jpg') img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img_prepocessed = preprocess_image(img) # 预处理图片，主要是归一化和resize rclasses, rscores, rbboxes = sess.run([classes, scores, bboxes], feed_dict={images: img_prepocessed}) rclasses, rscores, rbboxes = process_bboxes(rclasses, rscores, rbboxes) # 处理预测框，包括clip,sort,nms plt_bboxes(img, rclasses, rscores, rbboxes) # 绘制检测结果 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-09 20:04:04 "},"Artificial_Intelligence/Deep_Learning/GNN/GNN.html":{"url":"Artificial_Intelligence/Deep_Learning/GNN/GNN.html","title":"图神经网络","keywords":"","body":"GNN 图神经网络 综述 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-11 16:55:04 "},"Artificial_Intelligence/Deep_Learning/Model_Compression_and_Acceleration/Model_Compression_and_Acceleration.html":{"url":"Artificial_Intelligence/Deep_Learning/Model_Compression_and_Acceleration/Model_Compression_and_Acceleration.html","title":"模型 压缩 和 加速","keywords":"","body":"Model_Compression_and_Acceleration 模型压缩和加速 Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2021-01-08 13:41:55 "},"Artificial_Intelligence/Model_evaluation/Evaluation.html":{"url":"Artificial_Intelligence/Model_evaluation/Evaluation.html","title":"Model evaluation","keywords":"","body":"模型评估方法 混淆矩阵 - 准确率(Accuracy),精确率(Precision),召回率(Recall)和 F1-Measure ROC曲线 PR曲线 AUC logistic loss，logloss对数损失 使用场景 ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。 如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。 如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。 类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。 最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。 混淆矩阵 True Positive: 预测为真，预测与事实相符 True Negative: 预测为假，预测与事实相符 False Positive: 预测为真，预测与事实相悖 False Negative: 预测为假，预测与事实相悖 计算方法： recall: 召回率 召回率表示在真实为positive的样本中模型成功预测出的样本所占比例 召回率只和真实为positive的样本相关，与真实为negative的样本无关；而精确率则受到两类样本的影响 模型目标是使recall尽可能高 Precision： 精确率 精确率表示在预测为positive的样本中真实类别为positive的样本所占比例； accuracy: 准确率 准确率表示预测正确的样本（TP和TN）在所有样本（all data）中占的比例。 在数据集不平衡时，准确率将不能很好地表示模型的性能。可能会存在准确率很高，而少数类样本全分错的情况，此时应选择其它模型评价指标。 同样，accuracy应该尽可能的高 F1 值 同时测量召回率和精度 F1 值是精确率(Precision)和召回率(recall)的调和平均值 F1 认为精确率()Precision和召回率(recall)一样重要 F&#x1D6FD;方法： 当&#x1D6FD;为1是，此方法为F1方法，还有F2，F0.5等 当&#x1D6FD;>1时，&#x1D439;&#x1D6FD;认为召回率更重要 当0 ROC curve (receiver operating characteristic curve) ROC曲线对于样本类别是否平衡并不敏感，即其并不受样本先验分布的影响，因此在实际工作中，更多的是用ROC/AUC来对模型的性能进行评价 True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows: False Positive Rate (FPR) is defined as follows: ROC 曲线用于绘制采用不同分类阈值时的 TPR 与FPR。 降低分类阈值会导致将更多样本归为正类别，从而增加假正例和真正例的个数。下图显示了一个典型的 ROC 曲线。 ROC曲线上的点可以使用不同的分类阈值进行多次评估模型，更有效的方法是使用ROC曲线下面积 (0,0)点：我们把所有的个体都预测为假，那我们可以知道TP与FP都为0，因为TP表示预测为真实际也为真，而FP表示预测为真实际为假的个体； (0,1)点：我们所有预测为真的个体都正确，这是我们最理想的情况，此时 TP=TP+FN，而 FP=0； (1,0)点：这是预测最糟糕的情况，即所有的预测都是错误的，那么此时 TP=0，而 FP=FP+TN； (1,1)点：因为其是在 y=x的这条直线上，因此其相当于随机预测，即我预测一个个体为真还是假都是随机的。 因此我们可以发现如果一个模型的ROC曲线越靠近与左上角，那么该模型就越优，其泛化性能就越好， 但是对于两个模型: 如果模型A的ROC曲线完全包住了模型B 的ROC曲线，那么我们就认为模型A要优于模型B； 如果两条曲线有交叉的话，我们就通过比较ROC与X，Y轴所围得曲线的面积来判断，面积越大，模型的性能就越优，这个面积我们称之为AUC(area under ROC curve) ROC 优点： 兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。 ROC曲线选用的两个指标， TPR和FPR，都不依赖于具体的类别分布。 ROC 缺点： ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。ROC曲线的横轴采用FPR，根据公式，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。（当然也可以只分析ROC曲线左边一小段） TIP: 公式推导 AUC (Area under the ROC Curve). ROC曲线与坐标轴围成的面积即为AUC 曲线下面积对所有可能的分类阈值的效果进行综合衡量。曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。以下面的样本为例，逻辑回归预测从左到右以升序排列 曲线下面积表示随机正类别（绿色）样本位于随机负类别（红色）样本右侧的概率。 AUC 计算方法 在AUC＞0.5的情况下，AUC越接近于1，说明诊断效果越好； AUC在0.5～0.7时有较低准确性，AUC在0.7～0.9时有一定准确性，AUC在0.9以上时有较高准确性。 AUC 优势： AUC is scale-invariant（尺度不变性）. It measures how well predictions are ranked, rather than their absolute values. AUC is classification-threshold-invariant（分类阈值不变）. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.测量模型质量与阈值无关。 AUC 局限性 Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that. 无法得到良好校准的概率输出。 Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.数据量不均衡，AUC失效。 Precision Recall（PR曲线） P-R曲线刻画查准率和查全率之间的关系，查准率指的是在所有预测为正例的数据中，真正例所占的比例，查全率是指预测为真正例的数据占所有正例数据的比例。 一般来说，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低， PR曲线与ROC曲线的相同点是都采用了TPR(Recall)，都可以用AUC来衡量分类器的效果。不同点是ROC曲线使用了FPR，而PR曲线使用了Precision，因此PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。 PR曲线的绘制与ROC曲线类似，PR曲线的AUC面积计算公式为： Cost curve 损失曲线 代价曲线的作用就是找到一个最合适的阈值，使得分错的情况最少。 Cost curve 相比ROC，优势： Cost curve 能可视化代价。 Cost curve 的纵轴给出了我们对模型最关心的指标：代价或者错误率。且撇开 ROC 的短板 —— 代价不说，仅就错误率而言，CC 上一目了然的事情，在 ROC 空间里并不方便，每拉出一个点，至少得心算一次加法，一次减法。 Cost curve 的纵轴与先验 [公式] 直接相关。通常情况下，先验与样本中的频率并不相同。CC 可以展示模型在各个 [公式] 情况下的表现，也是 ROC 所不及的。 图中多条线段含义： 每一条线段实际上就是一组(FPR,FNR)组合决定的。而一组(FPR,FNR)又是由阈值决定的，不同的阈值就会有不同的(FPR,FNR)。 在ROC曲线时，我们取了11个阈值，就应该由11个(FPR,FNR)的组合，就应该绘制11条线段。 FNR表示正例中有多少被预测错了，变成了负例 FPR：表示负例中有多少预测错了，变成了正例 P(+):表示，你现在有的样本中，正例的比例，如果是之前方块的例子，P(+)就是0.7。 当我们认为，正例错判为负例的代价与负例错判为正例的代价相同时， P(+) = P。 当我们认为把负类判定为正类会造成更大的损失时，此时Cost10 > Cost01，带入公式1, 得 P2(+) ，这时候就有 P2(+) 对应到上图中， 正例概率 P(+) 就会往左移动，对应的阈值就会增大，模型对正类的判断就会更谨慎。 归一化代价。事实上这个东西就是错误率， 首先，横坐标是P(+)，由公式3可以知道，当P(+)=0时， Cost_norm = FPR；当P(+)=1时， Cost_norm = FNR。即，当我用来检测模型好坏的样本全是负例（即P(+)=0）,那我模型产生的错误就只有负例被错误的预测为正例这一种情况，就是(0,FPR)的由来！ 同样，当我用来检测模型好坏的样本全都是正例（即P(+)=1）,那我模型产生的错误就只有正例被错误的预测为负例的情况这一种情况，就是(1,FNR)的由来！ 两个连线中间的情况，用来检测模型的样本有正例也有负例的时候，也就是P(+)=0.x，这时候 Cost_norm 的取值就会同时受到FPR和FNR的影响。 因为FPR，FNR都是样本被分错的情况，本质也是错误率。 画ROC曲线 设置第一个阈值 当我们设置阈值为0.90时，模型 M 打分 > 0.90则为好方块， 计算得到一组（FPR，TPR）值，并将其画在ROC曲线图中 设置第二个阈值 同样，当我们设置阈值为0.85时，情况如下 再次得到一组（FPR，TPR）值，并将其画在ROC曲线图中 连续设置11个阈值 计算得到11组（FPR，TPR）值，将其画到ROC曲线图中，如图 将其画到ROC曲线图中，如图： 将离散的点使用线性插值的方式连接，得到ROC曲线，如图： Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/data_mining/":{"url":"Artificial_Intelligence/data_mining/","title":"数据挖掘 Data mining","keywords":"","body":"Data mining: The process of discovering patterns in data. The process must be automatic or semi-automatic. The patterns must be meaningful and useful, i.e. lead to some benefit and inform future decisions. A pattern is a series of data that repeat in a recognisable way. Data mining works on existing data, i.e. data that has already been generated, by people, machines, processes, etc. Concepts Nominal Score: quantifies how well the model performs on a data set. Training set for building a set of models. Test set for evaluating the model on unseen data. Positive examples are labelled with the right answer or capture the relationships of interest. Negative examples: are labelled with the wrong answer or contain relationships other than the ones we are looking for. Feature engineering is the process of transforming raw data into features that better represent the underlying problem and result in improved model accuracy on unseen data. Decision Tree - Node: corresponds to a decision to be made. Decision Tree - Branches from a node represent possible choices. Clustering - non-overlapping, i.e. each instance is in exactly one cluster, Clustering - overlapping, i.e. an instance may appear in multiple clusters. Patterns Allow making non-trivial predictions for new data. Can be expressed as: Black boxes, i.e. incomprehensible with hidden structure. Transparent boxes with visible structure. Structural patterns: Capture and explain data aspects in an explicit way. Can be used for informative decisions. E.g. rules in the form if-then-else. Real-World data Examples PageRank assigns measures to web pages, based on online search query relevance (Google). Email filtering classifies new messages as spams or hams. Online advertising based on users with similar purchases. Social media identify users with similar preferences. Process of data mining Data mining is a process for exploring large amounts of data to discover meaningful patterns and rules. Receives a data set as input. Investigates for a specific type of model. Produces a knowledge representation as output. Step 1: Determine the objective of the analysis: Identify the data mining problem type. Supervised learning: There is a target attribute. If nominal, then classification. E.g. to play or not in the weather data set. If numerical, then predition. E.g. predict power value in the CPU performance data set. Unsupervised learning: There is no target attribute. Cluster data into groups of similar objects. Find correlations or associations. There are other data mining tasks for other types of data. Step 2: Understand the data. Visualise the data, e.g. using histograms or scatter plots. Confirm that the objective can be achieved with the data set. begin with the data and then select an appropriate method. Step 3: Clean and prepare the data. Fix any problems with the data. Consider noise or missing values COnsider the data is representative or biased Consider whether the data is enough If the data is scarce, then data mining may not be effective. A rule of thumb is that the more, the better. However, very large data sets can be problematic when (i) the target variable appears in extremely rare patterns, or (ii) model building is very resource consuming. Step 4: Build the models. Select the most appropriate model for the data. Step 5: Evaluate the model. Assess whether the model achieves its goals. mesure the accuracy of model How well does the model described the observed data? Is the model comprehensible? Step 6: Iterate Usually, multiple iterations of the above steps are required to build a good enough model. Revise the performed steps, adapt and repeat. Data Set Attributes Numeric: Continuous or discrete with well-defined distance between values. Nominal: Categorical. Dichotomous: Binary or boolean or yes/no. Ordinal: Ordered but without well-defined distance, e.g. poor, reasonable, good and excellent health quality. Interval: Ordered, but also measured in fixed units, e.g. cool, mild and hot temperatures. Ratio: Measurements with a zero point, e.g. distance of objects from one object. The statement that a distance is three times larger than another distance makes sense. Data Set Issues may require the following steps: Assembly Integration Cleaning Transformation The following issues may need to be resolved: Sparsity. Missing values. Noise Data Mining Tasks Classification models the relationship between data elements to predict classes or labels. The data is classified, e.g. UK voters can be labelled as remain or leave. Models ways that attributes determine the class of instances. Supervised learning task because it is based on already classified instances. Numeric prediction models the relationship between data elements to predict numeric quantities. Models ways that attributes determine a numeric value. Variant of classification, but without discrete classes. The produced model is often more interesting than predicted values, e.g. how prices of cars are affected by their features. Supervised learning task because the predicted outcome is known for the data set that we use to build the model. Clustering models the relationship of data items based on their similarity to define groups and assign instances to them. Models ways that instances are similar or different from each other and ways that they can be grouped. Two instances within the same cluster should be similar. Two instances in different clusters should be different. Bylabellingtheclusters,wemayusetheminmeaningfulways. E.g. segment customers into groups. Unsupervised learning task because the data set does not have labels. Association models relationships between attributes with relevant rules. Models how some attributes determine other attributes. No specific class or label. May examine any subset of attributes to predict any other disjoint subset attributes. Usually involve only nominal data. E.g. use supermarket data, to identify combinations of products that occur together in transactions. Knowledge Representations Tables Trees Decision Tree Typically, each branch decision: is made for a single attribute of the data set. Missing Value Problem: Possible solutions, all these solutions propagate errors, especially when the number of missing values increases.: Ignore all instances with missing values. Each attribute may get the value missing. Set the most popular choice for each missing attribute value. Make a probabilistic (weighted) choice for each missing attribute value, based on the other instances. Functional Tree Computesafunctionofmultipleattributevaluesineachnode. Branches based on the value returned by the function. Regression Tree Predicts numeric values. Each node branches on the value of an attribute or on the value of a function of the attributes. A leaf specifies a predicted value for corresponding instances. Model Trees Similar to a regression tree, except that a regression equation predicts the numeric output value in each leaf. A regression equation predicts a numeric quantity as a function of the attributes. More sophisticated than linear regression and regression trees. Rules An expression in if-then format. Conjunction (and), Disjunction (or), General logic expressions (and/or). Linear models A linear model is a weighted sum of attribute values. E.g.PRP=2.47·CACH+37.06. All attribute values must be numeric. Typically visualised as a 2D scatter plot with a regression line, Linear models can be applied to classification problems, by defining decision boundaries separating instances that belong to different classes. Instance-based representations Clusters Dendrogram (Hierarchical Clustering) Networks Model Evaluation How good is the model? How does it perform on known data? How well does it predict for new data? A scoring function or error function computes the differences between the predictions and the actual outcome. Different data mining tasks use different score functions. Typically, we want to maximize score or minimize error. Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/data_mining/Classification.html":{"url":"Artificial_Intelligence/data_mining/Classification.html","title":"Classification and Regression","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/data_mining/Calculate.html":{"url":"Artificial_Intelligence/data_mining/Calculate.html","title":"计算题","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/computer_vision/":{"url":"Artificial_Intelligence/computer_vision/","title":"计算机视觉","keywords":"","body":" Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/Nature_Inspired_Algorithms/":{"url":"Artificial_Intelligence/Nature_Inspired_Algorithms/","title":"启发式算法 - Heuristic Algorithm","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/Nature_Inspired_Algorithms/GA.html":{"url":"Artificial_Intelligence/Nature_Inspired_Algorithms/GA.html","title":"遗传算法 - genetic algorithm(GA)","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/pattern_recongition/":{"url":"Artificial_Intelligence/pattern_recongition/","title":"模式识别","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/NLP/":{"url":"Artificial_Intelligence/NLP/","title":"NLP","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Artificial_Intelligence/Recommend_System/":{"url":"Artificial_Intelligence/Recommend_System/","title":"推荐系统","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Paper_with_code/":{"url":"Paper_with_code/","title":"AI 论文 State-of-The-Art","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Paper_with_code/computer_vision/":{"url":"Paper_with_code/computer_vision/","title":"计算机视觉","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Paper_with_code/NLP/":{"url":"Paper_with_code/NLP/","title":"NLP","keywords":"","body":"Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "},"Phd/":{"url":"Phd/","title":"PhD 导师信息","keywords":"","body":"PhD导师信息 名字 学校 研究方向 联系方式 要求 Professor EMINE YILMAZ University College London （信息检索，数据挖掘以及机器学习，概率和统计的应用领域） research interests lie in the areas of information retrieval, data mining, and applications of machine learning, probability and statistics emine.yilmaz@ucl.ac.uk strong backgrounds in machine learning, probability and statistics Copyright © GUORONG LI 2020 all right reserved，powered by Gitbook该文件修订时间： 2020-11-23 20:11:50 "}}